OLI Torus
Just like other business processes, a data science project is complex with moving parts that include understanding the business needs and objectives of the company that influences the stakeholders, existing system environment, and support structure. The data whose analysis supports the proposed solution may have to be gathered both from inside and outside the organization. Overall, this process of initiating and executing a data science project can be challenging, and it requires technical expertise as well as domain guidance. A Gartner study showed that 85% of data science projects fall short of expectations. Project expectations are defined by your client, and the data science team creates a solution vision that will meet their expectations. The path to defining those project expectations must be tread carefully to avoid project failure.

According to the Chaos Report , a group that tracks IT project failure, reports that data science projects might fail due to multiple reasons, including those that occur in general IT projects. However, there are some unique issues that data science teams must consider to ensure the success of their projects. Some of the reasons for data science project failure include:

Insufficient or inappropriate data,

Lack of technical data science skills,

Issues with project management,

Inaccurate interpretation of results, and

Mismanaged client expectations.

The above-mentioned issues are related to a misunderstanding regarding the client’s business needs and to inadequate communication between the data science project team and the business stakeholders. It is important to identify and understand your client’s business needs, environment, and current solution. This will increase the chances of meeting the business objectives and providing the right analytical solution.

In this unit, we will discuss how you derive analytic objectives from a client’s or organization’s business needs. In doing so, you need to broaden your understanding of the data science process beyond the gathering of data and the application of machine learning methods to a more or less clean dataset. While these aspects are certainly important parts of many data science projects, being a successful data scientist involves being mindful of the big picture to envision, design, implement and ultimately deploy creative solutions for real-world problems. You will find that this course introduces a data science approach that is grounded in scientific research, software engineering principles, and experimentation.

The remainder of this unit focuses on the ability to identify problems and envision a solution, which is among the most important skills a data scientist must possess.






OLI Torus
Business needs are general necessities for a company to remain competitive or expand in the long term. They can typically be stated in broader terms, e.g., acquiring new customers, reducing production costs, improving the quality of its products, or increasing brand awareness. At the start of a data science project, the data science team will work with a client to understand the specific business need.

To meet a particular need, the company will identify one or more specific business objectives it can work towards. These objectives are stated fairly concretely and often have time periods associated with them, after which, somewhat simplified, they will be considered reached (in case of success) or not reached (in case of failure). For example, a business may want to increase sales of a particular product by a certain margin through online advertising, be smarter about scaling production to market fluctuations, reduce travel costs for its logistics, or get a better sense of current trends in its customer base for better product development.

Given a business’s needs and objectives, one can now evaluate whether data science methods can be used to facilitate the company's efforts to meet them. However, the objectives in question will typically be framed in terms of business terminology and assumptions. A data science team will first engage with the client to understand the situation in sufficient depth and establish effective communication with the company’s domain experts. Once the team has familiarized themselves with the problem, as well as the available data and resources, it will work with the client to develop a solution vision. This vision is effectively a walkthrough of what a full data-driven solution to the problem could look like. Based on this, one can identify a set of analytic objectives that, if achieved, will facilitate realizing the solution and reaching the business goal.

In this context, an analytic objective states, in specific terms, what the data science project should produce (insight, resource, model, etc.) and serves as the main success criterion for the team. On the technical side, it drives the derivation of requirements for the project. On the business side, it is critical that analytical objectives stay firmly connected to the business objectives they facilitate.





We use a framework to formulate questions that carefully define business needs and extract business objectives that can be met using data science techniques. This framework can help a data science team meet the expectations of the client and deliver a solution that meets the business needs. The Evidence Value Proposition (EVP) framework was developed to help determine the desired analytic objectives and provide evidence that the objectives can be met with the appropriate data and technology. This methodology, as shown in figure 1 below, shows five (5) steps guided by questions that help formulate business objectives.




Foundations of Computational Data Science - F24 FCDS Pittsburgh


Module 1
6.
AI Philosophy: A Process, not a Product
Due:
not yet scheduled
LEARNING OBJECTIVES & PROFICIENCY
L1
Identify business/research/organizational problems that are suitable for data-driven solutions
L2
Anticipate the dynamics of the problem, the requirements, and the data and how they affect the relevance and accuracy of data-driven solutions.
AI Philosophy: A Process, not a Product


AI Philosophy: A Process, not a Product

The provision of analytical solutions to an organization requires understanding the organization’s needs and its readiness to incorporate and support any analytical solution. A good solution will fail if the organization and its stakeholders are not equipped to support the solution. When engaging with potential clients seeking analytical solutions, it is important to assess the organization’s readiness.

Data Science Ready. The organization has identified all the current sources of data to be considered, and those data have been normalized and integrated into a form that supports data science (statistical analysis, correlation, prediction, etc.). At this point, the organization is ready to begin an engagement with data scientists and AI developers. The Case in point scenario showed that Monro Inc. was data science ready. During interviews with the information technology team, the data manager provided adequate information about the company’s data architecture, appropriate data sources that would support the MotoManager app, as well as data that would be useful to build the desired prediction model.

Data Science Enabled. A preliminary analysis indicates that the data supports the desired analytic objectives (useful correlations are identified, predictive models prove to be accurate enough, etc.). At this point, the organization can claim that it is ready to use data science to influence the capabilities of its workflows, products, and services.

AI Ready. The organization has determined how to leverage the insights from data science (e.g., predicting customer preference) as part of an operational process and has implemented the appropriate software or software extensions to integrate data science into a relevant workflow, product, or service. At this point, the organization can claim that it understands how to incorporate AI into its workflows, products, and services to provide enhanced capabilities for end-users.

AI Enabled. The organization has deployed the new software in a relevant context and is able to directly measure the impact (e.g., increased sales). At this point, the organization can claim that it has implemented an AI solution and is gathering feedback to show that it really works with real end-users. An organization can introduce the data science decision into a real-world setting and measure if this implementation works.

Did I get this?
A company interested in an analytic solution is "Data Science Enabled" once:

All current data sources have been identified and normalized.

Analytic objectives have been defined, and the data supports all objectives.

Correct; The organization is ready to use data science.

A company that measures the implementation of an AI-based analytic solution is considered:

AI Enabled

AI Ready but not AI Enabled

Correct: The organization has deployed the new software in a relevant context, and is able to directly measure the impact.





Case in Point: MotoManager and the Evidence Value Proposition
Due:
not yet scheduled
LEARNING OBJECTIVES & PROFICIENCY
L1
Identify business/research/organizational problems that are suitable for data-driven solutions
L2
Anticipate the dynamics of the problem, the requirements, and the data and how they affect the relevance and accuracy of data-driven solutions.

MotoManager Case-in-Point

Here is an example of an AI consulting firm that used the EVP framework to meet the business needs of a popular automotive services provider.

Mr. Tire-Monro Muffler and Brake (a subsidiary of Monro Inc.) is a top-50 automotive part and general repair services provider in the U.S., with over 320 locations nationwide. In general, the automotive services industry struggles with customer retention. Companies record many one-time-only transactions (frequently with a deep-discount coupon) but fewer transactions from repeat customers (who typically provide much more revenue per year per customer). Lack of customer “stickiness” leads to a) less potential revenue and b) less data about customers in general, which could potentially be used to offer specific products and services to individual customers. Attempts to increase customer acquisition and retention via email marketing and television and online advertisements did little to increase the proportion of repeat customers.

Monro Inc. (the parent company of Mr. Tire) approached Cognistx (an AI applications company) to develop a data-driven solution to improve customer acquisition and retention. The video above provides a brief summary of how Cognistx engaged with Mr. Tire to develop MotoManager, a mobile app that was deployed by Mr. Tire. This solution led to measured increases in customer acquisition and retention, as well as increased revenue.

The Cognistx data science team met with the business leaders of Monro Inc. to gain an understanding of the company’s business needs related to customer retention. The data science team identified and interviewed all stakeholders from the business and technical teams at Monro Inc., including the Data Management, Information Technology, and Service Management teams. The IT managers provided information about the company’s data asset management structure, including data governance, data architecture, and data security management. Accessible and reliable data is important to the solution vision process; a company without adequate data management can not support an analytical solution that might meet its business needs.

Additional Reading

Data Management in the Enterprise

Finally, service managers were interviewed on customer service difficulties that could be addressed by the proposed solution. The service managers also identified the hardware and software gaps at various stores around the country. Once the interviews were completed, the Cognistx data science team formulated business objectives that would meet Monro Inc.’s business needs. The business objectives included:

Creating an application that provides customers with a customized service experience for their automotive needs.

Offering customers $50 coupon to download Monro Inc mobile app.

Onboarding customers to the application with the creation of customer profiles.

Providing tailored customer service management to very important (VIP) customers.

Classifying customers as VIP customers based on defined characteristics.

Creating a loyalty program to increase repeat customer transactions.

Business objectives should be measurable to ensure that business needs are met. The metrics used to assess the success of the project were:

The number of people who installed the app and on-boarded upon receiving a $50 e-coupon.

The number of times an on-boarded customer visited a store close to them.

The number of transactions completed with the app.

The total amount of revenue generated via the app compared to total cost of maintaining the app.

A model that can predict a repeat customer from among on-boarded customers with an accuracy of 85%.

The metrics were both technical (precision and accuracy of the model) and business-related (calculate return-on-investment).

Based on the business objectives, Cognistx developed an AI-enabled application for Monro Inc. called MotoManager. MotoManager captures a comprehensive profile of a customer through the onboarding process. Monro Inc. also sends a $50 coupon incentive to current and potential customers, and his coupon can be retrieved when a customer installs the application and completes their user profile. The MotoManager app uses customer data to provide customized reward incentives for booking services and making purchases from a customer’s local Mr. Tire store. As of 2019, the app has had 53,000 users, and Monro Inc. has reported significant increases in customer engagement and retention. The company has generated over $14 million US dollars from app-based transactions.




A data science project does not begin with building models; one must consider the needs of the client and set objectives to meet those needs. A data scientist must approach a project with a methodology. Similar to scientific research, a data science project follows frameworks that will guide the problem identification process. A data science team will work with a client to understand the business need(s). Those needs will are then translated to data science tasks.

Business objectives will be defined by the company to help meet business needs. These objectives are stated fairly concretely and often have time periods associated with them, after which they will be considered reached (in case of success), not reached (in case of failure). Given a business’s needs and objectives, one can now evaluate whether data science methods can be used to facilitate the company's efforts to meet them.

A data science team will engage with the client to understand the situation in sufficient depth and establish effective communication with the company’s domain experts. Once the team has familiarized themselves with the problem as well as the available data and resources, they will work with the client to develop a solution vision. Finally, the data science team will identify a set of analytic objectives that, if achieved, will facilitate realizing the solution and reaching the business goal.

This module introduced you to the Evidence Value Proposition framework (EVP). The EVP is a suitable framework that can be used to ensure that defined business objectives are met and has been developed to help determine the desired analytic objectives and provide evidence that the objectives can be met with the appropriate data and technology.

When engaging with potential clients seeking analytical solutions, it is important to assess the organization’s readiness.


Once a set of business goals has been identified, one can proceed to formulate analytic objectives that state how the application of analytical methods to data can facilitate reaching the business goal. An analytic objective can typically be phrased along with the following template:

As an incremental step towards business objective O

We work towards solving problem P

by focusing on specific tasks T

and applying analytic methods M in conjunction with data D

to create valuable functionality F and/or produce insight I

This formulation of an analytic objective can also be considered a research hypothesis because it effectively posits that a certain technique improves a metric with regard to a population of problem instances (comparable to treatment versus control condition). In fact, in academic settings, the effectiveness of data science methods will regularly be scrutinized using statistical tests, and it is good practice to principally strive for similar methodological rigor in industry applications. While data science teams and projects in the private sector may use different terminology and specific technical or other circumstances may differ, this pattern of formulating analytic objectives is a very good point of departure for framing data science work and effectively communicating with clients and domain experts. It is the connecting element between the problem, the technique to be applied, the associated requirements and evaluation, as well as the business objective. Also, one should keep in mind that projects often comprise multiple analytic objectives that form a larger plan.

In order for you to become proficient in working with this definition, we will now move to examine its components.



The ability to study and understand a project’s business objective in sufficient depth in order to maximize the benefit from leveraging data is a critical skill for data scientists. This skill can be thought of as having three aspects:

Engagement with the client and research about the specific circumstances of the client’s business.

First, one engages with the client by learning about their business or organization, its customers/clients, the market and its competitors, and the general state of the art in achieving business objectives of its kind. Often this may involve a workshop-like event between the business unit and the data science team, which may lead to a regular communication schedule (e.g., calls, in-person meetings, reports).

Engagement with the client and research around the general domain in which the client’s business operates .

Second, preparation will often require additional research around the client’s general domain and market using the internet, but also from subject area books and academic publications. A data scientist does not need to become an expert herself in the respective area or market, but she should be convinced that the domain is sufficiently understood and that she can intuitively explain why and how the business wants to reach the objective, as well as engage with the client’s experts in a serious conversation about the topic without frequently stumbling over misunderstandings.

Spending continuous efforts to maintain alignment of the ongoing project with changes or new information in both the general domain and specific circumstances.

Third, once the project has progressed to the design and implementation of the analytical models and experiments, the alignment to the business objective must be maintained. A common pitfall is that, once an intriguing machine learning aspect of the project is discovered (e.g., a dataset suitable for deep neural networks), it may appear to be more rewarding to invest resources there and neglect tasks that appear mundane or laborious (e.g., data collection, cleaning, or error analysis) but are equally important for the business objective. In extreme cases, for example, this can lead to the application of overly complex methods to unsuitable datasets, ultimately resulting in project failure. Another bad outcome is the development of models that solve problems the client does not actually have. One of the purposes of this course is to make you appreciate that a data science project’s success needs to be gauged in typically two ways: experimental outcomes as measured by technical performance metrics and their contribution to the greater effort of reaching a business objective.

Familiarizing oneself with different substantive domains in order to deliver good data science work takes patience and attention to detail. It is a lifelong learning process but is also one of the most rewarding aspects of this profession. It is not uncommon for data scientists to have some formal education or professional experience in specific disciplines (e.g., medicine, biology, finance, or business), which enables them to do highly effective work in that area. Similarly, trained data scientists may decide to specialize in a certain field because the depth of their expertise makes them sought after consultants, or because they consider it personally satisfying.

Did I get this?
What does the data science team need to do to understand the business objective?

The team should strive to understand the client’s explanations, but also make a professional effort of researching the business and the field/market it operates in .

The data science team should always have a member who is formally educated or professionally trained in the client’s area of work.

Correct: The data science team must assist the business in translating the business needs to data science tasks.

You are part of the data science team for a project and the analytic objectives are set. A model that predicts the lifetime value of a customer is needed and the data science team begins to build the model. Which statement is true?

The data science team will meet the business need and save time and cost.

The project is prone to failure as the team will likely develop a solution that does not meet the client's needs.

Correct: The data science team although equipped to build models, will miss essential steps including data preprocessing.




The problem underlying an analytic objective should satisfy the following criteria:

Solving the problem must be part of a possible solution vision toward the business objective.

Domain experts should be confident that data exists which, when analyzed, can facilitate that solution. This data must either be available or sources are available to retrieve the data.

The problem must be specific and realistic so that a corresponding data science project can succeed in principle.

First, with the business objective clearly stated, one can propose a solution vision describing how the business can reach the objective. The path to realizing this vision can then be decomposed into sub-problems. The data science team will then be responsible for leveraging data to help solve these sub-problems and/or produce data-derived insight that allows the client to make good decisions along the way.

As data-driven methods are still in the phase of being adopted across many fields, the perspective of using data to support business objectives may even be the main business objective that starts the whole engagement. In such a case, it is helpful to recall which higher-level business needs should be fulfilled in order to progress beyond “we want to leverage data somehow” and arrive at a proper project formulation that allows the statement of specific requirements and evaluation criteria, even if they include exploratory tasks.

Example
An online company asks for a data science project around the use of social media data for understanding its market, driven by the board’s desire to keep up with the competition. After the data science team studies the business and explains to the client a number of possibilities regarding how social media streams are typically used in retail businesses, the client will focus on the need to increase market share and identify the business objective of increasing sales of a particular product. There seem to be two ways forward: (1) Spend money on increased advertising of a unique feature of their product or (2) lower the price. The client asks for an analysis of social media data to inform their decision. This need for gauging consumer preferences now forms the problem component of the analytic goal.

Second, there must be a sound presumption that the problem’s solution must benefit from the use of data. A collaborating team of domain experts and data scientists should discuss the available data sources and their suitability for the solution, including data that would need to be collected as part of the project. In order to be suitable, the data should exhibit informative patterns relating to the problem. It takes a combination of substantive expertise and data science skills to assess this criterion.

Domain-specific problems around the availability of data include:

Lack of readiness in the organization (e.g., the organization's data is not readily processable), unsuitability of the data for the objective (e.g., data is old/stale, out of domain, incomplete, or suffers from an obvious bias), or difficulty in collecting data because the expert labor involved is too expensive.

Various technical objections, such as too little data for the required methods, fragmentation of data across multiple units with no suitable way of joining, or overwhelming imprecision/noise in the data. In cases where data is available, but the existence of a “signal” for the problem is uncertain, exploring whether the signal exists and the extent to which it can be leveraged can become an exploratory analytic objective in and of itself.

Typically, data science projects/consultations involve a preliminary data survey that informs or even precedes a longer substantive discussion. It is very strongly recommended that such a survey be conducted before the team commits to fulfilling any analytical expectations on the part of the client.

Third, both the domain experts and data scientists must be in agreement that the problem is specific and realistic enough so that a data science project attempting to make progress towards it can succeed in principle. In other words, the system and/or insight produced by the project must add enough value to be deemed a success if executed properly. On the client's side, this criterion mandates a moderation of expectations and ensures that the data science component of the whole project can be evaluated. For example, while it should mostly be avoided, a solution vision may prove to be idealistic and somewhat resemble a science-fiction scenario. In such cases, the main purpose of the data science project is to assess its feasibility based on available data and methods and should be explicitly stated as such. On the other hand, the technicians must be very careful not to exaggerate analytical capacities and lead to unwarranted impressions that certain functionality is within reach. For example, once an initial data sample has been surveyed, the results should be communicated as being contingent on the assumption that the sample is representative of the larger dataset. Similarly, if a particular neural network model performs a classification task very well on some domain, the principle feasibility of transferring it to a second domain with reasonable performance should be explicitly tested before promising that it can perform at the same level.

Did I get this?
What is the main purpose of the problem statement in the analytic objective?

It explains why the business objective has not been reached.

It focuses on specific steps necessary to achieve the business objective that can benefit from data-driven methods in the project.

Correct: A well defined, problem will allow the data science and business team define and achieve the business objectives.




Once the team has identified the precise problem for which data should be leveraged, it is advisable to explicitly characterize the task to be done at the analytical level. To do this, one casts the “heart” of the problem into one or more categories of typical data science tasks:

Classification: Individual instances in the dataset have a categorical (i.e., non-numerical) label associated with them or will be labeled as such. The goal of the project is to develop a system capable of categorizing data points using these labels. A common variant of classification is sequence labeling, where individual data points are not independent but form a series. Typical examples of classification are sentiment analysis of text and labeling images as depicting certain objects (animals, cars, etc.).

Regression: Individual instances in the dataset have a numerical label associated with them whose magnitude carries a specific meaning. The goal of the project is to develop a model capable of predicting this target score for individual data instances. Like classification, regression is often applied to dependent series of data points. Typical examples of regression include predicting measurements in medical or demographic data over time.

Retrieval & Ranking: The dataset can be thought of as one or more collections of data points and queries. In response to the query, one or more “ideal” data points should be retrieved and presented in “correct” order. A typical example is a search engine returning a ranked list of results in response to a query.

Recommendation: The dataset consists of users and items, as well as information about the preferences of users for specific items. The task is to find items to recommend to users towards the maximization of some utility. Typical examples are movie recommendations on Netflix and product recommendations on Amazon.

Clustering: The dataset is assumed to have some latent structure which should be discovered by partitioning the data points into groups that are close to each other in variants of the feature space. A typical example is the exploratory analysis of unlabeled data.

Anomaly Detection: The dataset is assumed to have some latent structure that should be discovered in order to identify instances that do not adhere to the pattern. A typical example is the detection of fake customer reviews in online retail data.

Domain-specific tasks: Aside from the generic task types explained above, some domains have developed specific task patterns and associated evaluation metrics that should be used if the analytic goal is sufficiently specific. This is particularly true of natural language processing and image analysis. For example, machine translation will commonly be characterized as a text generation task, and models will be evaluated using a specialized metric called the BLEU score . Similarly, models that segment a part of an image containing a specific object may be evaluated using average precision in conjunction with an “intersection over union” threshold.

It should be noted that some of these categories overlap somewhat in that the methods associated with them can be used in different ways. For example, ranking problems can be solved using classification or regression methods. Still, if the primary nature of the problem is that items must be ranked, it should be evaluated using ranking experiments and metrics, even if it is doing classification “under the hood.”

The purpose of identifying the type of task at this stage is not to firmly restrict the project to a narrow instrumentarium, but to characterize it for purposes of planning and communication. Specifically, a possible quantitative evaluation and the metrics used therein will typically closely correspond to the problem's primary nature.

Did I get this?
Which primary role does the task statement play in the analytic objective?

It works as a contract between client and data science team that only specific methods (e.g., classification, regression) may be used in the project.

It characterizes the task for purposes of planning and the nature of the eventual evaluation.

Correct: The task statement will provide a possible quantitative evaluation and the metrics used therein will correspond to the problems' primary nature.

Consider a ranking problem that can be solved using regression or classification tasks:

The problem should be evaluated using the method that the data scientist/team can implement. This will ensure that the problem is solved adequately.

The problem should be evaluated using an experiment based on the primary nature of the problem.

Correct: This will ensure that you are addressing the problem with the right method.


If you have not already, you should notice a pattern at this stage. The conceptual progression Business Need ⇒ Business Objective ⇒ Analytic Objective including Problem Statement ⇒ Task Definition ⇒ Method & Data Statement serves the purpose of gradually refining our understanding of what the client needs until we arrive at a technical project specification that the technicians can design against. The statement of methods to be applied will vary in specificity depending on your project, but three elements are important:

The statement must be precise enough so that the data science team understands it as a concise summary of the technical approach.

At the same time, it should allow for testing different techniques around the main conceptual idea.

The methods should in principle be suitable to produce the target functionality/insight for the task given the available data.

As you become more proficient in the discipline of data science (e.g., after having completed the more advanced units of this course), you will develop refined intuitions about which method to propose in which context. The most general categories of methods one can identify in this statement typically include:

Supervised learning methods which involve learning to predict a target variable (typically through regression or classification) by training on “true” example data points whose target variable has manually been labeled or is available by other means.

Unsupervised learning methods which deal with finding patterns in unlabeled data without an explicit prediction target.

Semi-Supervised learning methods which encompass hybrid methods that combine supervised and unsupervised learning in different ways.

This course is very focused on the application of machine learning, which has a large overlap with various kinds of statistical methods. It is possible to phrase your method statement around the use of statistics, but it is recommended that one qualifies this rather broad term as something adequate for the project.

Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.

In many contexts, the method will need to be stated much more precisely than that, especially if the problem domain is already well-studied in data science or there is some prior work that should be extended. The method can range from specifying a particular family of models (e.g., linear vs. non-linear), using a new feature set, testing the explainability of a particular model’s predictions, optimizing hyperparameters for faster training and inference in neural networks, and more. We can illustrate the specific vs. general statement in the context of an example:

Example
Your client gives you access to a large dataset of electronics product manufacturing and customer support data and asks you to help improve quality control (business objective) as the quality control personnel do not reliably find all potential defects (problem). Your model should be able to predict product failure within one month after the sale (task 1) and identify predictors measured at quality control time (task 2). In a pilot project, you propose to apply “traditional supervised learning methods to train a classifier and examine the model for predictive variables.” You then proceed to conduct the project using basic logistic regression and some nonlinear tree-based models as they allow straightforward model explanation.

Variation: Your pilot project was a success, and your models are being used to better inform quality control personnel. However, it turns out that it still produces a high false-positive rate (problem) and causes shipping delays, which the client wants to minimize (business objective). You are asked to improve the model’s performance by reducing its false-positive rate (task). You are further given access to quality control diagnostic equipment readings, which the engineers believe are useful to discern whether a product is defective or just “needs to be broken in.” You hence propose to integrate the reading data using a special signal encoding algorithm in combination with a nonlinear model architecture to improve the model.

The method and data are the heart of a data science project. Finding the best tool for the task is, of course, one of the core skills of being a good data scientist. This course will give you an introduction to basic methods and provide you with the opportunity to apply them in course projects. As you gain deeper knowledge and experience, you will become better not only at analyzing problems and tasks in different domains but also at researching data science literature and libraries effectively and coming up with project proposals that are aligned with state-of-the-art solutions. At the same time, thinking through different approaches and deciding on a set of methods and datasets benefit from teamwork, open discussion, and active seeking of advice and feedback from your peers, mentors, and relevant specialists.

A good beginning strategy is to check your proposed method against the target functionality or insight by conceptually thinking through its application and explicitly formulating expected results.

Example
In the above electronics scenario, you imagine training a logistic regression model on the features in the data to predict product failure. Once trained, influential features should receive a high weight in the regression equation, allowing you to identify them easily, similar to correlation analysis. If you train a decision tree, you can identify features by traversing its branches. On the other hand, if you were to propose to train a complex random forest model to predict product failure with maximal accuracy, you may encounter the objection that random forests are not as easily interpretable as simpler models. It would hence be an unsuitable method for task 2 in the pilot stage. A good way would be to go with the simpler models for now and leave more complex models for later phases if needed. In the variation, a colleague may suggest that the signal encoding algorithm you plan to use is outdated and recommends you use a more recently developed encoding. You may research both algorithms and make an informed decision, or even use both algorithms in a comparative experiment.

Did I get this?
The statement of methods is important because:

It allows you to define the types and sources of data that can produce the target insight for an analytical task.

Similar to the task and problem statement, you are gaining an understanding of the business needs.

Correct: A better understanding of the business needs will lead to well defined problems, objectives, tasks and ultimately meeting the client's expectations.

A data scientist is tasked to discover the inherent groupings in a marketing firm's customer dataset. The analytic objectives have been defined and it is time to select the appropriate method for this objective.

Semi-Supervised Learning Method

Supervised Learning Method

Unsupervised Learning Method

Correct: Unsupervised learning methods are best applied to tasks were an outcome is not known.



Proposing Datasets & Collection Methods

Just as the methods proposed must be suitable to achieve the analytic goal, so must the data on which the methods operate. The most important criterion in this regard is, of course, that the data contains patterns that are informative for the analytic objective and allow one to successfully tackle the proposed task and make progress towards solving the problem in a data-driven way. Beyond this main requirement, and depending on the project context and proposed methods, specific data sources can be more or less suitable for the project. Possible considerations include:

Is the dataset of the appropriate size for the proposed method? How many data points and how many features does it contain?

What distribution do the individual phenomena in the data follow?

Is the dataset raw or readily processable? What preprocessing is necessary and how will it influence the relevant patterns?

Is the data complete or are some parts of it missing?

Is the dataset clean or noisy? Does measurement error play a role?

Are there access or disclosure restrictions to the data? Is it confidential, private, sensitive, partially redacted, or classified? Is it subject to licensing restrictions?

Well-studied benchmark datasets are available to the data science community for many tasks. While prior work on a dataset is a good indicator of its utility for a task, it cannot replace the process of familiarizing yourself with the data before commencing serious experimentation work. It is good practice to investigate the suitability of the proposed dataset for the task and method before moving on with the project unless the exploration of the dataset itself is understood as part of the analytic goal. This is typically done through research and preliminary data surveys, possibly in collaboration with domain experts.

While statistical analysis and machine learning are, of course, core pillars of data science, effective collection, curation, and interaction with data are equally important and regularly the subject of analytic objectives and even entire projects. As such, the core method and data component of an analytic objective need not necessarily always be about training a model on available data but can also be about collecting data to enable subsequent analysis.

When proposing data collection as part of an analytical objective, the collection methods must be scrutinized as to whether they are likely to succeed in producing a valuable dataset resource. Data collection, especially involving human annotators, is its own research field. Relevant considerations include:

How well will the collection represent/approximate/cover the desired distribution of phenomena relevant to the problem?

Does the collection require human annotators? Can it be done using crowdworkers?

What qualifications do the human annotators need to possess? How can they be effectively trained for the task?

Do the human annotators need to be examined/tested before and/or after collection?

How should the annotation task be designed to ensure productive and correct annotation? How will agreement between annotators be measured?

How much data should be gathered to enable progress towards the analytic objective? How much data can be gathered given the budget?

Once the data has been gathered, what cleaning and curation needs to happen?

Are there any approvals/permissions that need to be obtained before the collection can proceed (e.g., human subject research)?

This course includes an introduction to data collection methods. For purposes of this unit, the main takeaway is that collecting good datasets requires an amount of skill, care, and attention to detail comparable to those needed for doing good data analysis. Like core machine learning efforts, data collection projects are conducted with specific analytic objectives in mind and hence can be framed and assessed using the same template.

Did I get this?
How does data collection and curation relate to the field of data science?

Data collection and curation is an auxiliary problem and is not a primary concern for data scientists.

Data collection and curation is a complex sub-discipline of data science and is equally important as data analysis .

Correct: inaccurate data and data from the wrong source can compromise the solution.



Finally, the analytic objective should state what specific functionality, insight, or resource is gained from leveraging the data and methods you propose relative to the current situation and assuming the project is successful as proposed.

Valuable Functionality: In many practical scenarios, this may simply be that, for example, a predictive model can successfully solve the task and contribute to the problem solution, as stated. In more complex situations, the benefit gained may only be an incremental yet necessary step towards producing a model that solves the task. For example, the task may be to link passages in news text that state false information about certain historical events to a reference database of those events. Initially, one would need to detect whether passages talk about historical events at all before then discriminating which precise one they address. This detector, if implemented successfully, would add valuable functionality to the linking task.

It should be noted that analytic objectives can, of course, be reframed to focus on specific tasks, in which case an incremental step would become the main goal. In the example just given, one can change the analytic objective so that historic text detection becomes the main task in the first phase of the project. In fact, if the circumstances and the client allow for such a rescoping, then this may even be preferable. One of the main points of this unit is that explicitly formulating, discussing, and committing to analytic objectives supports a productive data science project lifecycle and ensures a proper alignment of the work with the business objective.

Valuable Insight: In some project contexts, the client may not need a piece of software that automates analytic functionality but rather requires insight from data in order to make decisions. This area of data science blends into what is commonly referred to as “business intelligence.”

Valuable Resource: Data science projects can also produce resources to be used by the organization or further projects. For example, the project may be intended to collect and curate a competition dataset and publish it along with some baseline results to facilitate research on a certain topic.

Improvement over the current situation: Naturally, the project should improve over the currently available functionality, information, and resources. In industry settings, this is usually obvious. In academic and other research settings, however, you will be characterizing your project as improving over the state-of-the-art results. This can be done via a survey of related work and possibly some exploration of existing methods and datasets. While this will often be left implicit in the statement of the analytic objective, project proposals (especially academic ones) may require an explicit section on related work.

Assumption of project success: While it is natural that one would propose a project with confidence to succeed, it is worth noting that many data science projects (especially in academic settings) are exploratory to different degrees. The dataset may contain too much noise on top of the interesting patterns, or the computational effort may be too large. One should be mindful of what can be done to distill some value-added from the data, even if the main objective may fail due to factors beyond one’s control.




Now that you have studied the elements of a properly framed analytical objective, we shift towards explaining three basic archetypes of hypotheses that will cover a fair amount of projects one encounters in data science. They are provided here as purely illustrative example instances of the general template on which you can base your own formulations.

Not all framings of analytic objectives will include every individual element, as some of them may not be necessary depending on the situation. In industry settings, the problem and task may be merged, and the added valuable functionality may be evident from a model that performs its function well. In academic settings, the overarching interest may be that of advancing state of the art in research, and hence the statement may either not include an explicit business objective or state it as a problem solution vision.

Constructive

A constructive analytical objective states that it is, in principle, possible to develop a desired functionality from the available methods and data without the need to fully optimize its performance yet. One can think of it as a proof-of-concept or prototyping endeavor.

Example
Industry Constructive Objective

In order to increase sales from the company’s online store (Business objective)

...we work towards increasing the click-through rate of its advertising through targeted content (Problem)

...by classifying website visitors into youth, middle-age, and senior demographics (Task)

...using supervised learning models on curated internal datasets (Method)

Example
Research constructive objective:

(Business objective omitted due to project being primarily research)

In order to enable more effective search of audio collections (Problem)

We demonstrate the feasibility of a system that retrieves audio pieces from short natural language descriptions of their sounds (Task)

using neural models on a dataset of short clips of classical music and their descriptions (Method)

towards developing suitable multi-modal audio-textual encoding (valuable functionality)

Benchmarking

In scenarios where the feasibility of an analytical task has been established, projects may be targeted toward improvement over the state-of-the-art in some performance metrics by using innovative methods/features/data. This is typically the case if one works on leaderboard-type datasets where there are models.

Example
Industry Benchmarking Objective

The client is a logistics company that wants to speed up its automatic package sorting (Business objective)

We focus on the problem of handwritten address recognition from shipping label scans (Problem and Task)

We want to combine neural image recognition with language models on company-internal data (Method and Data)

To improve performance beyond the current model based on standard convolutional neural networks without language information (Valuable functionality)

Example
Research Bench-marking Objective

(Business objective omitted due to project being primarily research)

For the task of span-based question answering from text (problem and task merged because span-based question answering is a common leaderboard task)

We want to combine graph-based knowledge bases with neural attention models (Method)

To improve over state of the art performance on realistic news text (valuable functionality and data)

Exploratory

Exploratory objectives are typically formed when data is available that is related to a problem of interest but needs to be surveyed before it can be used in projects pursuing constructive or benchmarking objectives.

Example
Industry Exploratory Objective

The client runs a complex semi-automatic manufacturing pipeline and wants to make it more efficient (Business objective)

Specifically, he would like to see whether some parts of the process statistically interdepend so that bottlenecks and critical components can be identified (Problem and Task)

We want to conduct a qualitative survey and basic statistical analysis on a dataset of production machinery sensor readings provided by the client (Methods and Data)

Towards identifying correlating events across the production process that can be used for process optimization .

Example
Research Exploratory Objective

(Business objective omitted due to project being primarily research)

The development of AI dialogue systems suffers from a lack of clear training signal of how satisfied the user is with the chat bot’s replies (Problem)

We want to conduct a sparse labeling of conversation quality and produce basic topic models for a dataset of chat protocols (Methods and Data)

In order to develop a per-topic quality scoring rubric for the eventual annotation of a larger dataset. (Task/Valuable Insight)

Did I get this?
A county in the US is interested in using a risk assessment tool to assess a criminal defendant's likelihood to re-offend. The county's office of analytics has defined the analytic objectives for this project. Answer the following questions based on this scenario:

A constructive analytical objective for this scenario would be:

Demonstrate feasibility of the tool to assess the likelihood of recidivism using random forest models on a dataset that has offender data from 1960 until date.

Improve the performance of existing models developed by a neighboring county.

Correct: the stated objective is to develop a desired functionality from the available data and models without the need to fully optimize its performance yet.




This module focused on formulating the analytic objective, which is also considered a research hypothesis because it effectively posits that a certain technique improves a metric with regard to a population of problem instances.

A well-framed analytic objective will include the following elements:

Understanding the business objective,

Identifying the problem,

Focusing on a well-defined task,

Checking the proposed method against the target insight

Proposing data collection and curation methods, and

Framing the analytic objective.

Analytic objectives can be reframed to focus on specific tasks, in which case an incremental step would become the primary goal.

An analytic objective should state what specific functionality, insight, or resource is gained from leveraging that data and methods you propose, relative to the current situation, and assuming the project is successful as proposed.




Data Science projects can be complex in nature and require the input and efforts of many stakeholders. A Data Scientist will lead the process, and it is important that a well-defined workflow is followed. The workflow will ensure that all stakeholders are on the same page and requirements are defined and met.

A data scientist will produce a solution that is effective and achieves business and analytic objectives with the end goal of meeting a business need. In this module, we will explore the data science lifecycle. Keep in mind that, just like the system development life cycle (SDLC), the data science life cycle is not linear. Real-world problems will introduce hurdles that require the process to be iterative in nature. The lifecycle will give structure to the process and ensure that the data scientist stays on task.


Figure 2. Data Science Life Cycle (courtesy: Microsoft )

You can draw similarities between the CRISP-DM lifecycle and the data science lifecycle. Perusing the Internet, you will also find that different data science solution vendors have adapted the lifecycle to fit their products and the solutions supported by their tools. The data science lifecycle is briefly explained below:

Business Understanding. “We fail more often because we solve the wrong problem than because we get the wrong solution to the right problem.” – Professor Russell L. Ackoff.

Data scientists are tasked with providing solutions to difficult business problems, and those solutions should be supported by factual data. Prior to solving a problem, it is important to understand the context of the business and the problem. This must include defining business and analytical objectives, as well as identifying data sources.

Data Acquisition. This process involves obtaining data from various sources and may also require setting up a data collection task and infrastructure.

Subsequently, you will perform data preparation to ensure the data is ready for analysis.

Data Preparation. This is the process of cleaning and transforming raw data prior to processing and analysis. This needs to be done carefully as assumptions made here may influence, or even limit, the use of the data during analysis.

Data Exploration and Cleaning. The quality of your dataset will determine your success in meeting your business objectives. Data exploration includes identifying variables, conducting univariate and multivariate analyses, identifying outliers, anomalies, and missing values, as well as feature creation and selection. We will cover these topics in future units.

Modeling. Later in this course, you will explore modeling and learn about choosing the appropriate model based on the problem. You will study algorithms to implement analytical models and tune their hyper-parameters to achieve the desired performance. We will learn about the balance between generalizability and performance. In general, you want your model to learn and perform well but also to be robust when tested on unseen data.

Feature Engineering is needed to prepare proper datasets that are compatible with the suitable algorithms and to improve the performance of models by leveraging domain knowledge to capture the signal of interest in the features.

Model Training is made efficient when you have adequately prepared your data and engineered new features. Model training involves maximizing performance and finding a balance between performance and generalization. Even in cases when a Data Scientist has collected millions of records, data should be considered and treated as a scarce resource since it may be expensive to obtain.

Models are trained on dedicated training data and evaluated on dedicated test data. Models should not be tested on the data they have been trained on. The ability to match the training performance on unseen test data is referred to as the model's ability to generalize. To operationalize this during training, a validation data set is often sampled from the test data to allow an estimation of test performance during training. In the lifecycle, it is important that this separation is anticipated early because it may influence what parts of the data may be surveyed for feature engineering and model design at the beginning of the project without violating the training/test split.

Model Evaluation is an essential step in the lifecycle. Typically, analytical solutions are meant to provide results when fitted with different datasets or when new data is introduced. Depending on the nature of the task (as stated in the analytic objective), model evaluation will follow corresponding metrics and techniques that will be explored in this course.

Deployment. Once you have evaluated your models to ensure accuracy and performance, you will deploy the model to an environment for application consumption.

Did I get this?
At what stage do you identify the business objectives of a data science project:

Business Understanding.

Data Understanding.

Correct: This first phase sets the pace for the rest of the project! The data science team will understand the business needs and define objectives to meet those needs.

The process of using transforming raw data into informative properties that represent the business problem you are trying to solve is called:

Model Training.

Feature Engineering.

Data Preparation.

Correct! You use domain knowledge to extract features from raw data using statistical techniques.

 



 As we have seen, the data science process involves multiple steps that require the expertise of team members in defined roles. The data science team is the talent that will help develop the analytical solutions that meet the business objectives of a data-related problem. Certain organizations can provide the structure and support for the different responsibilities within the process. In smaller organizations, personnel in the data science process might wear multiple hats to ensure the efficient execution of tasks and the development of solutions. Below, you will find the roles of a typical data science team. This list of roles will vary depending on the domain and size of the organization.

Data Scientist. This role involves solving business tasks using machine learning model development and statistical techniques. This individual identifies trends and patterns within the data and makes predictions based on trends. The data scientist will write code to support the data analysis and model-building process.

Data Engineer. The Data Engineer specializes in data structures and algorithms, as well as in working with data in databases and other large repositories.

Solutions Architect. This is a customer-facing role that ensures end-to-end customer deployment for company-related data services. The Solutions Architect interacts with clients to design, coordinate, and execute solution prototypes.

Machine Learning (ML) Engineer. The ML engineer performs modeling tasks that are different than the tasks the data scientist performs in that the ML Engineer is further away from the domain side of the project. The ML Engineer spends a considerable amount of time programming and creating ML solutions but also needs to have strong statistical skills.

Data/Business Analyst. A data analyst has data gathering, analysis, and visualization skills. Like the data scientist, she provides insights from data to inform decision-making. She develops key performance indicators and utilizes business intelligence and analytics tools. Compared to data scientists, however, data/business analysts are typically firmly rooted in the business domain and are not necessarily as proficient in programming and advanced machine learning.

Software Engineer. The Software Engineer handles the alignment between the business objectives and solution and is responsible for integrating the implemented data-driven system into the appropriate applications within the enterprise.

Domain Experts. Also known as subject matter experts, domain experts are the actors who know the most about the problem on the business side. Their role is to define the framework for the data science project, and hence they are a key participant in the process. A domain expert will translate business needs and characteristics to the data scientists and eventually judge the solution as successful or not by assessing whether the business objective has been achieved or not.

Did I get this?
Which statement below is true?

A Machine Learning Engineer's duties are close to the domain-side of the project.

A data scientist should work with a domain expert to adequately understand the business need.

A domain expert and business analyst are essentially the same role as they both understand the business processes of a company.

Correct: The domain expert will help the data scientist frame the business needs.

The data analyst and data scientist typically perform similar tasks; they also should have the same skills.

True

False

Correct: This data scientist identifies trends and patterns within the data and makes predictions based on trends while the data analyst 's role is typically firmly rooted in the business domain and less technically proficient in systems programming and advanced machine learning.



The data science lifecycle structures the activities of the data science team. It should not be considered to be linear as there must be iterations of questioning and research involved in each phase. The framework consists of several major stages: The framework involves input from various members of the data science team, as well as the client, as stated below:

Business Understanding involves framing the objectives and assessing data science readiness. The client and data science team are involved in this stage to ensure that the analytic solutions meet the business objectives.

Data Acquisition involves gathering data from various appropriate sources. Data preparation techniques are employed to ensure the data is useful for analysis.

Modeling involves choosing the appropriate model for the problem (we see why business understanding comes first!) even though it is often mistaken as the first stage of the process. Modeling typically consists of feature engineering, algorithm selection, model training, and evaluation.

Deployment involves the implementation of the solution developed in the operating environment of the business. One should always remember that a business needs to measure the impact of the deployed solution to ensure the success of the solution.

The data science lifecycle can seem daunting at first, but a data scientist will not complete all the tasks alone. A productive team will consist of individuals with complementary skills filling various roles that can ensure the project is successfully executed.





Data collection is the process of gathering and organizing data that can meet defined business and analytic objectives. The data collection process in the data science lifecycle can be compared to the data collection process in scholarly research. Data collection should be conducted systematically to ensure that the data are valid and reliable. Data collection also involves attending to issues of validity, reliability, and ethics, such as provisions for informed consent from participants. We will explore these issues in upcoming modules.

As a professional who works with data, it is important to know where data come from and think about the analytical approaches that one will take to analyze data. Before going further into data analysis, we want to understand where the data that are provided to us come from or what approaches have been used to gather data for the study. We need to ask these questions to guide us in thinking about what process generated the data and the type of data that we may be working with or collecting. In general, there are two key types of data:

Organic or process data

Data collected from a designed study

Organic or process data are data that are generated by an automated computerized information system or extracted from images, video, or audio recordings. This type of data is generated organically as a result of some process continuously or over a period of time.

Examples of organic or process data:

Financial or stock market exchange transactions

Web browser history

Web or mobile application activity history

Netflix viewing history

Surveillance camera video recordings

The term “big data” refers to these types of datasets comprising organically produced data from automated processes over time in massive quantities. . Data scientists mine these data to study trends and discover interesting relationships. But processing such massive quantities requires significant computational resources. Thus compiling and processing such massive quantities of data efficiently and getting them ready for analysis are exciting research and practice areas in and of itself.

Data collected from a designed study– as the name suggests– derives data from specific studies designed to address particular research topics. The main difference between this type of data and organic data is that data collected from a designed study are collected based on a certain guided agenda by theory, prior knowledge, literature, etc., in order to test particular hypotheses or to attempt to answer predetermined research questions.

Here are examples of data that can be collected from a designed study:

Questionnaires and surveys. Questionnaires are used to collect data from a group of individuals. Questionnaires can be administered on paper or online. In general, it might be easier to distribute questionnaires online as there are efficient tools that can analyze the collected data. Questionnaires can have open-ended, closed-ended, rating, Likert-scale, or multiple-choice questions. Data cleaning is still a consideration with questionnaire data as errors can occur. For example, responses to open-ended questions can contain misspellings, among other errors.

Interviews. Interviews are open-ended question-answering dialogs between an interviewer and one or more interviewees. Interviews are guided by an interview protocol designed to provide instructions for the interview process, the questions to be asked, and the space to take notes during the interview.

Observation is the process of gathering open-ended, firsthand information by observing people and places at a research site. Data collected during these observations can support or complement the data collected during interviews and from questionnaires.

Focus groups can be used to collect shared understanding from several individuals as well as to get views from specific people. A focus group interview is a process of collecting data through interviews with a group of people, typically four to six. The researcher asks a small number of general questions and elicits responses from all individuals in the group. Focus groups are advantageous when the interaction among interviewees will likely yield the best information and when interviewees are similar to and cooperative with each other.

As you may have noticed from the provided examples, this type of data arises from a design data collection rather than organically from an automated process. So instead of knowing what a user’s Netflix viewing habits are over time, we might want to ask a group of individuals that are sampled from all the Netflix viewers and then interview or survey them about their opinions on a particular topic such as a new pilot feature or a newly added movie.


Figure 1: Drawing a representative sample from the population. (Source: https://www.voxco.com/ )

In figure 1, we have a population of interest but we draw a representative sample of individuals from that population because it is usually difficult to measure everyone from that targeted population. There is another way to leverage process data in a designed study: we specifically design a way to extract a subset of the massive quantity of process data collected that can serve the purpose of the study design and research objectives.

You can appreciate the key differences in the data collected through a designed study. Such data are very rigorously designed data collections that people might be interested in looking at, as opposed to just large data sets that arise organically.

Did I get this?
Which of the following best describes the term "organic data" in the context of data collection?

Organic data refer to data that arise organically from an automated process over a period of time, such as those generated by a computerized information system or extracted from images, videos, or audio recordings.

Organic data typically refer to datasets that are difficult to measure from the targeted population due to their vast size.

Organic data are primarily collected through methods like questionnaires, interviews, observations, and focus groups.

Organic data are data collected from a designed study, based on a certain guided agenda by theory, prior knowledge, literature, etc.

Correct. Organic data, also known as process data, are generated organically as a result of some process continuously or over a period of time. They are typically generated by automated computerized information systems or extracted from images, videos, or audio recordings. Examples include financial transactions, web browser history, app usage history, viewing history, and surveillance camera video recordings.




To connect the study’s objectives with the data gathered, data scientists need to come up with a sound study design for execution while ensuring the validity and integrity of the inference. There is a range of study design options, from the exploratory analysis of data that is organically available, to those very highly planned efforts for collecting and analyzing data aligned to a specific question. Study design encompasses everything in preparation for data-driven research. A study can fall into multiple categories of study designs.

Exploratory

Confirmatory

Bottom-up (without a pre-specified question)

Can lead to knowledge discovery or new theory

Uses inductive logic and the logic of discovery

Top-down (with a specified falsifiable hypothesis)

Tests an existing theory

Use deductive logic, the logic of justification, and reconstructed logic

Comparative

Non-Comparative

Contrasts one subject with another based on certain measures.

Estimates or predicts absolute outcomes of the certain subject matter without explicitly making a comparison with its counterpart.

Experimental

Observational

The purpose of experiments is to compare responses of subjects to some outcome measures, under different conditions. Those conditions are levels of a variable that can influence the outcomes. The data scientist has the experimental control of being able to assign subjects to the conditions.

To conduct experiments, there is often a plan of manipulation or assignment of the subjects to treatment. These are called experimental designs.

Good experimental designs use randomization to determine which treatment a subject receives.

The purpose of observational studies is to draw inferences about the effect of an “exposure” or intervention on subjects, where the assignment of subjects to groups is observed rather than manipulated (e.g., through randomization) by the data scientist.

Observational research involves the direct observation of individuals in their natural settings. As such, who does or does not receive intervention is determined by individual preferences, practice patterns, or policy decisions.

It is therefore important for readers of observational research to consider if alternative explanations for study results exist.

Establishing causal inference is central to science. However, it is not possible to establish cause and effect relationships definitively with a nonexperimental study, whether it be an observational study with an available sample or a sample survey using random sampling. With an observational study, there is a strong possibility that the sample is not representative of the population. With an observational study or a survey, there is always the possibility that some unmeasured variable could be responsible for patterns observed in the data. With a well-designed experiment that randomly assigns subjects to treatments, those treatments should roughly balance any unmeasured variables. Because a randomized experiment balances the groups being compared on other factors, it is possible to study causal inference more accurately with an experiment than with an observational study. Observational studies are more passive and self-selected as subjects are exposed to a condition rather than being assigned. However, when the random assignment in experimental design is impractical or unethical, observational studies are the next best bet.

In general, more data is better because more data yields more information. However, the manner in which data is collected is arguably more important than the availability of that data itself. If the data that is being collected has too little information to inform about the questions of interest, then the resulting conclusions may not be very informative. Power analysis is a process by which we can assess whether a given study design is likely to yield meaningful findings. Bias is another issue that can result from an unfair sampling of a population, or when measurements are systematically inaccurate on average. In the next section, we will address the issue of bias and validity of a study.

Did I get this?
Which of the following statements is necessarily true about "data collected from a designed study"?

Data collected from a designed study comprise massive, messy datasets that require significant computational resources to process.

Data collected from a designed study are generated organically and continuously over a period of time.

Data collected from a designed study are collected based on a certain guided agenda by theory, prior knowledge, literature, etc., in order to test particular hypotheses or to answer predetermined research questions.

Data collected from a designed study are generated by automated computerized systems or extracted from images, videos, or audio recordings.

Correct. Data collected from a designed study are indeed collected based on a guided agenda. They are derived from specific studies designed to address particular research topics.
It is critical in any research to have a clear and unambiguous definition of the population of interest. That is, it pays to be explicit, rather than vague, about the nature of the population we are interested in studying. Doing so will ensure proper inference or conclusions about the data we study. Regardless of the study design, any analysis could suffer from potential incorrect actions taken in any part of the data science lifecycle.

Validity measures how much the intended test interpretation (of the concept or construct that the test is assumed to measure) matches the proposed purpose of the test. This evidence leading to the assessment of validity is based on test content, response processes, internal structure, relations to other variables, and the consequences of testing.

Threats to validity refer to specific reasons for why we can be wrong when we make an inference in an experiment because of covariance, causation constructs, or whether the causal relationship holds over variations in persons, setting, treatments, and outcomes. In an observational study, the threat to validity can arise by the inability to account for whether the observed changes can be attributed to the exposure or intervention and not to other causes and whether we can generalize that exposure more universally causes the outcomes.

There are four types of validity:

Statistical conclusion validity refers to the appropriate use of statistics (e.g., violating statistical assumptions, restricted range on a variable, low power) to infer whether the presumed independent and dependent variables covary in the experiment.

Construct validity refers to the validity of inferences about the constructs (or variables) in the study.

Internal validity relates to the validity of inferences drawn about the cause-and-effect relationship between the independent and dependent variables.

External validity refers to the validity of the cause and effect relationship being generalizable to other persons, settings, treatment variables, and measures.

In this module, we will discuss external and internal validity in more detail.

External Validity
As data scientists, once we have defined the population of interest for the study, we must work hard to ensure that the data we will collect or the data given to us is representative of that population. For example, to investigate the impact of class size on high school student achievement, we need to decide whether it is possible to obtain a simple random sample of students from the population of students who are enrolling in formal education institutions in the United States. Alternatively, we might decide that we only want to study students in public schools, private schools, charter schools, etc., or that we want to study all high school students regardless of age. No matter what the sampling plan is, it is critical that the data we use are a representative sample of the population we want to study. Doing so is crucial to ensure the external validity of the study. External validity refers to the ability to generalize the findings or results to a known population of interest. Threats to external validity are problems that threaten our ability to draw correct inferences from the sample data to other persons, settings, treatment variables, and measures.

Sampling bias is bias in which data is collected in a way that some members or groups of members in a population are systematically more likely or less likely to be selected in a sample than others. Sampling bias results in discriminatory data with over- or under-represented instances that are related to the study design or data collection method and can occur in both probabilistic and nonprobabilistic sampling. A study measuring the completion rate of graduate students in the United States with a sample of students from one socioeconomic background, race, or gender will undermine the external validity of that study. This means the results of the study can not be truly generalized to the entire population of graduate students in the United States.

Internal Validity
As data scientists, we want to conduct sound research that produces meaningful, impactful, or novel results for stakeholders. To produce such results, we need to ensure confidence in the ability to draw inferences from the data about the population of interest established in the study after ruling out any alternative explanations. Failure to do so would result in internal validity threats. Threats to internal validity are problems in drawing correct inferences about whether the covariation (i.e., the variation in one variable contributes to the variation in the other variable) between the presumed treatment variable and the outcome reflects a causal relationship.

Table 1 (adapted from Creswell (2012)) displays the threats to internal validity, their descriptions, and suggestions for data scientists to avoid such a threat.

Table 1. Threats to Internal Validity.

Type of Threat to Internal Validity
Description
Suggested response by Data Scientist

History

Time passes between the beginning of the experiment and the end, and events may occur between the pre-test and post-test that influence the outcome. In educational experiments, it is impossible to have a tightly controlled environment and monitor all events.

The data scientist can have the control and experimental groups experience the same activities (except for the treatment) during the experiment.

Maturation

Individuals develop or change during the experiment (i.e., become older, wiser, stronger, and more experienced), and these changes may affect their scores between the pre-test and post-test.

A careful selection of participants who mature or develop in a similar way for both the control and experimental groups helps guard against this problem.

Regression to the mean

Participants with extreme scores are selected for the experiment. Naturally, their scores will probably change during the experiment. Scores, over time, regress toward the mean.

The data scientist can select participants who do not have extreme scores as entering characteristics for the experiment.

Selection

Participants can be selected who have certain characteristics that predispose them to have certain outcomes (e.g., cognitive ability, receptiveness to treatment, or familiarity with a treatment)

Random selection may partly address this threat.

Mortality (also called study attrition)

Participants drop out during the experiment for any number of reasons, and drawing conclusions from scores may be difficult.

The data scientist can recruit a large sample to account for potential dropouts or compare the outcome of those who drop out with those who continue.

Diffusion of treatments (also called cross-contamination of groups)

Participants in the control and experimental groups communicate with each other. This communication can influence how both groups score on the outcomes.

The data scientist must keep the two groups as separate as possible during the experiment.

Compensatory equalization

When only the experimental group receives a treatment, an inequality exists that may threaten the validity of the study. The benefits (i.e., the goods or services believed to be desirable) of the experimental treatment need to be equally distributed among the groups in the study.

The data scientist can provide benefits to both groups, such as giving the control group the treatment after the experiment ends or giving the control group a different type of treatment during the experiment.

Compensatory rivalry

Participants in the control group feel that they are being devalued, as compared to the experimental group, because they do not experience the treatment.

The data scientist can try to avoid this threat by attempting to reduce the awareness and expectations of the presumed benefits of the experimental treatment.

Resentful demoralization

When a control group is used, individuals in this group may become resentful and demoralized because they perceive that they receive a less desirable treatment than other groups.

The data scientist can provide treatment to this group after the experiment has concluded or provide services equally attractive to the experimental treatment but not directed toward the same outcome as the treatment.

Testing

Participants become familiar with the outcome measure and remember responses for later testing

To overcome this threat, the data scientist can measure the outcome less frequently and use different items on the post-test than those used during earlier testing.

Instrumentation

The instrument changes between a pre-test and post-test, thus impacting the results of the outcome.

The data scientist must standardize procedures so that the same observational scales or instrument is used throughout the experiment.

Statistical Bias
Statistical bias is the bias that leads to a systematic discrepancy between the true parameters of the population of interest and the statistical features used to estimate those parameters. Bias made can be consciously or unconsciously, and it will affect the performance of a data science model but, most importantly, the analytic solution and the decisions made after the implementation of that solution.

Statistical bias results from violations of external validity or internal validity of a study. In the previous module, we explored sampling bias that undermines external validity. In this module, we will explore additional common statistical biases that you need to be aware of and take into account during the data understanding process.

Selection Bias, a threat to internal validity, occurs when there is a mismatch between the data selected for the study and the subject matter that the data scientist wants to make inferences about. Selection bias is usually a concern of studies using convenience samples.

Self-selection Bias occurs when individuals select themselves to be included in a study. Self-selection bias is a threat to the external validity of the study since such bias is usually untrollable during the data collection phase. Self-selection bias is often associated with certain characteristics of the sample that induce such individuals to be included in the resulting study sample. Take the example of a survey. If the response rate of a survey is not perfect, it is likely that certain characteristics of those individuals are related to the reason why they responded to the survey.

Confirmation Bias. Your prior knowledge, beliefs, and values can play a role in the data that is used to build your analytic solution. This is because, as humans, we are prone to use our personal beliefs and experiences to guide us through daily life and decision-making. This type of bias occurs when we favor evidence that confirms our personal beliefs, values, and hypotheses.

Information Bias. Also known as measurement bias, it occurs when data is collected, measured, or interpreted wrongly. Misclassification of observations is an example of information bias. For example, an observation with attributes similar to the stereotypical female student is recorded as female when that observation is actually from a male student. Another example is the misclassification of patients. In the context of the COVID-19 pandemic, groups under the age of 45 are seen as low risk. o during a screening exercise, those in that age group might not be screened and therefore classified as negative. The data collected then has a misclassification bias and is not accurate. One way to control information bias is to implement blinding.

Confounding Bias occurs when incorrect inferences are made about the subject matters while failing to account for a potentially confounding variable, an exogenous factor that causes the subject matters of interest.

Did I get this?
Confounding variables are primarily a threat to the:

Statistical significance of the result.

External validity of a study.

Generalizability of the result.

Internal validity of a study.

Correct. Confounding variables cause confounding bias in a study because these variables other than the independent variable co-varies with the independent variable and may be an alternative cause of the dependent variable. This is a threat to the internal validity of the study.

Match each type of bias in the context of a study investigating the effectiveness of a new nutritional supplement on athlete performance, with its most appropriate definition/description.

Types of Bias:

1. Selection Bias

2. Self-selection Bias

3. Confirmation Bias

4. Information Bias

5. Confounding Bias

1.
The type of bias that occurs when there is a mismatch between the data selected for the study and the population the researchers want to make inferences about. For example, the study might unintentionally include only athletes from one sport.

2.
The type of bias that occurs when individuals select themselves to be included in a study, possibly because they expect to benefit from the supplement, leading to a sample that may not be representative of the general athlete population.

3.
The type of bias where researchers subconsciously favor evidence that supports their hypothesis, such as giving more weight to results showing an improvement in performance after taking the supplement.

4.
The type of bias that occurs when data is collected, measured, or interpreted incorrectly. For example, athletes' performance may be inaccurately measured due to faulty equipment or errors in recording results.

5.
The type of bias that occurs when incorrect inferences are made about the subject matter while failing to account for a potentially confounding variable, like the athlete's training regimen or dietary habits, which may also affect performance.

Submit
Reset
Correct.

Selection bias refers to the systematic differences between the selected sample and the population, thus E is the right match for Selection Bias.

Self-selection bias occurs when individuals select themselves into a group causing a biased or unrepresentative sample. Therefore, D matches with Self-selection Bias.

Confirmation bias is the tendency to search for, interpret, favor, and recall information in a way that confirms one's preexisting beliefs or hypotheses. Thus, C matches Confirmation Bias.

Information bias is a type of bias that occurs due to the incorrect collection, recording, or interpretation of data, making A the correct match for Information Bias.

Confounding bias occurs when the researcher does not take into account an exogenous variable that is affecting the observed relationships, so B is the right match for Confounding Bias.




Data collection is the process of gathering and organizing data that can meet defined business and analytic objectives. Data collection also consists of attending to issues of validity, reliability, and ethics, such as provisions for informed consent from participants.

Data scientists need to develop a sound study design for execution while ensuring the validity and integrity of the inference. There is a range of study design options, from an exploratory analysis of data that is organically available to highly planned efforts. The manner in which data is collected is arguably more important than the availability of that data itself.

Validity is the development of sound evidence to demonstrate the intended test interpretation. In an observational study, the threat to validity concerns whether the observed changes can be attributed to the exposure or intervention and not to other causes and whether we can generalize that exposure more universally causes the outcomes.

Threats to internal validity are problems in drawing correct inferences about whether the covariation between the presumed treatment variable and the outcome reflects a causal relationship.

Threats to external validity are problems that threaten our ability to draw correct inferences from the sample data to other persons, settings, treatment variables, and measures.

Statistical bias results from violations of external validity or internal validity of a study. Common statistical biases that you need to be aware of and take into account during the data understanding process are:

Selection Bias: a threat to internal validity occurs when there is a mismatch between the data selected for the study and the subject matter that the data scientist wants to make inferences about.

Self-selection Bias: If the response rate of a survey is not perfect, it is likely that certain characteristics of those individuals are related to the reason why they responded to the survey.

Confirmation Bias: This type of bias occurs when we favor evidence that confirms our personal beliefs, values, and hypotheses.

Information Bias: The data collected has a misclassification bias and is not accurate. One way to control information bias is to implement blinding.

Confounding Bias: occurs when incorrect inferences are made about the subject matter while failing to account for a potentially confounding variable.



As we practice data science, it is important that we do so with an ethical framework in mind. There are many ways we can cause unintended negative consequences through our work as data scientists if we blindly focus on the techniques we learn without considering their consequences. In this module, we will talk about how we could potentially make mistakes and how we can avoid making these mistakes.

Ethics
Let’s start with a simple definition of ethics. Ethics are rules that we, as fellow citizens, voluntarily follow because we believe they make the world a better place. Ethics guide us to distinguish right from wrong and are at the bedrock of human civilization.

Consider this simple example: Ethics stop you from shoplifting. There may be other things that could stop you from shoplifting, such as the possibility of being caught by the store owner, but ethics would stop you even if there is no one in the store to catch you. What stops you is the fundamental principle of what you think is right. Ethics are the shared values of the society that compel us to think about how we want to be treated, which then translates into how we treat others.

Ethics are distinct from the law. Consider an example where you tell a friend a secret, and the friend promises not to tell anyone else. The friend then breaks that promise and spreads your story. The friend may not have broken any law and is unlikely to face any legal consequences. Nonetheless, we can agree that the friend’s behavior is unethical because it is a common principle of human relationships and interactions to keep one’s promises, especially to one’s own friends.

Ethics are not laws, but laws often follow the shared social values of ethics. Laws are created and enforced to ensure these common social values. However, despite having generally agreed upon shared values, not everyone will act in accordance with those values, either because they don’t share them or because they’ve justified to themselves in some way that they’re not truly violating them. For example, just because we have a shared principle saying it is wrong to shoplift doesn’t mean there are no shoplifters in society. It merely means that we all have agreed that stealing is wrong as a society and can therefore pass legislation that punishes thieves. These laws are then enforced by our criminal justice system, which also determines the punishments for those who violate them.

The Tragedy of the Commons
One practical way to think about the benefit of ethics is through the lens of economics. In general, society as a whole does best when individuals maximize their own private returns. This is the fundamental principle of a modern free-market economy. However, there are situations where we see that the results of collective actions from individuals under the name of maximizing private benefits are a detriment to society.

When individuals act in their own interest while collectively accessing a public resource (called a commons), it is likely they will ultimately deplete or otherwise corrupt that resource. This situation is known as the tragedy of the commons, a term used for the first time by Garret Hardin in 1968 and published in Science Magazine (requires CMU credentials to access). The tragedy of the commons stresses that “the population problem has no technical solution; it requires a fundamental extension in morality,” in which “population” refers to members of society and “extension in morality” refers to ethical principles. It explains individuals’ tendency to make decisions based on selfish needs that sometimes lead them to ignore the best interests of the group as a whole– of which they are a part themselves.

In 2009, Elinor Ostrom became the first woman to be awarded the Nobel Memorial Prize in Economic Sciences for revisiting the tragedy of the commons and her work on governing the commons (requires CMU credentials to access). She showed cases where such “tragedy” was not inevitable and that if humans cooperated with one another and monitored and enforced rules to manage the commons, then the “tragedy” could be avoided.

Now that we’ve learned about what ethics are and how together we can avoid unintended negative consequences from selfish and unethical practices, in the subsequent modules, we will link this with data science and demonstrate the core values of ethical data science practices to motivate the discussion of how data scientists can apply ethical principles.

Did I get this?
What does the term "Tragedy of the Commons" refer to in the context of economics and ethics?

The scenario where collective actions by individuals to maximize their own benefits lead to the depletion or corruption of a shared public resource.

The tragedy that occurs when individuals fail to share their privately-owned resources with the wider society.

The impossibility of avoiding ethical dilemmas in economic decision-making.

The inability of a society to provide public resources due to economic constraints.

Correct. This option accurately describes the "Tragedy of the Commons". When individuals act in their own interest while collectively accessing a public resource, they can deplete or corrupt that resource, leading to negative consequences for the society.



Introduction to Data Science Ethics
For much of human civilization, people did the best they could to thrive under challenging circumstances. These were times when there weren’t many rules to follow, so people did and took what they could and weren’t exactly ethical, but arguably this ethic was necessary to survive difficult times. That is also where we see the situation of the tragedy of the commons, mentioned in the previous module. We also saw that the tragedy could be overcome – that is, we can begin to produce a better civilization for all. In this improved civilization, we no longer do things just because we can or feel we have to. We now have to follow rules of ethics that give us more benefits collectively than they cost us.

We have virtually unlimited access to data as data scientists today, and we have unprecedented analytical techniques with which to analyze that data. So the question we should be thinking about is whether we should do something just because it is technically possible. Are there things that are possible to do but which we can agree would not be right to do? This may sound strange, but it is a question that modern science has already begun considering and continues to do so.

Think about how data science creates impacts and the tremendous excitement about how data science applications provide better ways of doing things in society. Think about the power you have as a data scientist and how that power influences people’s lives, potentially for the better. With that great power comes great responsibility. It is crucial as data scientists that we be responsible when exercising that great power.

The difficult thing about being an ethical data scientist is not about understanding ethics. It is the connection to how one applies ethical principles to data science practice; it is about doing “good” or ethical data science.

Doing Good Data Science

By Rijksdienst voor het Cultureel Erfgoed, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=23391637

This is the Amsterdam Civil Registry Office in the Netherlands. Buildings like these were record offices that contained cabinets full of records about the Dutch population. During World War II, following the Nazi invasion and occupation of the Netherlands in 1940, the Nazis took over the registry office and used these records to identify members of the Dutch resistance. The records about their birth were used as a potent weapon to identify members of the Dutch population who were Jewish and to therefore decide whom to send to extermination camps.

The Dutch resistance saw that the mere existence of the Civil Registry Office was dangerous in the hands of the Nazi war machine. So they planned a bombing of the Office in 1943 in an effort to destroy records that would help the Nazis identify Jews. Figure 2 is of a plaque in Amsterdam that lists the names of the people in the Dutch Resistance who bombed this record office. They were the ones that identified links between these records and what was happening in the war, took actions to prevent it, and paid the price. They were captured and executed.


By Design: Willem Sandberg. Photographer: Frans Willemsen - Own work, CC BY-SA 3.0 nl, https://commons.wikimedia.org/w/index.php?curid=32468279

We hope this is a good place to start the conversation about doing good data science and the motivation to do so. In the rest of the module, we hope to convince you, if you’re not already convinced, that the story of the 1943 bombing of the Amsterdam civil registry office is highly relevant to the work you do as a data scientist. While the data science work you do may be mundane by comparison and not involve a life-or-death decision, ethical data science practices matter to everyday things that affect all of us. The goal of this module is to help you participate in the ethical debates that you will face as a data scientist, infuse your data science work with ethical principles, and inform you to be thoughtful, deliberate, and ethical – the kind of data scientists that we all hope that you’re going to be.

The Five Cs
[Required Reading]

Please read chapter 3 from Loukides, Mason, H., & Patil, D. (2018). Ethics and Data Science (1st edition). O’Reilly Media, Inc.

Note: When prompted to select institution, select "Not listed? Click here" and enter your CMU email address to access content.

Did I get this?
Which of the following scenarios best illustrates the concept of the Five Cs: consent, clarity, consistency, control, and consequences in data ethics?

A smartphone application is designed for sharing photos and videos. The app is programmed to gather and store users' location data. While this isn't expressly communicated to the users during their interaction with the app, this pertinent detail is subtly embedded in the terms and conditions that users agree to during installation. The app is prepared to admit to this data collection practice only when a user explicitly inquires about it.

A medical research firm provides clear guidelines on what health data will be collected from participants, ensures consistent handling of data over time, allows individuals to control the use of their data, assesses and communicates the potential harm from the use of data, and obtains explicit consent from the participants.

A company collects user data without explicit consent and sells it to third parties for advertising purposes.

A social media platform collects user data and uses it to tailor advertisements, but does not allow users to opt out of data collection.

Correct. This scenario encompasses all Five Cs: Consent is obtained from the participants; clarity is provided regarding the data being collected; consistency is maintained in the handling of data; control is granted to the individuals over their data; consequences, in terms of potential harm from the use of data, are considered.

Which of the following represents the Five Cs as metnioned in the data science ethics framework in Loukides et al. (2018)

Compliance, Curation, Cybersecurity, Control, and Confidentiality

Consent, Compliance, Consistency, Confidentiality, and Curation

Consent, Clarity, Consistency, Control, and Consequences

Confidentiality, Clarity, Consistency, Control, and Calculations

Correct.




Ethical practices matter in data science because of their impact on human well-being and society at large. When it comes to human subjects research, the concept of informed consent is critical because it involves the right of the individual to know that they are being studied and the right to know how they are being studied. In this module, we will explore the concept of informed consent. First, we will explore what human subject research means, then we will learn about the link to the evolution of informed consent.

Human Subjects Research
Begun in 1932, a study conducted by the United States Public Health Service (USPHS) at Tuskegee University and funded by the Centers for Disease Control (CDC), investigated the cause and development of untreated latent syphilis. Some 399 African American men in Alabama who had syphilis were recruited and matched against 201 uninfected subjects who served as a control group.


Figure 1. Scenes from the Tuskegee Syphilis Study. (Source: https://www.rmpbs.org/)

The subjects were instructed to make regular visits to the clinic, where they would be given a health exam, care for minor medical issues, and a hot meal. The participants were enrolled without their informed consent to a “special free treatment,” which was actually intended to study the neurological effects of syphilis.

By the 1950s, when it became clear that penicillin, an antibiotic drug, was a safe and effective treatment for syphilis, the subjects were either prevented from knowing about it or denied such treatment. No subjects were treated with penicillin. The study continued until 1972, when the Department of Health, Education, and Welfare (HEW) terminated the experiment after accounts of the study appeared in the national press, driven by some whistleblowers. At that time, 74 of the test subjects were still alive. An investigatory panel appointed by HEW in August 1972 found the harm being done by the study was “ethically unjustified” and stated that penicillin should have been used to treat the men. As a result, the National Research Act mandated that all federally funded proposed research with human subjects be approved by an institutional review board (IRB). The IRBs monitor a process called informed consent. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.

In the case of the Tuskegee Study of Untreated Syphilis in the African American Male, the subjects were not informed about the study of neurological effects of syphilis. In addition, they were misinformed about possible treatments for syphilis and were told that syphilis could not be treated. The subjects did willingly consent to the experiment, but their consent was not properly informed, and it was not clear if the researcher told the subjects that they had the right to withdraw their consent at any time.

The case here is that the researcher was evaluating the benefit to society or science versus the harm to the participants. A fundamental principle of informed consent is that the party facing potential harm has the right to decide on their own the balance between the benefit to society, as well as any compensation they are receiving from the experiment, and the risk of harm they face. Since full details of the potential harm and benefits are often very complex, it can be nontrivial for the human subject to be fully informed of them. For this reason, an IRB would come in, determine if the study is just and ethical, and ensure that the informed consent principles are appropriately followed.

Informed Consent
Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. It is important to note that, progressive as it may appear, there are still limitations to the principle of informed consent. Informed consent was developed in the context of research that would be conducted on human subjects to collect data prospectively. In today’s data science practices and applications, informed is usually something that is hidden in numerous pages of fine print, and users are required to say “I accept” before the process can begin. From an ethical point of view, setting aside the law, there is a consensus that claiming that somebody has been informed because they were given many pages of fine print to read without an actual opportunity to read them is an unethical means of obtaining consent. The concept of voluntary is also questionable, as consent is being obtained precisely when a user already intends to use a service or technology. Users, in these cases, are typically not given the information well in advance, providing them with adequate time to understand the risks or terms prior to consenting.

There is also a question of what the data will actually be used for once consent has been obtained. For example, a user may consent to give data about themselves to a merchant for a specific service, but it does not mean that the data is authorized to be repurposed. Not all repurposing of data is unethical. On the contrary, repurposing data can bring significant benefits to society in the case of medical data of one patient being studied to help future patients. One caveat here is that, in many cases, what is intended to be studied comes after the data has been collected. Physicians and medical researchers may not know the questions to be asked when data is being collected; they simply know that more information would help. This type of research is called retrospective data analysis.

In terms of informed consent, the problem here is how to inform subjects exactly what they are consenting to while at the same time making it comprehensive enough to include potential research questions that one might ask. So, again, this is a crucial question for conducting meaningful and ethical data science research.

Did I get this?
The Tuskegee Syphilis Study was deemed to be “ethically unjust” by the Department of Health, Education, and Welfare (HEW) because:

The subjects were forced to participate in the study.

The study proceeds without approval from an available existing institutional review board (IRB).

The subjects were not treated for syphilis when treatments were available.

The subjects were not compensated for participation in the experiment.

The subjects were not aware of all the intentions of the study.

Submit
Reset
Correct. When it became clear that penicillin was found to be a safe and effective treatment for syphilis, the subjects were either prevented from knowing about it or denied such treatment.

Correct. The subjects were not aware that the researchers were studying the neurological effects of syphilis.


Data governance defines how data is accessed and managed within an organization. It is important because it facilitates effective data management and has positive implications for the quality, security, and integrity of data used for analysis. An organization that handles data efficiently understands that data governance impacts data quality and the decisions made from the data available to the organization. This unit focuses on data governance as a component of data management and how it influences the development of analytic solutions in an organization and its decision-making. Any organization that stores and utilizes data should have a data governance strategy for internal data, or data stored within the organization, and external data.

Data governance is beneficial because it provides a reliable and consistent view of enterprise-wide data. It ensures that there is a plan for improved quality of data, maps the location of data in the enterprise, reduces the scourge of data silos, and improves data management overall.

Reading: Data Governance in the Cloud.

Data governance is the responsibility of an entire organization; although it is often administered by the data management team, all users of data in an organization are considered stakeholders of the organization's data. The Data Governance Institute defines a stakeholder as an individual or group that makes or is affected by data-driven decisions within an organization. In addition to data governance policies, data stakeholders will have an influence on the state and use of data. As a quick reminder, the data science team works with different individuals in an organization to define business and analytic objectives during the data science project lifecycle, as well as to determine the requirements for the analytic solution. So why is data governance important to a member of a data science project team?

Data governance is more than just policymaking for data. Iit influences business strategy because data are now (more than ever) considered an asset to an organization. An organization that has embedded data governance principles into its data infrastructure or data management framework is an organization able to abide by data standards (whether industry-set or company-defined).

Data governance best practices for organizations are met when data has integrity, and data-related decisions and controls are transparent and can be audited. Another best practice gaining ground in the industry is for organizations to collect and store data that is unbiased. Unbiased data means something different to each organization and industry, but the general idea is that the data represent all members of a population that could be served by an organization. This best practice will positively influence the development of ethical models and algorithms for analytic solutions.

Reading: Data Governance and Its Implications for Ethical Models.

What is Data Governance Used for?
Data governance is necessary to ensure that data is safe, secure, private, usable, and in compliance with both internal and external data policies. Data governance allows setting and enforcing controls that allow greater access to data, gaining security and privacy from the controls on data. Some common use cases include:

Data stewardship. Data governance often means giving accountability and responsibility for both the data itself and the processes that ensure its proper use to “data stewards.”

Data quality. Data governance is also used to ensure data quality, which refers to any activities or techniques designed to make sure data is suitable to be used. Data quality is generally judged on six dimensions: accuracy, completeness, consistency, timeliness, validity, and uniqueness.

Data management. This is a broad concept encompassing all aspects of managing data as an enterprise asset, from collection and storage to usage and oversight, making sure that data are leveraged securely, efficiently, and cost-effectively before they are disposed of.



While data science often includes descriptive analysis (explaining what is actually happening), the prevalence of prescriptive analytics (explaining what needs to be done) continues to grow. As everything grows increasingly computerized and automated, data science has now become something that drives decision-making, sometimes without any input from a human. As these fully autonomous systems are entrusted with ever-greater responsibilities, unintentional and sometimes disastrous results can occur. We focus particularly on large automated systems because that is usually where the question of responsibility is most difficult or most consequential. To properly deal with the question of responsibility, we need to ask: How can we effectively control large automated systems?

When discussing accountability, we are not specifically looking for someone to blame when something goes wrong. What we strive for is for the systems we design to do what they are supposed to and do so responsibly and ethically, according to values or principles we wish them to adhere to. So, as data scientists, we carry considerable responsibility for any ethical failures of the systems we’ve engineered. When thinking about accountability mechanisms, we need to think about who specifically carries that responsibility. Responsibility is usually personal, but it can also be organizational.

Madeleine Elish wrote a very fascinating article in 2016 titled Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction . In this article, she introduces the concept of the moral crumple zone to describe how systems are designed to absorb the responsibility of human actors who have limited control over the behavior of an automated or autonomous system. Taken from the concept of automotive crumple zones, which are designed to be destroyed in an accident, absorbing the force of the impact and protecting the passengers, the moral crumple zone protects the integrity of the system, at the expense of the nearest human operator. Elish uses the idea of the moral crumple zone pejoratively, saying that someone is picked in advance to be blamed in the event that something goes badly wrong. If the system is designed to have someone intervene if something happens, it is the person’s responsibility to intervene and prevent the worst from happening.

The key takeaway from this section is that accountability is not merely about finding who is to blame; blame can be engineered and planned in advance. Accountability is about all the little decisions made by a group of people who created a system, at each step of the way. There are both ex-ante and post hoc accountability, sometimes called anticipatory versus remedial. We need to think about accountability before something goes wrong and design a system for someone whom we are responsible for. If something goes wrong, we also need to figure out what went wrong and ensure it doesn’t happen again. We care about both.

Transparency
In the previous section, we talked about how data governance in an organization must embrace principles of transparency and auditability when making decisions about data. Accountability in the decision-making process is attained by designing and implementing data systems that are transparent and auditable. We will dive deeper into what those descriptors mean in this section.

When an ethical concern arises in a data science solution, transparency means disclosing the involvement and actions of human actors, the data being used and its source, the algorithms being used and their intent, or sometimes, the very presence of data science or AI solutions in the product or service in the first place.

In the past, data scientists have used human involvement or the lack of human involvement to justify the outcome of a data science solution. As the data science field progresses and AI applications become more prominent in our daily lives, governments, regulators, and users have all called for more transparency. Initiatives such as “Why am I seeing this ad?” (Figure 1) is a progressive step toward solving the challenge of transparency in products and services.



Figure 2 shows the flow of control of “Why am I seeing this ad?” on LinkedIn , how the matching and standardization modules work in the backend, and how the results are displayed to the users. Although it is unlikely that all users of LinkedIn would go through the details of the algorithm that decides which ads to show, it is LinkedIn’s effort to ensure transparency and control to members. More importantly, such transparency could also ensure that the system can be easily audited, if necessary.

Limitations of the Transparency Ideal

The title of this section is taken from a 2016 article by Mike Ananny and Kate Crawford . In this article, the authors challenge the ideal of transparency, its limitations, and alternative strategies for algorithmic accountability, exploring the following tenets:

Transparency can be disconnected from power

Transparency can be harmful

Transparency can intentionally occlude

Transparency can create false binaries

Transparency can invoke neoliberal models of agency

Transparency does not necessarily build trust

Transparency entails professional boundary work

Transparency can privilege seeing over understanding

Transparency has technical limitations

Transparency has temporal limitations

Besides these limitations, other pitfalls of “reverse-engineered” as a strategy for transparency:

Set reasonable expectations to disclose what is known. While we may say that we need to understand and disclose how a data science solution works, there is a chance that we really don’t know exactly how it works. We don’t know what we don’t know. When a customer wonders why their favorite product is being discontinued, it may not be known exactly why this decision was made. The decision-maker could have been acting logically but could also have been acting illogically, with no clear explanation for his or her decision.

"Complexity distributes responsibility" - Joseph Weizenbaum (1976). When a computer program gets to a certain level of complexity, it is difficult or even impossible to identify who is responsible for which component of the system. This is the matter of ex-ante versus post hoc accountability, as mentioned in the previous section. As data science projects become increasingly complex, there are limitations on deciding who should be responsible when something goes wrong. This is a post hoc accountability matter. In this case, it is much more relevant and sensible to think about ex-ante accountability. That is, we all understand that it might be difficult to trace back who is responsible for the ethical failure, so everyone in a team needs to embrace ethical practices individually to avoid mistakes.

Without a critical audience, algorithms cannot be held accountable. Transparency is not a one-way street. It requires disclosures from the data scientist as well as critical audiences that take in the information and respond to it. Transparency without a proper audience is meaningless. More importantly, without a proper audience, transparency can lead to even greater ethical failures: it can mean knowing about an ethical failure without taking any action to prevent or remedy it.

Auditing
Another component of accountability in data science is auditing. Auditing has been used in social science research as an experimental test to discover if a system is doing what it was intended to do and whether it results in desirable or undesirable consequences. Auditing in social science was developed in the 1970s by economists at the Department of Housing and Urban Development (HUD) to investigate racial discrimination in housing. The experiment involved sending two people, one black and one white, to apply for the same apartment at the same time, and seeing whether the landlord gave the two applicants different answers. They repeated this process many times. If the results showed that there was a pattern of landlords responding in favor of one group over the other, then this could be considered evidence of racial discrimination.

Auditing has been used in the United States to diagnose employment and racial discrimination. A famous field experiment on labor market discrimination is another example of how audit studies were conducted. The experimenter used made-up names that were likely to be associated with a particular race or gender and sent mocked-up resumes to employers using these names to see whether the applicants from ostensibly different groups received callbacks at different rates. What they found was that even with exactly the same resume, people received callbacks at different rates depending on their names.

In data science applications, when transparency might not be feasible due to the protection of trade secrets or prevention of the system being gamed by bad actors, auditing is a counterpart to transparency for accountability. Auditing can be performed by an internal team whose job is to think through security vulnerabilities within their own organization. Auditing can also be performed by an external party to test whether the system is doing any harm.

Did I get this?
Amazon built a model to score job applicants. However, despite gender not being used as a feature, the system was found to have an inherent bias against female applicants. As a result, Amazon terminated this program.

What if Amazon did not terminate the use of the model, and there were complaints about discrimination in hiring decisions? If one wants to audit the system and reverse engineer externally, what potential problem will make this approach unsuccessful?

In this "black box" model, only the output is observable.

External audits cannot be attempted against Amazon because they didn't make the codes available to the public.

One will not be able to determine intent.

Reverse engineering is no longer necessary if Amazon reveals the data being used.

None of the above.

Correct. External audits and reverse engineering are helpful in finding problems with the model. However, it is not possible to determine why such a system is producing such results (that is, intent).



One primary goal of data science is to generate specific conclusions based on the presentation of evidence or data. The term “data-driven” reflects the central role of information we have about the past in making such inferences, as evidence is always something that was captured in the past, even if it reflects beliefs, attitudes, or plans we have about the future. We try our best as data scientists to make predictions or prescriptions about the future–but we cannot capture information about the future in the present. It may sound obvious to point this out, but there are profound implications to considering this directionality of time. In an important sense, all of a data scientist’s work is bound up with information about the past. So is data science backward-looking?

For example, let’s say you have a longitudinal dataset about income and education and you want to make some inferences from it. If the data go back far enough, you will get to the time in history when women were either prohibited or otherwise discouraged from furthering their education. As a result, it was often difficult for them to get into many professions, and their incomes were often correspondingly low. Now, if you didn’t take that into account or consider it when creating your framing questions, you could end up with conclusions like “girls aren’t smart enough to go to college” or “women don’t like high-paying jobs.”

The importance of recognizing that these conclusions, while technically possible but maybe problematic to assume, is that in history, there may have been situations where discrimination existed against girls or women getting an education, or against them working in certain fields. As data scientists, we must take these factors into account when analyzing data about the past, so that they do not impact our conclusions in such a way as to reproduce or perpetuate these problems in the future.

One can also argue that data science is discriminatory, in a sense. The nature of data science entails making predictions, and classifications, and separating one group from another. The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result. If we are trying to infer or optimize some parameter based on our dataset, but the data contains far more examples from group A than from group B, our inference or optimization will be inherently skewed toward members of group A. For example, if your job is to analyze a group of nurses at a hospital in order to find the best performers, your model may find that most, or perhaps all, of the good nurses are females. If you were to take that model to predict the suitability of a candidate for a nurse position, you might wrongly decline a qualified male candidate.

As a data scientist, it is important to watch out for this bias toward what is most prevalent, or most “normal,” about a given dataset under analysis. Because minority subgroups often exist in our data, and because many data science techniques can be adversely affected by this data imbalance, we need to be aware of these possibilities, take steps to account for them, and work to ensure that our results are based on sound reasoning and not unwarranted assumptions.

Reading: Introduction section of "Raw Data" Is An Oxymoron by Lisa Gitleman and Virginia Jackson.

Case Study

The location, tier level, and capacity of all 6th grades in Boston for the 2016-17 academic year. Illustration by the Boston Area Research Initiative.

The school assignment algorithm implemented by the Boston Public School (BPS) starting in the 2014-2015 school year, using a school assignment policy to assign students to good schools as close as possible to their homes. The goal is to increase access to high-quality schools while reducing the commute. Researchers found that despite the program's implementation shortening students' distances and minimizing travel time to and from school, the system failed to allocate students to high-quality neighborhood schools due to the disparity in allocations of high-quality schools across Boston's neighborhoods. As a result, the new system had a disproportionately negative impact on minority families with limited access to high-quality schools in their own neighborhoods in the first place. While the program's intention was just, and reducing the distance to school and busing is desirable, the algorithm's design phase did not account for the limited quality-school availability among specific neighborhoods. The resulting algorithm could have been positive and supportive of the minority families but had a negative impact on them instead, in a way that could have been avoided with proper consideration and design.

Did I get this?
Amazon built a model to score job applicants. However, despite gender not being used as a feature, the system was found to have an inherent bias against female applicants. As a result, Amazon terminated this program.

You are one of the data scientists that are involved in the development of this program. You are concerned about the ethical program of the model. Which one of these statements is true?

You have no ethical responsibility if ethics isn't listed in your job description.

You have no ethical responsibility if your manager ultimately makes all the decisions.

You have no ethical responsibility if the model was already planned before you were hired.

You have no ethical responsibility if your job is technical.

None of these is true.

Correct. Ethically problematic systems are produced by individuals with good intentions. Being a technical person in the team, you have a crucial role to play in implementing ethical practices and identifying ethical problems in your model; everyone in the team is collectively accountable for decisions made by it.



Data scientists have virtually unlimited access to data and analytical techniques with which to analyze that data. As data scientists, we should be thinking about whether we should do something just because it is technically possible.

Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.

Data governance defines how data is accessed and managed within an organization. It is beneficial because it provides a reliable and consistent view of enterprise-wide data. It ensures that there is a plan for improved quality of data, reduces the scourge of data silos, and improves data management overall.

Accountability is about all the little decisions made by a group of people who created a system at each step of the way. There is both ex-ante and post hoc accountability, sometimes called anticipatory versus remedial. We need to think about accountability before something goes wrong - to design a system for someone whom we are responsible for.

Data scientists try their best to make predictions about the future based on the information in the present. In an important sense, all of a data scientist's work is bound up with information about the past. Data science involves making predictions and classifications and separating one group from another. The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result. As a data scientist, it is important to watch out for this bias toward what is most prevalent or most "normal" about a given dataset.

Did I get this?
Amazon built a model to score job applicants. However, despite gender not being used as a feature, the system was found to have an inherent bias against female applicants. As a result, Amazon terminated this program.

How would you identify "harm" in relation to the potential ethical problems that arise in this situation? Check all that apply.

Harm refers only to physical harm.

Harm is individual.

A lost job opportunity is a harm.

Termination of the program is a harm.

Reinforcing or producing negative ideas about gender is a harm.

Submit
Reset
Correct. "Harm" goes beyond physical harm. Instead, it refers to any adverse effect made to any individual or group of individuals as the result of an ethically-problematic model.







While it is incredibly difficult, if not impossible, to know the relative speeds of different operations on a computer, accessing and loading data from memory tends to be one of the slowest if poorly done. To better use memory, you need to understand that memory is not a singular shelf from which you can pull and place data. Instead, it operates as a series of increasingly higher shelves, with those shelves at the top being much more expensive but way faster to reach than the shelves at the bottom. This is known as a memory hierarchy; it serves as a great basic model of memory for performance-intensive applications like those found in data science.

The memory hierarchy can be visualized as a pyramid, where at the top lie the bits of memory that are the fastest to access but the most expensive, while those at the bottom are the slowest to access but the least expensive:
slow to fast:
network-connected storages 
traditional disk
flash disk 
main memory
cache
registers


With a higher level of abstraction as a Data Scientist, you may be interested in the next comparison:


With this hierarchy of memory, most computer systems will attempt to try to find some memory at the top level and then keep moving down until the datum is found. Once it is found, the system will then note the datum / the datum’s location in higher levels, with the hope that the datum, or the datum around it, will be useful later on. You’ve implemented this partially in P1.

As most of this management is done outside of your control, it can be frustrating to try to figure out how to make data access code faster. However, while the systems managing the data are opaque, it pays to instead focus on two simple facts that underlie all attempts at managing this hierarchy:

Temporal Locality: When you access some data on your computer once, you’re probably going to access it again in the near future. Thus, if you can keep the data you’re accessing to one small section at a time, you can achieve better performance.

Spatial Locality: When you access one element in an array or another complex data structure, you are probably going to access data next to it. Thus, the management system will not just store the data at your location, but also the data near your requested location for future quick access. Thus, if you can access data sequentially in memory, you can achieve better performance.

Following these two principles can help guide you towards faster code in general. For example, consider the following two pseudocode sections:

Section 1
sum = 0
for index from 0 to 99:
    sum = sum + array[index]
return sum
Section 2
sum = 0
list_of_numbers = [0,2,...,99]
for index from 0 to 99:
    random_index = random number from list_of_numbers not chosen yet
    sum = sum + array[random_index]
return sum
The code in section 1 accesses the elements of array in order, from 0 to 99, while the code in section 2 accesses the elements of array randomly. While both tasks perform essentially the same task -- summing the elements of an array -- the code in section 1 will be much faster than the code in section 2. With the code in section 1, when we access the element at array[0], the computer stores elements near it in cache, so the element at array[1] is likely to be in cache for fast access on the next iteration of the loop. Conversely, with the code in section 2, the first element accessed might be at, say, array[78], the next element might be at array[19], and there is less probability that the element at array[19] will be in cache for fast access. A good general rule of thumb is simple and direct access patterns often beat complex and potentially insufficient algorithms.









Abstract Data Types
When we design algorithms, we need to represent data items based on what is expected of them in terms of functionality in a formal abstract or mathematical sense. For example, we may need to represent our data as a set as in mathematics because the operations we will involve them in are things we normally do with sets:

inserting an element into a set

deleting an element from a set

intersecting, unionizing two sets

subtracting a set from another set

testing whether a set is equal to another one or is a subset of another one

testing if a set contains a certain element

Note that we have intentionally not said anything about what kind of an element a set contains. In fact, sets do not really care what types of elements are stored in them, except that they expect that at least you can test if two elements are equal or not. In a way, we have abstracted sets from what is contained in them.

Modern computers provide standard representations for data such as integers, and floating point numbers, which are approximations to their mathematical analogs of mathematical integers and real numbers, differing only in the range or the precision of the numbers that can be represented. Our sets can be sets of integers or real numbers or any other structured data we can build from these -- e.g., complex numbers, etc.

What is important for an abstract data type description are the following:

What are the mathematical descriptions of each of the operations one can do?

What are the types of data items that are input to each operation?

What are the types of data items that are output from each operation?

For instance, for the first operation above, we input an element into a set and get a new set which is guaranteed to contain the given element after the operation. For intersection, we get two sets and return a set of only those elements that are in both of the given sets. Finally, for the last two operations, the output is a binary-valued boolean type whose values can be true or false. Note that these specifications are independent of what type(s) the elements in the sets are, or how the sets are represented, or how each operation is implemented in code.

So ideally, the programmer decides on the abstract data types that will be used in the algorithmic solution to a problem by concentrating on the operations that will be needed. At this point, she does not need to worry about how those data structures are represented in detail and how the operations are implemented.

The most important idea one should remember about abstract data types is that abstract data types determine functionality. Functionality is the most important aspect of an abstract data type that any user of that abstract data type (i.e., a client programmer) needs to know. This is typically communicated through an API that names the operations and the input-output data for each call in the API, along with some description of what function that call provides.

Concrete Data Structures
A data structure specifies how data of an abstract data type is represented and how the operations are implemented. For example, a set can be represented in many different ways:

We can represent a set as an array of elements. This is an efficient representation in terms of the memory required. On the other hand, many of the operations would be very inefficient. For example, to decide if the set contains a certain element, we have to start at the beginning and systematically compare it to every element in the array until we either locate it or exhaust the array. Other operations, such as the intersection of two sets or the insertion of a new element, would have additional complications in terms of steps required or memory allocated to represent the set. With suitable coding, all the operations can be implemented, but in this case, almost all operations would require a number of steps that is proportional either to the number of elements in the set (e.g., insertion or search) or is proportional to the product of the sizes of each set (e.g., intersection). We leave it as an exercise to see how much time these operations would take if one implemented a set as a sorted array.

We can represent a set as a hash table. Hash tables consume additional memory in addition to the memory required to store the data but allow for a search for any element based on its key in expected constant time. So if you plan to do a lot of searching most of the time, that will be desirable. You can also insert new elements or delete old elements from a set in the expected constant time. However, the intersection of two sets would take a comparatively longer time, and so would the operation of finding the element with the minimum value.

There are many other concrete data structures that one can use to implement the abstract data type for sets.

Similarly, a specific concrete data structure can be used to implement multiple abstract data sets. For example, a pair of real numbers as a concrete structure can implement

Complex numbers, where the pair either encodes the real and imaginary parts of a Cartesian representation OR the magnitude and the argument of a polar representation of a complex number. All operations on complex numbers (e.g., exponentiation of a complex number to a complex number) would then be implemented in these representations.

Two-dimensional vectors in a Cartesian vector space where the two numbers represent the components of a vector along the x and y axes. One can then implement operations such as vector addition or the dot product of two vectors using this representation.

Note that it is the abstract data type that determines whether the operations to be done on the concrete representations are sensible or not. For instance, exponentiation of a complex number to a complex number makes sense in the domain of complex numbers but not in the domain of two-dimensional vectors, and similarly, the dot product of two vectors does not make sense in the domain of complex numbers, even though in both cases the underlying concrete representations are the same.

The important point to remember about concrete data structures is that they are used to implement abstract data types correctly, but this implementation determines the cost of the abstract data.




Algorithms represent data and relations between data items using a variety of abstract data types. The most commonly used abstract data types are the following:

Sequences: Sequences are also called one-dimensional arrays. Typically, arrays are fixed size. Items are identified by positions or indices, starting with 0. Given an index, any item can be accessed in constant time; old items can be replaced with new values similarly. Languages like Python allow the extension of an array if one needs to add new values to the end of the array. Further, Python lists can hold data values or different underlying types.

Sets: Sets let one represent the equivalent of finite sets in mathematics, with elements coming from some domain where equality between the elements in the domain is defined in some way. The operations on sets are the typical operations one does on mathematical sets:

Intersection and union of sets,

Subtraction of one set from another set,

Inserting elements into a set or removing an element from a set,

Checking if a given element of the domain is a member of a set,

Computing the size of a set.

In ordered sets, the elements are assumed to be orderable based on a 
 relationship. With sorted sets, we can also do operations such as

Find the 
 smallest element of a set,

Find the ordered position in the set of a specific value 
 (same as finding the number of elements in the set less than a given 
 ).

Tables: Tables are an extension of sets. Each element in a table 
 consists of a key 
 and an associated value 
 . Key values in a table should be unique. Set operations like union, intersection, and difference on tables are done based on the key values with some provisions for conflicts. But one mainly uses tables typically for finding the value 
 associated with a key. Similarly, tables can be ordered based on a key if there is a need.

Graphs: Graphs are the most versatile abstract data types. They are typically used to represent a set of items (called nodes) along with asymmetric or symmetric relations between those items (called edges). For example, graphs can be used to represent

Social networks with nodes representing people and edges representing “friendship” or “follows” relations between them.

Transportation networks with nodes representing “intersections” and edges representing the roads between the intersections, with a distance measure associated with each road.

Neural networks with nodes representing neurons and edges representing weighted connections.

There is a whole set of operations one can do on graphs that can compute all kinds of useful information about the data and the relations. Here are some of such operations:

What is the shortest distance between any two intersections?

Who are the people on a social network with more that 100 connections?

Trees: Trees are special cases of graphs and are used to represent hierarchical relations such as parent–child relations. This restriction usually allows for more memory-efficient representations or time-efficient operations for specific classes of operations.

Priority Queues: Priority queues are essentially sets where each element consists of a value, and a priority, and the only operations one can do are

Inserting a value with its priority

Finding or deleting a value with the maximum priority.

While we listed these as abstract data types, some of these types are used to implement others, usually in a nested way. For instance, sets and tables can be used to implement graphs, while trees (specifically their balanced binary variants) can be used to efficiently implement sets, tables, or sequences. More details on these are beyond the scope of this section and are typically the topic of an introductory course and book on data structures and algorithms.

One can also refer to https://en.wikipedia.org/wiki/Abstract_data_ type for more details on abstract data types.

Did I get this?
Which of the following statements is true about the tree data structure in computer science?

Every node in the tree can have many children but only has exactly one parent.

The only way to implement a tree is by defining a custom class Node that stores the node properties (e.g., value, parent, children).

Traversing through all nodes in a tree can be performed with or without recursion.

The root node cannot be a leaf node.

Correct. BFS can be performed iteratively by maintaining a queue of visited nodes and DFS by maintaining a stack of visited nodes.

Previously, we alluded to concepts such as the time an operation takes or the memory a data structure requires. When we design an algorithm to solve a specific problem, we consider the the time it takes to execute along with the maximum memory that it needs during execution.

The fine details of the time and memory that an algorithm takes can be quite complex. In addition, an algorithm's performance depends on the hardware it is executed on, including the clock speed of the CPUs, the number of CPU cores, the speed of the memory interface, the number and sizes of cache memory, etc. Such factors make it tedious and possibly intractable to compare specific implementations of algorithms, or functions, on different computers.

Instead, theoretical computer science has developed simple but effective mathematical tools to compare algorithms in terms of the number of steps they execute as a function of the size of their input. These tools are based on what is called asymptotic analysis. The basic idea in asymptotic analysis is to model the growth rate of the time an algorithm takes as the size of its input increases. In particular, we model how algorithms behave as the size of their inputs increase towards infinity. Does the time that one algorithm takes grow faster, equally as fast, or slower than another? Some algorithms are fundamentally faster than others, and no amount of clever coding will get around this fact.

The number of steps an algorithm takes is defined in a machine-independent way on a hypothetical computer called a Random Access Machine, or RAM, using a standard Model of Computation:

Sequential execution.

Each simple operation -- such as arithmetic (
 ), comparisons (
 ), if/while, etc. -- takes exactly one time step.

Each memory access takes exactly one time step.

Unlimited memory.

A RAM model makes several simplifying assumptions. For instance, on most computers multiplication takes more steps than addition. And memory access times vary greatly depending on exactly where the data is stored. The RAM model abstracts away from such details to get at fundamental characteristics of algorithm performance.

With such a model, we can describe an algorithm's complexity using Big-O notation. Big-O notation ignores levels of detail that do not affect the comparison of algorithms in the limit. It considers an algorithm's behavior only for larger input values, abstracting away from the algorithm's variability on small values. It ignores additive and multiplicative constants because these don't tell us anything about the fundamentals of the algorithm. As we will see, it doesn't even include lower-order polynomials when higher-order polynomials dominate the performance of an algorithm for large input values.

More specifically, Big-O complexity, O(n)
 , defines a function that, when multiplied by some constant, is an upper bound on the growth rate of an implementation of the algorithm, f(n)
 , for large values of its input. Figure 1 shows this graphically: The function g(n)
 multiplied by some constant 
 is greater than or equal to f(n)
 for all input values 
 greater than 
 . We say that f(n)
 is O(g(n))
 .


Little-o complexity, o(n)
 , is similar to Big-O complexity except that it defines a function that is strictly greater than 
 . Big-Omega complexity, Omega(n)
 , on the other hand, defines a lower bound. And Big-Theta complexity, Theta(n)
 , defines a function that is both a lower bound and an upper bound when multiplied by different constants.

Formal definitions of Big-O notation are listed below. After the definitions, further elaboration and examples are provided.

 or Big-O: 
 is 
 if there are positive constants 
 and 
 such that for 
 ,
 . In other words, 
 is an upper bound on 
 .

 or Little-O: 
 is 
 if there are positive constants 
 and 
 such that for 
 ,
 . In other words, 
 is strictly greater than 
 for all 
 .

 or Big-Omega: 
 is 
 if there are positive constants 
 and 
 such that for 
 , 
 . In other words, 
 is a lower bound on 
 .

 or Big-Theta: 
 is 
 if there are positive constants 
 , 
 , and 
 such that for 
 , 
 and 
 In other words, 
 provides both an upper bound and a lower bound on 
 .

As a simple example, consider the Big-O complexity of searching for whether a given integer 
 is in an array of 
 integers. If the array is unsorted, a straightforward algorithm would be a loop through the elements of the array, checking whether each element is equal to 
 . Let's estimate that checking each array element takes 
 steps: increasing the array index, accessing the element at that index, and comparing whether the element is equal to 
 . In the worst case, the algorithm will not determine whether 
 is in the array until it compares the last element, so the function 
 for the number of steps will be 
 . Choosing 
 and 
 , we can see that 
 is 
 . In other words the algorithm is grows linearly.

By contrast, for a sorted array, a straightforward algorithm is a binary search: check a middle element, 
 , of the array, then if 
 is not equal to 
 , check a middle element of the lower or upper half of the array partitioned by 
 (depending upon whether 
 or 
 , respectively), and so on, checking a middle element of each partition until either an element equal to 
 is found or there are no more elements to check. This algorithm for a sorted array takes many fewer steps: Since each step halves the number of elements to search through, it takes 
 steps in the worst case, where 
 is the number of steps the algorithm takes for each element it checks. Big-O notation ignores the multiplicative constant 
 and the additive constant 
 , so this algorithm on a sorted array is 
 .

The complexity of a sum of functions is determined by the function with greater complexity. This is because as n gets larger, the function with less complexity has less and less impact on the resulting complexity. For instance, consider a sum of polynomials, 
 : For 
 , 
 , or 
 , for which the value of the smaller term (
 ) is 
 of the value of 
 . For 
 , 
 , or 
 , so the value of the smaller term is 
 of the value of 
 . As 
 gets larger and larger, the value of 
 becomes insignificant relative to the value of 
 . Table 1 below shows that when 
 reaches 
 billion, an algorithm taking 
 nanosecond per step takes 
 second for 
 and 
 years for 
 ! So we say that 
 is 
 rather than 
 . More generally, Big-O notation doesn't include lower-order polynomials when higher-order polynomials dominate the performance of an algorithm.

On the other hand, when functions are multiplied together, the resulting complexity depends on the component functions. If one of the functions is a constant, we can ignore the constant function because it can simply be considered part of the constant 
 in the complexity definitions: 
 . But if neither function is constant, the values of both functions can have a big effect on the resulting value. For instance, if 
 and 
 , 
 , which is 
 . For all the complexity definitions, the complexity of a product is the product of the complexities. E.g. 
 .

Let's compute the complexity of a simple algorithm for matrix multiplication: We can assume without loss of generality the general case of multiplying two 
 matrices. We compute the dot product of each row by each column. The dot product for one row by one column takes 
 multiplications (and 
 additions), which is 
 since we ignore the additive constant 
 . This 
 operation must be done for 
 rows and 
 columns, or 
 times. The combination is 
 , which is 
 .

Big-O notation groups algorithms into a set of equivalence classes; all algorithms within a class are considered essentially equivalent. A list of common algorithm classes in order of increasing complexity follows.

Constant: 
 . E.g., adding two numbers.

Logarithmic: 
 . E.g., binary search.

Linear: 
 . E.g., searching through an unsorted array.

Superlinear: 
 . E.g., the mergesort algorithm.

Quadratic: 
 . E.g., operating on all pairs of items among n items.

Cubic: 
 . E.g., operating on all triples of items among n items.

Exponential: 
 for a given 
 . E.g., operating on all subsets of n items.

Factorial: 
 . E.g., operating on all orderings of n items.

A faster growing algorithm is said to dominate a slower growing one:

The following table shows some examples of how time complexity grows for different orders of complexity on different values of 
 with each step taking one nanosecond (
 ). As you can see, exponential and factorial algorithms quickly become unmanageable as 
 gets larger.



Memory or space complexity can be computed using the same techniques we've used for the time complexity, substituting memory elements for steps. Since each memory access takes exactly one time step, and computation requires operations in addition to memory access, time complexity is always greater than space complexity. Space complexity includes both the space used by the input and auxiliary space: the additional, temporary space used by the algorithm during computation.

Time and space complexity are inter-related and involve important tradeoffs. In general, time requirements can be decreased by increasing memory requirements, and vice versa. For example, time complexity can often be reduced by memoization: Instead of doing a difficult computation every time (consuming time), store each unique result in memory. Then, before doing the computation again, check if the result is already stored in memory. Clearly, the opposite holds true as well: memory can be saved by not storing intermediate results in memory and instead taking the time to perform more computations.

The ramifications of high time complexity vs. high space complexity differ. A highly complex algorithm can cause a program to be slow -- sometimes unacceptably so, as evidenced by the table above. But if an algorithm requires more memory than the machine can make available, the program won't complete.

For more details on asymptotic analysis, a good resource is The Algorithm Design Manual (Skiena, 2020) [1] .

Did I get this?
Consider the following Python function that returns the index of a given target within a given sorted array (or -1 if target doesn’t exist in the array).

def func(arr, target):
    low = 0
    high = len(arr) - 1

    while low <= high:
        mid = (low + high) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            low = mid + 1
        else:
            high = mid - 1

    return -1

my_list = [2, 5, 8, 12, 16, 23, 38, 56, 72, 91]
target = 16

result = func(my_list, target )

Which of the following time complexities is closest to the worst-case performance of the above function?

O(logn) +

O(n)

O(n^2)

O(nlogn)

O(1)

This option is correct. The program repeatedly divides the search range in half. As a result, the runtime of the program grows logarithmically with the input size.


Computer memory can be visualized as a pyramid: At the top are smaller memory elements that are fastest to access but the most expensive. At the bottom are larger memory elements that are slowest to access but the least expensive.

Much of memory management is outside the programmer's control, but two principles can help in achieving faster performance. (1) Temporal locality: Data that was recently accessed is more likely to be accessed again, so focus on accessing data one small section at a time. (2) Spatial locality: Data that is near data that was recently accessed is most likely to be in cache (which is fast memory), so try to access data sequentially in memory.

Functionality is the most vital characteristic of an abstract data type that every user must understand. It is often communicated via an API that specifies the operations and input-output data for each API call, as well as a description of the function that the call delivers.

A specific concrete data structure can be used to implement multiple abstract data sets. For example, a pair of real numbers as a concrete structure can implement two-dimensional vectors in a cartesian vector space where the two numbers represent the components of a vector along the x and y axes. Operations such as vector addition or the dot product of two vectors can be then implemented using this representation.

Concrete data structures are used to implement abstract data types correctly, but this implementation determines the cost of the abstract data.

Learning to use the right data structure for different use cases is an essential skill for writing efficient programs and building complex applications.

An algorithm represents data and relations between data items using a variety of abstract data types. Some of the most commonly used abstract data types include Sequences, Sets, Tables, Graphs, Trees, and Priority Queues.

Asymptotic analysis is the study of how an algorithm grows as a function of the size of the input data to the algorithm. The basic idea is to model how the growth rate of two functions compares to large input. When analyzing an algorithm, we build a mathematical model of how the number of steps the algorithm executes depends on the size of the input.

Big-O Notation: We say a function 
 is 
 if there are positive constants 
 and 
 such that for 
 . That is, beyond 
 , 
 grows at most as fast as 
 .

It is important to understand how the time and memory cost grow with the size of the input, i.e., the time and space complexity for an algorithm. There is often a trade-off between runtime and space complexity, which typically comes in the form of storing intermediate values (using more space) to avoid re-computing them (reducing runtime).

Some important data structures commonly used in Python are lists, dictionaries, tuples, linked lists, sets, and trees. These data structures have different strengths for representing abstract data types, and their time and space complexities vary accordingly.

All data is stored as binary code in memory, typically with 8 bits per byte. The various data types all use one or more bytes as their underlying representation, but with the bytes representing different elements of the data type. These data types include Boolean, character, string, signed integer, unsigned integer, and float



A matrix (2D array) is a common data structure that encodes the relationship between elements stored in rows and columns. For example, a movie review site may store the rating history of its users in a matrix, where the cell at row i and column j is the rating score of user i for movie j. While this information can also be stored in a database-style table where every row contains the user id, movie id, and corresponding score rating, the matrix format allows for more complex computations over the entire rating data. As you will see in Project 2, an example of such computations is using least-square errors to build a recommendation system (i.e., given a user’s movie rating history, which movie would they want to watch next?)

However, the disadvantage of the matrix format is that matrices can be very sparse in certain domains. Here sparsity refers to the fact that the majority of entries are unknown or missing. In the example above, every user is only able to watch and rate only a very small portion of the entire movie catalog, so most cells in the matrix would be empty. In general, if the number of non-empty cells is roughly equal to or lower than the number of rows or columns in a matrix (e.g., if a 5 x 5 matrix only has about 5 non-empty cells), this matrix is considered sparse (although this is not a hard-and-fast rule).

Empty cells can be assigned a placeholder value, such as 0 (if the data is assumed to be positive) or null/NaN (if the data is assumed to be signed). In either case, the primary issue is that sparsity leads to a waste of memory and computational resources:

The placeholder values still consume actual memory. For example, storing a 10000 x 10000 sparse matrix of integers takes about 381MB, even if most entries are 0 and do not carry actual meaning. There is no point in representing data that does not exist!

Many computations on sparse matrices yield trivial results due to addition/subtraction or multiplication with 0s; however, they still need to be carried out by the computer.

Alternate matrix representations have been devised to reflect the underlying sparsity and avoid the above issues. In this module, we will introduce a number of approaches, along with their mechanisms, strengths, and weaknesses. Then, we provide general pointers to applications of sparse matrices in different areas of data science and machine learning.




Throughout this section, we will make use of the following matrix as an example:


We refer to the above representation, where the entire matrix with missing values is written out, as the dense matrix format. Equivalent representations of A as a sparse matrix instead aim to store only the non-zero (non-empty) values and operate on them, as described below. Note that to be consistent with common library implementations, we will use zero-based indexing when referring to row and column indices.

Coordinate Format (COO)
COO is a straightforward that stores a matrix as three lists: a list of non-zero values, a list of the non-zero values’ row indices, and a list of the non-zero values’ column indices. In this way, the matrix A is represented as a tuple of three lists (in addition to the matrix shape):

data = [7, 5, 1, 3, 2, 8]
col = [2, 4, 0, 2, 4, 3]
row = [0, 0, 2, 2, 3, 3]
Here data[i], row[i] and col[i] represent the actual value, row index, and column index of the i-th non-zero value in the matrix, respectively. Note that although we say i-th value, there is no ordering constraint here – the non-zero values can be arranged in any order in data, as long as their row and column indices are also arranged accordingly.

Updating entries in COO is simple: new entries can be appended to the end of the three lists, while zeroing an entry means finding its locations in the three lists and removing those data points. COO doesn’t support efficient arithmetic operations, but it can be quickly converted to other sparse formats that support these operations.

Dictionary of Keys (DOK)
DOK uses a dictionary representation that maps the location (row index and column index) of every non-zero element to its value. With the example matrix A,


its DOK representation is

{
	(2, 0) : 7, (4, 0) : 5, (0, 2) : 1,
	(2, 2) : 3, (4, 3) : 2, (3, 4) : 8
}
This format allows for fast element access by row and column index. It can also be quickly converted to and from the COO format, although it doesn’t support efficient arithmetic operations.

Compressed Sparse Row Format (CSR)
For the example matrix A,


the following CSR representation is explained below.

data = [7, 5, 1, 3, 8, 2]
col = [2, 4, 0, 2, 3, 4]
row = [0, 2, 2, 4, 6]
The data and col lists are the same as COO except that values must be specified in row and column order. The row list is different.

data: a list of the non-zero values in the matrix

col: a list of the column indices of the non-zero values

row: a list of m+1 values, where m is the number of rows in the matrix. Each entry in the row list except for the last specifies the entry within the col list where the column indices for that row's data start -- i.e., the offset within the col list for the start of that row's entries. In other words, for row[i],col[row[i]] is the entry within the col list that specifies the column for that row's first entry. For any subsequent entries in the same row i, the column indices are at col[row[i]+1], col[row[i]+2], etc. Due to this offset scheme, it is also true that the last entry for every row is col[row[i+1]-1], which is the index for the col entry preceding the first col entry for the next row.

We'll explain the last row entry in item (5) below.

For matrix A, which has 4 rows (rows 0 to 3):

row[0] is 0 since there is no offset within the col list for row 0. The first two entries in the col list are col[0]=2 and col[1]=4, which are the column indices for values 7 and 5 within row 0. I.e., the non-zero entries row 0 are (0,2) = 7 and (0,4) = 5.

row[1] is 2 to indicate col[2], which is the first entry within the col list that is not for row 0. There are no non-zero entries in row 1.

row[2] is also 2 to indicate col[2] -- since there are no non-zero entries within row 1, the offset within the col list did not increase. The next two entries within the col list are col[2]=0 and col[3]=2, which are the column indices for values 1 and 3 within row 2. I.e., the non-zero entries in row 2 are (2,0) = 1 and (2,2) = 3.

row[3] is 4 to indicate col[4] since the offset within the col list increased by two after listing the entries for row 2. The next two entries within the col list are col[4]=3 and col[5]=4, which are the column indices for values 8 and 2 within row 3. I.e., the non-zero entries in row 3 are (3,3) = 8 and (3,4) = 2.

row[4] is a special case. It could be considered redundant since every non-zero value in the matrix has already been specified by the preceding entries, as described above. The value of row[4] (or in general, row[m]) is the total number of non-zero entries in the matrix. This corresponds to the last entry in the col list (which contains an entry for every non-zero value), but since the col list is zero-indexed we have to subtract 1. This extra entry makes it so that for every row i, including the last row m, the value of the last entry in the row is also specified by col[row[i+1]-1], which can be useful for calculations involving such matrix representations. For matrix A, the last entry for row[3] is col[row[3+1]-1] = col[row[4]-1] = col[6-1] = col[5]. The value of col[5] is 4. So the last value for row 3, again, is at (3,4) = 2.

An equivalent and simpler way to calculate the col list offsets within the row list is by counting the number of non-zero entries in the rows above each row.

Note that with this representation order is important.

This compressed row representation is what gives “CSR” its name, as the rows are compressed to save space. (Think about why this compresses the space, and in what cases this might not compress space in the sparse representation).

This format allows for efficient row access and arithmetic operations (including elementwise matrix operations and matrix-vector products). For example, a matrix-vector product Mx involves computing the dot product between every row of M and x:
We know that the non-zero entries in row Mi are the ones at indices (i, col[row[i]]), (i, col[row[i]+1]), …, (i, col[row[i+1]-1]), so only these entries should be multiplied by the corresponding entries in x.

At the same time, column access is slow with CSR, and conversion to other sparsity formats is generally (but not always) expensive. Consider the case where the number of elements is rather few. In this case, what is the time-complexity of conversion to COO?

Compressed Sparse Column Format (CSC)
The CSC format behaves similarly to CSR, but with the columns being compressed instead of the rows, but in a very similar format. Its underlying representation consists of three lists:

data: a list of the non-zero values in the matrix

col: a list of n+1 values, where n is the number of columns in the original matrix.

col[i] denotes the number of non-zero entries that appear in the columns before (to the left of) the i-th column in the original matrix

row: a list of the row indices of the non-zero values.

With the example matrix A,


its CSC representation is

data = [1, 7, 3, 8, 5, 2,]
col = [0, 1, 1, 3, 4, 6]
row = [2, 0, 2, 3, 0, 3]
Similar to CSR, the order is important here, as entries that appear in earlier (left) columns need to be listed before those in later (right) columns.

This format allows for efficient column access and arithmetic operations (including elementwise matrix operations and matrix-vector products, although CSR is faster for the latter). For example, a matrix-vector product Mx can also be expressed as a linear combination of the columns of M, where the coefficients are the entries in x:

We know that the non-zero entries in column M(j) are the ones in indices (j, row[col[j]]), (j, row[col[j]+1]), …, (j, row[col[j+1]-1]), so only these entries should be multiplied with the corresponding entries in x.

At the same time, row access is slow with CSC, and conversion to other sparsity formats is, again, “generally” expensive. What’s key here to note is, again, that in certain cases, conversions might be readily easy to do. As CSC and CSR compress rows and columns in a similar fashion, can you think of an example where converting between one to another does not require accessing the underlying data at all but just moving references?

Application of Sparse Matrices
Here we provide a brief preview of how sparse matrices are used in different data science domains. We will discuss these domains in more detail in their corresponding modules later on.

Natural Language Processing

A crucial component of natural language processing is converting text data to numerical features which can then be used for subsequent modeling and training. Many techniques that perform this conversion yield feature vectors with very large dimensions but also high sparsity. For example, given a corpus C, the bag-of-word technique transforms an input document into a binary vector 
 where 
 is 1 if the i-th word in the corpus is present in the document. The size of this vector is the size of the corpus itself, which can easily reach tens of thousands for real-life documents.

Recommender Systems

At the beginning of this module, we mentioned the user-movie rating matrix as an example of a very large but sparse data structure. This kind of matrix data format is typically used as input to recommendation algorithms, which attempt to predict missing data based on present data (e.g., predict a user’s rating of a movie they haven’t rated, based on their past ratings of other movies). A standard technique for performing such predictions is collaborative filtering, which attempts to approximate the original user-movie rating matrix as a product of two lower-ranked matrices that have been obtained by a factorization of the original matrix. This factorization involves complex computations over the rows and columns of X, which motivate the need to store X in a sparse format. Recommender systems and collaborative filtering will be discussed in more detail within a later section of this module.

Sparse Modeling

The sparse format is suitable for storing not only the input data but also model parameters in certain domains. For example, in computational biology, we typically need to build predictive models (e.g., linear regression, logistic regression) over a very large number of features. If, however, we expect that only a small subset of features carry predictive power, we can opt to store the model weights in a sparse vector/matrix format to optimize training and inference.

Did I get this?
Consider an integer matrix of size 1000 x 1000, with 500 non-zero entries. If each integer costs 4 bytes to store, how many bytes in total are needed to store this matrix in dense format and in sparse COO format?

4,000,000 bytes in dense format and 6000 bytes in COO format. ++

4,000,000 bytes in dense format and 2000 bytes in COO format.

4000 bytes in dense format and 2000 bytes in COO format.

4000 bytes in dense format and 6000 bytes in COO format.

Which of the following is a sensible approach to modifying and doing computations on data in sparse format?

Keep data in COO/DOK to both modify and perform computations.

Modify data in CSR/CSC, convert data to COO/DOK for computation.

Keep data in CSR/CSC to both modify and perform computations.

Modify data in COO/DOK and convert data to CSR/CSC for computation.++




The matrix factorization approach to collaborative filtering assumes that the data can be represented by low rank matrices. In this section, we explain what this assumption means, and describe the singular value decomposition, which can be used to compute the best low rank approximation of a matrix. In the case when not all the data have been observed (e.g., in the movie ratings case, when not every user has rated every single movie), we cannot use the existing singular value decomposition algorithm, but we can still compute a similar low rank representation. The core idea is that even though the matrix of users by ratings is huge, we can approximate it as the product of several, much smaller matrices.

Rank
The rank of a matrix is the number of linearly independent rows. A set of vectors is linearly independent if there is no way to sum them to zero: Mathematically, 
 only if every single 
 is set to zero.

In the matrix 
 :

 

there are many choices for what to fill in as the missing values; any numbers substituted in for ? would lead to a valid matrix. However, if we want to make sure that the matrix has rank 1, there is only one possible choice: choosing the values that make each row a multiple of the first row.

 

Now, the second and third row are each a product of the first row,  
 
 , and scalars,  
 
 . This makes the the information in each row of matrix 
 redundant because each row is a multiple of the others. Going back to the definition, this means that the set of rows of this matrix are not linearly independent. For example, you can construct the zero vector by multiplying the second row by 
 
 and adding it to the first. There is only one way to fill in missing entries in the matrix to create a rank-1 matrix.

When we assume that the matrix of movie ratings has a low rank, we similarly limit the possible choices for missing values: The low rank assumption makes it possible to infer the missing values because it limits the number of possible ways they may vary.

Another way to write our matrix 
 is as a multiplication of two vectors 
 and 
 :

 
 

This is an outer product of two vectors, which always creates a rank-1 matrix because each row is a multiple of the other rows. We will show how we can write any matrix as the sum of rank-1 matrices, and we can use this fact to construct a low rank approximation of a matrix.

Low Rank in the collaborative filtering problem
In the dataset of users and movie reviews, we are going to assume that each user's taste can be explained by a linear combination of other users. For example, some users might prefer romantic comedies, or they might like action movies from a certain franchise. If we can identify a few users who are similar to the user whose ratings we want to predict, we can use a linear combination of the other users' ratings to make predictions.

Going back to our missing data example, we could fill in any numbers for the missing values, but there is only one way to fill in the missing values to create a rank-1 matrix. In the collaborative filtering problem, if we can assume that each user is not linearly independent of the other users, we can assume that the matrix of user ratings is well-approximated by a low rank matrix. This means that even though we can have tens of thousands of users and movies, we assume there are only a small number of dimensions that describe how users rate movies.

The matrix factorization approach is a data-driven way to identify the dimensions (features) along which a movie varies and simultaneously how much each user's ratings depend on those dimensions. Instead of explicitly coding these dimensions, we allow the model to choose dimensions as it sees fit. The low rank model of reviews was one of the first well-performing models for the Netflix prize (a movie review prediction competition). This is the model you will implement in P2.

What problem the SVD solves
In this section, we will show that the singular value decomposition (SVD) can be used to represent a matrix as a sum of rank-1 matrices. We will not be calculating the SVD; rather, this subsection is intended to inform your intuitions about calculating an approximation of the SVD.

We can write any matrix 
 as a sum of rank-1 terms 
 , where the 
 and 
 are vectors (in bold), and the 
 values are all scalars. Notice this the same form as the previous equation to represent 
 , but now with more than one term in the sum.

The values of 
 correspond to different movie features, such as the amount of action, whether there is romance, the actors, etc. These, however, are abstract movie features obtained by machine learning, so we can’t say that a particular value of 
 is a particular feature such as “romance.” The values 
 are the weights given to each feature in computing the total rating for the movie.

By adding more and more features, we can get a more and more accurate approximation of the overall matrix 
 values. However, in practice, usually a few larger 
 values dominate the feature weighting so that the smaller 
 values have negligible effect.

The singular value decomposition gives us values to plug into this formula by factoring the matrix into the product of three matrices, 
 , 
 , and 
 , so that 
 . The columns of 
 are vectors denoted as 
 to 
 and columns of 
 are 
 to 
 . There is a also a third matrix, 
 , of singular values, which has nonnegative terms 
 along the diagonal, and is zero for all non-diagonal terms. The singular values are sorted from largest to smallest. The number of nonzero singular values is equal to the rank of the matrix 
 .

The matrices 
 and 
 are also orthonomal, so that each matrix times its transpose yields the identity matrix, 
 and 
 . The condition means that each column of 
 is linearly independent of the others, and the same for 
 .

If you write out the matrix multiplication, you can verify that the product of the matrices 
 is a sum of rank one matrices 
 . (See the Wikipedia entry for Outer Product for more details.)


SVD as a Sum of Rank-1 Matrices

If we look at the sum, we see that the size of a singular value 
 tells us how much it contributes to the sum. If a singular value is zero, it can omitted from the sum as it contributes nothing. If a singular value is small, it contributes only a small amount to the sum, and it can be omitted without losing much accuracy. Truncating the sum at a fixed number of terms 
 gives us a rank-
 approximation, 
 . The approximation the SVD gives us is actually the best rank-
 approximation in terms of the least-squares error between each entry of 
 and 
 .

It is often the case that real data are low rank or approximately low rank, and are well represented by the SVD, which can be much smaller. In the movie ratings example here, the (Netflix prize) data were approximately 
 users by 
 movies, but a rank-
 approximation of the data was able to do quite well at making predictions. This model uses about 
 % of the storage it would take to store all 
 entries of the matrix 
 -- and it is the model you will implement.

The SVD is closely related to algorithms such as Principal Components Analysis (PCA), and many other models which use a low rank matrix to fit the data. We will not use algorithms to fit the SVD to fit the movie rating data, but we are using the same idea, of a low-rank approximation to a larger matrix, in order to fit the data.

Relationship to Collaborative Filtering
In the collaborative filtering for movie prediction example, the data matrix has a row for each user, a column for each movie, and the entries are numeric ratings. If we had every entry of the matrix available, we could use the SVD to compute an optimal low-rank representation of the data. Unfortunately, most of the entries of the rating matrix are missing, because most users have only seen (and rated) a small subset of movies. We want to do a matrix factorization which will give us a result like the SVD, but we will solve for it using alternating minimization, instead of using specialized SVD algorithms.

Instead of writing our low rank, approximate matrix as 
 , we write it as the product of two matrices 
 . When we approximate 
 as the product of these two matrices, we will choose 
 and 
 to be much smaller. For example, in the Netflix prize dataset, there were about 
 users and  
 movies, but 
 and 
 can be chosen to be 
 and 
 so that the product has at most rank 
 .

We never have to explicitly fill in and work with 
 ; we only need to calculate and update 
 and 
 , which are much smaller. By being aware of sparsity and taking care to implement everything using sparse matrix operations, we can ensure that the calculations to update 
 and 
 are efficient.

Connection to Eigendecomposition
In this section we explain how the eigenvalue decomposition is related to the singular value decomposition. The eigenvalue decomposition is a special case of the SVD that only works when the matrix is a square matrix. There are two symmetric and square matrices, 
 and 
 , which we can construct from a matrix of data, and the eigenvalue decomposition of these two matrices is closely related to the SVD, as we will show in this section.

In the collaborative filtering setting, 
 is size 
 (users) by 
 (movies), so the first matrix is 
 and the second is 
 . The first matrix, 
 , is the product of each row of 
 (a user) with another row of 
 (a different user), so that the 
 th entry is the dot product between a pair of users. If two users have similar taste in movies, their dot products will be higher. The other matrix, 
 , represents how similar the ratings of a pair of movies are. Compare these two dot product matrices to the user-user and item-item similarities described in the collaborative filtering reading: Note that the 
 and 
 computations both involve computing dot-products between pairs of user vectors and product vectors respectively, whereas computing the cosine similarity for the user-user approach and the item-item approach involves computing normalized dot-products over the same sets of pairs of vectors.

The matrices 
 and 
 can also be decomposed into their eigenvectors and eigenvalues. An eigenvector is a vector 
 that when multiplied by a matrix, the result is a scaled version of the same vector 
 , namely 
 , where 
 is a scalar called an eigenvalue. The eigendecomposition of a matrix 
 has the eigenvectors as columns of a matrix 
 , and the eigenvalues along the diagonal of a matrix 
 , so the the matrix can be written as 
 .

Next, we will show the relationship between the eigendecomposition and the SVD.

 
 
 
 
 
 
If we write the matrices 
 and 
 and then plug in the SVD for 
 , the inner terms will cancel and we will be left with 
 and 
 . The columns of 
 are the eigenvectors of 
 , and describe the relationship between the users. The columns of 
 are the eigenvectors of 
 , and describe the relationship between the movies. The singular values are the square roots of the eigenvalues and are the same for both matrices.

Summary
In this module, we have described the idea of a low rank approximation to a matrix, and what this assumption means for the collaborative filtering problem. If we had observed all the data (if every user rated every single movie), we could use the SVD to compute a low-rank approximation to the user by ratings matrix.

More Information on the SVD
This reading is based on the first chapter of this book :

Brunton, Steven L., and J. Nathan Kutz. Data-driven science and engineering: Machine learning, dynamical systems, and control. Cambridge University Press, 2022.

which both introduces the SVD and contains several example applications described in detail. The authors also produced a YouTube series. In particular, these two videos are a good introduction to the SVD and applications to low rank matrix approximation:

On the SVD (Mathematical Overview) https://www.youtube.com/watch?v=nbBvuuNVfco

On Matrix Approximation https://www.youtube.com/watch?v=xy3QyyhiuY4

Matrix Cookbook
The Matrix Cookbook is a useful and concise collection of facts for working with matrices, including identities, inequalities, and relationships between matrices.

Connections to Other Models
Many classic machine learning methods are some variation of low-rank modeling. This paper is nearly book-length, but well-structured and full of interesting ideas and connections between related models:

Generalized Low Rank Models M. Udell, C. Horn, R. Zadeh, and S. Boyd Foundations and Trends in Machine Learning, 2016: https://web.stanford.edu/~udell/doc/udell16_glrm.pdf

Why low-rank matrix factorization models work
One of the first people to try matrix factorization for the Netflix prize was the author of this blog post from 2006, which immediately became one of the best models on the Netflix Prize leaderboard: https://sifter.org/~simon/journal/20061211.html

The New York times article about the Netflix Prize mentions this blog post and notes that idiosyncratic movies like "Napoleon Dynamite" are especially hard to predict: https://www.nytimes.com/2008/11/23/magazine/23Netflix-t.html

The method here is the same method you will use in this assignment, except the author also added regularization terms to keep the matrices 
 and 
 small. The regularization here works out to be equivalent to minimizing the sum of the singular values, which is an upper bound of the rank of the matrix: Regularization makes the solution you obtain as low rank as possible.

In this good (if somewhat dense) theory paper , the authors prove that you can recover the exact solution (find a low rank matrix) if you observe 
 entries. If you plug in values for your dataset, notice that this bound is only a small fraction of the total entries in the matrix.

Recht, Benjamin, Maryam Fazel, and Pablo A. Parrilo. "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization." SIAM review 52.3 (2010): 471-501.



Matrix representation is useful for complex computations on data, like building recommendation systems. However, matrices can be sparse, leading to wasted memory and computational resources. Alternative matrix representations have been devised to address these issues, which are explored in this module along with their applications in data science and machine learning.

Sparse matrix representations like COO, DOK, CSR, and CSC efficiently store and operate on matrices with many empty values. They are particularly useful in areas like natural language processing, recommender systems, and sparse modeling, where data and model parameters tend to be sparse. Each format has its own strengths and weaknesses in terms of updating entries, arithmetic operations, and row or column access.




Statistical Inference is the process of drawing an informed conclusion about an aspect of your entire dataset using statistical methods. Those conclusions are typically drawn using exploratory data analysis or summary statistics. The goal of this process is to use probability theory to make inferences about your data. This is the first step of learning about the attributes of your population from the sample that you have drawn. Understanding statistical inference ensures that you analyze your data properly and eventually draw the right conclusions for decision-making purposes.

If you recall from a previous unit, you learned that the objective of your data science project could be to explore the data and gather insights from that exploratory exercise. You can use statistical inference to draw scientific conclusions and test hypotheses. The significance of a sample data set or descriptive statistics is often in question during the EDA process, but using statistical inference techniques can give significance to your conclusions from EDA. Statistical inference techniques are categorized under Estimation and Hypothesis Testing.

Sampling Distribution

Voter preference is a variable that varies among voters. Likewise, the sample proportion voting for a given candidate is a variable. If a sample was randomly drawn from a larger population, the act of random sampling makes the sample itself a random variable. Before the sample is obtained, its value is unknown, and that value varies from sample to sample. If several random samples of size n=2705 each were selected, a certain predictable amount of variation would occur in the sample proportion values. This distribution is called a sampling distribution. The sampling distribution of a statistic is the probability distribution that specifies probabilities for the possible values the statistic can take.

Each sample statistic has a sampling distribution. There is a sampling distribution of a sample mean, a sampling distribution of a sample proportion, a sampling distribution of a sample median, and so forth. A sampling distribution is merely a type of probability distribution. A sampling distribution specifies probabilities not for individual observations but for possible values of a statistic computed from the observations. A sampling distribution allows us to calculate, for example, probabilities about the sample proportion of individuals who voted for the Republican in an exit poll. Before the voters are selected for the exit poll, this is a variable. It has a sampling distribution that describes the probabilities of the possible values.

Random Sampling

Suppose a student decides to record her commuting times on various days. She selects these days at random from the school year, and her daily commuting time has the cumulative distribution function in Figure 1.


Figure 1. Cumulative Distribution Function of Commuting Time.

Because these days were selected at random, knowing the value of the commuting time on one of these randomly selected days provides no information about the commuting time on another of the days. That is because the days were selected at random, and the values of the commuting time on each of the different days are independently distributed random variables.

The situation described is an example of the simplest sampling scheme used in statistics, called simple random sampling, in which n objects are selected at random from a population (the population of commuting days) and each member of the population (each day) is equally likely to be included in the sample.

The n observations in the sample are denoted 
 , …, 
 , where 
 is the first observation, 
 is the second observation, and so forth. In the commuting example, 
 is the commuting time on the first of her n randomly selected days, and 
 is the commuting time on the 
 of her randomly selected days.

Because the members of the population included in the sample are selected at random, the values of the observations 
 , …, 
 are themselves random. If different members of the population are chosen, their values of Y will differ. Thus the act of random sampling means that 
 , …, 
 can be treated as random variables. Before they are sampled, 
 , …, 
 can take on many possible values; after they are sampled, a specific value is recorded for each observation.

I,I.D.

Because 
 , …, 
 are randomly drawn from the same population, the marginal distribution of 
 is the same for each i = 1,.., n; this marginal distribution is the distribution of Y in the population being sampled. When 
 has the same marginal distribution for i = 1,..., n, then 
 , …, 
 , are said to be identically distributed.

Under simple random sampling, knowing the value of 
 provides no information about 
 , so the conditional distribution of 
 given 
 , is the same as the marginal distribution of 
 . In other words, under simple random sampling, 
 is distributed independently of 
 , …, 
 .

When 
 , …, 
 are drawn from the same distribution and are independently distributed, they are said to be independently and identically distributed (or i,i.d.).

Standard Error

The sample mean, 
 , is a variable because its value varies from sample to sample. In practice, when we analyze data and find 
 , we don't know how close it falls to the population mean 
 because we do not know the value of 
 . Using information about the spread of the sampling distribution, though, we can predict how close it falls. For example, the sampling distribution might tell us that with high probability, 
 falls within 10 units of 
 .

For random samples, it fluctuates around the population mean 
 , sometimes being smaller and sometimes being larger. In fact, the mean of the sampling distribution of 
 equals 
 . If we repeatedly took samples, then, in the long run, the mean of the sample means would equal the population mean 
 . The spread of the sampling distribution of 
 is described by its standard deviation, which is called the standard error of 
 . The standard error of 
 is denoted by 
 .

For a random sample of size n, the standard error of 
 depends on n and the population standard deviation 
 by 
 
 .

Confidence Interval

Because of random sampling error, it is impossible to learn the exact value of the population mean of Y using only the information in a sample. However, it is possible to use data from a random sample to construct a set of values that contains the true population mean 
 with a certain prespecified probability. Such a set is called a confidence set, and the prespecified probability that 
 is contained in this set is called the confidence level. The confidence set for 
 turns out to be all the possible values of the mean between a lower and an upper limit so that the confidence set is an interval, called a confidence interval.

Consider this example:

Example
Title

Consider that we are measuring the heights of 40 randomly selected male soccer players, our sample mean is 175cm. We calculate the standard deviation of the athletes' heights to be 20cm. Let us calculate the CI.

n = 40, mean = 175, s = 20.

You will decide on the CI to use (95%) and then find the z-value for the selected CI. A 95% confidence interval means that if we were to repeat our sampling process many, many times, then 95% of those confidence intervals would contain the true population mean.

The z-value for 95% CI is 1.960

We calculate the 175 ± 1.960 × 20/

175cm ± 6.20cm

168.8cm to 181.2cm

Degrees of Freedom

Usually, the standard deviation for the population of interest is not known. In this case, the standard deviation is replaced by the estimated standard deviation s, also known as the standard error. Since the standard error is an estimate of the true value of the standard deviation, the sample mean follows the t-distribution with mean and standard deviation. The t-distribution is also described by its degrees of freedom. For a sample of size n, the t-distribution will have n-1 degrees of freedom. The notation for a t-distribution with k degrees of freedom is t(k). As the sample size n increases, the t-distribution becomes closer to the normal distribution since the standard error approaches the true standard deviation for large n.

Did I get this?
The process of drawing an informed conclusion about an aspect of a dataset using statistical methods is defined as:

Statistical Inference. ++

Exploratory Data Analysis.

Point Estimate.

Data Visualization.

Correct. You can draw conclusions after performing inferential statistics on your data.




Although we know that we should not make inferences from not enough data, we often forget that. We tend to overgeneralize from small samples. Sometimes, this is called the law of small numbers. Now, there is no statistical law of small numbers–it is used here as a satire, playing on the law of large numbers. The law of large numbers states that, under general conditions, as the sample size gets large, the mean of the sample will be near the mean of the overall population with a very high probability.

In his book Thinking, Fast and Slow , Daniel Kahneman describes examples of cognitive biases of fast thinking. Drawing naive conclusions and making inferences about the population from such a small sample size is one of them. The book provides a great illustration of the dangers of acting as if the law of small numbers is actually a law.

A study of the incidence of kidney cancer in the 3,141 counties of the United States reveals a remarkable pattern. You may find out that the counties in the United States with the lowest incidence of kidney cancer are mostly rural rather than urban. Now, you can probably imagine why that's true if you're healthy or living out of the countryside with better air, or maybe you're eating from the food that you're growing, and you're getting better nutrition. You can come up with some interesting reasons why this rural lifestyle would lead to a lower rate of kidney cancer.

Now consider also the counties in which the incidence of kidney cancer is highest. You may find these ailing counties tend to be also mostly rural, sparsely populated, and located in the Midwest, the South, and the West. So now you're going to maybe come up with some reasonable explanation: "Oh, well, It is easy to infer that their high cancer rates might be directly due to the poverty of the rural lifestyle–no access to good medical care, a high-fat diet, and too much alcohol, too much tobacco.”

None of these explanations is correct. What's really going on is that the rural counties have fewer people. So we have a smaller sample size and, therefore, more variation in our observed kidney cancer rate even though there isn't any difference in the actual cancer rate because we have more variation for the small counties. Both the lowest and the highest rates come from the counties with a small population, the rural counties.



logo
Foundations of Computational Data Science - F24 FCDS Pittsburgh


Module 8
52.
Hypothesis Testing
Due:
not yet scheduled
Introduction to Statistical Hypothesis Testing
When evaluating model performance, it is easy to fall into the trap of thinking that a better score is strictly better for model performance. Even if you use tools like k-fold cross-validation to get estimates of your prediction error or loss function, it is still quite challenging to confirm that you have not learned some constant model or that these performance estimates are truly “different enough” for you to pick one model over another.

To understand why this might be the case, consider a case where you have a hundred features, each randomly chosen from the set {0,1}, with your label also being uniformly at random chosen from {0,1}. If you run k-means cross-fold validation, your prediction error will not be 0.5, but usually much lower. This is due to the fact that your dataset is just a sample of the entire set of features that the problem can have. No matter how you try to classify the elements of the dataset, a trivial but “good enough” classifier will suggest strong performance due to random associations between the features and the label, despite the fact that, in this case, there are no associations between the features and the label.

Thinking about this problem more carefully, it becomes clear that most of the measures we discuss in machine learning to look at model performance have built-in uncertainty that we need to utilize to ensure that our systems work when deployed. These uncertainties can cause situations where the best-performing model on a training set might not be the best-performing model on a test set, no matter how you check the performance metric in question.

In general, no matter how great your dataset has become after cleaning and post-processing, there will still be some associations that come about from the dataset itself, which you will be unable to correct for. As a result, when comparing different models and different hyper-parameters for the same model, it can pay to take a page from statistics and do a hypothesis test on your performance measures.

A hypothesis test is simply a statistical procedure by which you can test some assumption about some fact about the true distribution of your dataset, known as a population parameter, and decide if you have a statistically significant result.

💡 Before you continue, it is important to note that these tests can be easily misused if not carefully thought about and reasoned with. Take your time through this chapter, as it is important to think carefully about if this is the tool you need for the problem at hand.

Performing a hypothesis test involves three major steps:

Deciding what your Null Hypothesis, 
 and what your Alternative Hypothesis are, 
 . This will depend on the test you perform, but in general, 
 refers to what you wish to “disprove,” and 
 refers to what you wish to demonstrate as more possible than the null.

Computing some sort of “test-statistic”. This is a measure of how unlikely the observed metric is, given the null hypothesis, and depends heavily on what distribution we assume the metric has in our problem.

Looking up the p-value for that test statistic, and comparing it to some pre-defined confidence “threshold”, 
 . This 
 is the minimum likelihood threshold for failing to reject the null hypothesis. If we are lower than 
 , we can reject the null, and tentatively suggest the alternative is more possible.

💡 It is important to note that “rejecting the null” does NOT mean “accepting the alternative”. All we are saying here is that, given our assumptions of the distribution of the metric in question, it is unlikely for the null hypothesis to hold. As a result, as the alternative hypothesis is the negation of the null, it is more likely to hold.

When performing these tests, you will need to be incredibly careful in the language you use to frame the results. These are tools to demonstrate that certain hypotheses are unlikely given the assumptions and evidence; they are NOT iron-clad rules that infallibly demonstrate that some fact about your data or the universe must hold.

That said, these tests can allow you to tentatively separate models based on their metrics, and suggest when a model is likely to perform better than another in general. This makes them useful in fields like Automated Machine Learning and when you want to compare models a little more thoroughly.

Without further ado, let’s discuss the first statistical test of this module and one of the forerunners of hypothesis testing: Welch’s t-test.

Welch’s t-test
Something we generally wish to do when we compare different metrics or other values about data or models are means or averages. For example, if you had two different average cross-fold validation metrics, it would be nice to know if that difference is statistically significant, i.e., is it likely to have happened due to random chance or not.

If we want to compare these means 
 and 
 against each other, we first need to define some sort of null and alternative hypotheses. Here, we have two options.

We could try to test if they are just different from each other with the following hypotheses:



Or we could test that one is strictly larger than the other:



The first kind of test is known as a “two-tailed t-test,” while the second is known as a “one-tailed t-test.” Either way, we’ll end up following the same procedure, so we’ll continue onwards with our next goal: figuring out what sort of test statistics we wish to compute. Generally, these test statistics come with their own particular distribution, from which we can calculate a “p-value,” or the probability that such a test statistic can happen given the null hypothesis.

In our case, we have two averages and want to look at their differences. For the student’s t-test, we shall use the aptly named “t-test statistic”:

 
 
 

In particular, we are going to use what is known as “Welch’s t-test statistic,” which is used when we have two averages with potentially different variances. This 
 value is distributed according to the t-distribution, which is essentially a more conservative estimate of the normal distribution, which is better when we have fewer degrees of freedom, i.e., approximately fewer samples. To calculate the degrees of freedom, we simply need to compute the following:

 
 
 

With these values, we can then compute the p-value or the probability that our null hypothesis holds, given our parameters. If this p-value is less than some predefined value, then they are different, and we can be more confident that we have different results.

While this test is not the simplest test, it does give us our first method of comparing different model performances. If we have enough data, we can create multiple sets of test and training datasets and try this test on two models to see if they have differing performances.

However, there are some problems with this testing procedure as is. Firstly, we do make some key assertions about the distribution of our metrics, namely that they follow a t-distribution. Given that accuracy metrics might not necessarily be normally distributed, we will want tests that assume less when our models get better.

Additionally, we cannot use the test as is without heavily segmenting the dataset. If we do not have enough data or wish to apply something more sensible than simply splitting the dataset three ways and applying a k-fold CV to each section, we will need to account for that.

Paired t-tests in Model Comparison
McNemar Test

To help combat some of the issues with Welch’s t-test, we can use the McNemar test instead. This test compares the error of two different models and determines if those errors are strictly the same or strictly different. Here, we let the error be simply 
 .

If we let the error of model 
 be 
 and the error of model 
 be 
 , then our associated hypotheses are:



For this test, our test metric is actually much simpler:

 

💡 In fact, this is the corrected McNemar Test, which helps when we are comparing high-accuracy measures.

Instead of following a t-distribution, this metric instead follows a Chi-Squared distribution with one degree of freedom. If you have at least 25 misclassified examples, this test is suitable for your data.

While there is no end-all-be-all hypothesis test in this space, it is worth mentioning that this test assumes far less about accuracy than the standard Welch’s t-test. Namely, it just assumes the samples you have tested are independent or that no datum’s feature-label pairing depends on another datum’s feature-label pairing.

On the other hand, this test does only work for accuracy values. When you are trying to compare other loss metrics, you need to use Welch’s or another paired t-test variety.

However, both of these tests do suffer a single, incredibly critical flaw.

Namely, they suffer from “p-hacking.”

P-Hacking

From our introduction to hypothesis testing, remember that the p-value is simply the probability that our null hypothesis implies the result we have. Due to this definition, we run into problems when we try to take paired tests, which look at pairs of models or pairs of means and expand them to handle more than two models at a time.

One way to see this is to think of flipping a heavily weighted coin, where one side of the coin comes up 
 percent of the time and the other side comes up 
 percent of the time. In this situation, even if we have a really, really low 
 , comparing multiple metrics on the same data could result in some null hypothesis being rejected when, in all likelihood, the null hypothesis holds.

Such a result would lead to a false comparison, where we find a statistically significant conclusion, not due to our data analysis skills but simply due to flipping the coin enough times. In research, this has led to situations where published research had a result that came from finding a singular interesting conclusion after sifting through a number of conclusions that did not pan out.

For our problem, this is especially grave. Consider that, for n models, we would want to perform 
 comparisons. As 
 , the chances of having a poor comparison skyrocket as the number of models increases.

Given this problem, then, the question is, how are we going to correct it and thus ensure that our models are statistically significantly different from each other?

The Friedman Test
To correct this issue, we must introduce the concept of the Friedman test. If we have n data sets to compare with and algorithms to compare, we first define the concept of a “relative rank” between algorithms as the order in which the algorithms are ranked on a singular dataset. For example, if on the first dataset, a simple linear classifier gets first on our loss metric, it would have a rank of 1 on that dataset.

💡 If there are ties, you will need to change the rank slightly to compensate. If you are interested, feel free to look around for one of the many ways to handle this case.

With this, we can then define the Friedman test in terms of the average rank of the 
 algorithm, 
 , among all datasets.

The hypotheses are the following:


 They are not all equal.

and the associated statistic is:

 
 

For this particular test, if you have at least 15 datasets or at least 4 algorithms, you can quite easily use a Chi-Squared distribution to check statistical significance. If you have neither of these cases, you will need to use a table specific to the Friedman test to get the p-value.

With this test, the main problem comes from what happens after you reject the null. The test itself simply states that “there is likely some difference between the ranks of each algorithm”. Thus, if you want to then pick the best algorithm out of the lot, you will need to do what is called a “post hoc test” to find the best-performing algorithm, assuming the Friedman test’s null hypothesis was successfully rejected.

There are a wide variety of these tests and many ways to display them. As calculating them can be relatively intensive, we will simply note that there are two types of post hoc tests:

Tests that perform all pairwise comparisons: Here, we compare all algorithms with each other, and determine which algorithms are better than each other. These tests work better than simply applying the paired-test, but still suffer from many comparisons.

Tests that compare with a baseline: When you are working on a challenge or on improving a model, typically you can look at it instead as a problem of “which of the models I’ve tested are better than the baseline”? These tests determine this, with the added benefit of only a linear number of comparisons on the number of algorithms used.

Overall, statistical tests like these help us make more sense of it, while accounting for some of the problems associated with multiple-comparisons testing. While they are computationally expensive and relatively difficult to run, they are key to having model evaluation strategies that make sense, and in making better sense of the training and tuning process for ML models.

Errors in Hypothesis Testing
According to Dermatology Associates, hyper-pigmentation is the number one skin health concern for Black females ages 18-45. Skincare Co. is one of the leading manufacturers of skin care products. Skincare Co. is looking to develop a 120-day skincare line to target this population and this skin health concern. You are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation. Your preliminary research has found that administering hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage. This is different from claims that have been made about this ingredient (previous claims state that there will be no damage). This claim or belief has been formulated, and it should be tested with evidence that refutes or proves that it is true. You can use hypothesis testing to provide this evidence. To construct a hypothesis test:

Identify the population parameter of interest.

Determine whether you will be conducting a one-tailed or two-tailed test.

Define a null hypothesis, often denoted as 
 . The null hypothesis is considered the status quo or, in the case of our example: The administration of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will not lead to permanent skin damage.

...then define an alternative hypothesis, denoted as 
 . This would be the opposite of the null hypothesis.

The example above does not cover the entirety of identifying your null and alternative hypotheses. You must know that if proven, your alternative hypothesis is a call to action, i.e., if you reject your null hypothesis, then the status quo has been changed, and the decision-makers must take action. How do we test our hypothesis statistically?

Let us also keep in mind that these tests are not error-proof! You want to be sure that you do not accept the null hypothesis when the null hypothesis should be rejected and reject the alternative hypothesis when it should be accepted. To avoid this, we consider the two error types in hypothesis testing.

Type I error occurs when you reject the null hypothesis when it should be accepted.

Type II error occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected.

Considering our skin care manufacturer example above. A Type I error would mean that the company does not include this ingredient in their skincare line when they should have been able to do so. The company stands to lose customers to companies with products that include this ingredient that is effective in treating this condition with no side effects. The consequences of committing a Type II error would mean that the company includes hydroquinone in their new skincare line targeted towards hyper-pigmentation when they should not have done so. The cost of this error would mean producing a skin-damaging treatment product that would lead to loss of customers and possible lawsuits.

p-value
As you may have wondered by now, what the p-value is and its role in hypothesis testing. The p-value is a term you often encounter in hypothesis testing. The p-value of a test is the smallest 
 value at which the test would reject the null hypothesis. The smaller the p-value, the greater the evidence against the null hypothesis.

Consider the example where you are calculating the p-Value for a test statistic with z-score = -2.878. Assuming 
 = 0.05, should you reject or accept the null hypothesis? (consider a two-tailed test)

Here, given a z-score of -2.878, we can calculate the p-value as,

p-value = 

💡 Note: Since we’re conducting a two-tailed test, we can then multiply this value by 2.

If you locate -2.878 in a z-score table , you get a value of 0.002.

p-value = 

p-value = 0.004

So, we have our p-value < 

Hence, we can conclude that we should reject the null hypothesis as a p-value less than 0.05 is typically considered to be statistically significant.

Did I get this?
Imagine you are working on a large-scale randomized controlled trial examining the effect of a drug, comparing the results between a control group and a test group of people with an undisclosed illness.

What are the potential null and alternative hypotheses for this scenario?

The null hypothesis is that there is no difference between the demographics and lifestyles of the two groups, whereas the alternative hypothesis is that there is a difference between the demographics and lifestyles of the two groups, and thus that the trial is invalidated.

The null hypothesis is that there is no causal difference between the effect of the drug on both groups, and the alternative hypothesis is that there is a causal difference between the effect of the drug on both groups.

The null hypothesis is that there is no difference between the effect of the drug on both groups, and the alternative hypothesis is that there is a difference between the effect of the drug on both groups.+++

The null hypothesis is that there is a difference between the effect of the drug on both groups, while the alternative hypothesis is that there is no difference between the effect of the drug on both groups.



Statistical Inference is the process of drawing inferences from your data using probability theory. Statistical inference is used to draw scientific conclusions and test hypotheses.

If n samples are drawn from the same distribution and are independently distributed, they are said to be independently and identically distributed.

As the sample size n increases, the standard error approaches the true standard deviation for large n. This is because the standard error is an estimate of the true value of the standard deviation.

A hypothesis test is a statistical procedure by which you can test some assumption about some fact about the true distribution of your dataset.

A hypothesis test is used to demonstrate that certain hypotheses are unlikely given the assumptions and evidence; they are NOT iron-clad rules that infallibly demonstrate that some fact about your data or the universe must hold.

The two error types in hypothesis testing:

Type I error occurs when you reject the null hypothesis when it should be accepted.

Type II error occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected.

A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis.



Statistics is the science of using data to learn about the world around us. In this course, we use the term "statistics" in the broad sense to refer to methods for obtaining and analyzing data. Specifically, statistics provides methods for:

Design: Planning how to gather data for research studies,

Description: Summarizing the data, and

Inference: Making predictions based on the data.

Design refers to planning how to obtain the data. For a survey, for example, the design aspects would specify how to select the people to interview and would construct the questionnaire to administer.

Description refers to summarizing data to help understand the information they provide. For example, an analysis of the number of siblings based on a survey might start with a list of the number reported for each of the people who responded to that question that year. The raw data are a complete listing of observations, person-by-person. These are not easy to comprehend, however. We get bogged down in numbers. For the presentation of results, instead of listing all observations, we could summarize the data with a graph or table showing the percentages of respondents reporting one sibling, two siblings, three siblings, and so on. Alternatively, we could just report the average number of siblings, let’s say 3, or the most common response, let’s say 2. Graphs, tables, and numerical summaries are called descriptive statistics.

Inference refers to making predictions based on data. For instance, for the survey data on the number of siblings, suppose 15.6% reported having no siblings. Can we use this information to predict the percentage of all adults in the U.S. at that time who is an only child? Predictions made using data are called statistical inferences. We will explore statistical inference in more detail in the upcoming section.

Description and inference are the two types of statistical analysis - ways of analyzing the data. Data scientists use descriptive and inferential statistics to answer questions about data. For instance, "Is changing the website layout associated with an increase in visitors?" or "Does student performance in schools depends on the amount of money spent per student, the size of the classes, or the teachers' salaries?"

Statistical methods help us determine the factors that explain the variability among subjects. Any characteristic we can measure for each subject is called a variable. The name reflects the values of the characteristics that vary among subjects. In data science, we usually see the term feature used interchangeably with variable.

Different subjects may have different values of variables: the values the variable takes to form the measurement scale. These measurement scales result in data exhibiting different types. In this section, we discuss the measurement scales of data analogous to data types. The valid values for a feature depending on its data type. Thus data types affect the methods we will choose to develop an analytic solution.

When discussing data types in data science, data is typically categorized as numeric or categorical and classified as one of the four measurement scale types. Quantitative data is represented as continuous or discrete values, while categorical data can be nominal or ordinal values.

Quantitative Data

Categorical Data

Discrete: I have one sibling.

Ordinal: I can rate my customer service experience at the grocery store as Good.

Continuous: It takes 1 hour and 20 minutes to get to school.

Nominal: What is your hair color? Brown.

Quantitative and Categorical Data

Data is quantitative when the measurement scale has numerical values. The values represent different magnitudes of the variable. Examples of quantitative variables are income, the number of siblings, age, the number of years of education completed, etc.

Meanwhile, data is categorical when the measurement scale is a set of categories. For example, marital status, with categories (single, married, divorced, widowed), is categorical. For Americans, states are categorical, with the categories Pennsylvania, Montana, Utah, and so on; for Canadians, the province of residence is categorical, with the categories Alberta, British Columbia, and so on. Other categorical data are whether employed (yes, no), favorite type of music (classical, country, folk, jazz, rock), and political party preference.

For categorical data, distinct categories differ in quality, not in numerical magnitude. Categorical data are often called qualitative. We distinguish between categorical and quantitative data because different analytical methods apply to each type. For example, the average is a statistical summary of a quantitative variable because it captures numerical values. It's possible to find the average for quantitative data such as income but not for categorical data such as religious affiliation or favorite type of music.

Structured and Unstructured Data

Structured Data is organized facts that are presented in fixed formats and are easy to extract. This data can be stored in spreadsheets, relational databases, and other repositories in, for example, a row and column format. Unstructured data is most difficult to extract. It is not easily stored in typical relational databases and spreadsheets because it does not neatly fit in the row and column structure or cannot be maintained in formats that are uniform. Text, multimedia files, and log files from servers are examples of unstructured data. New generation database frameworks, also known as NoSQL databases, have been developed specifically to handle unstructured data. Unlike structured data, unstructured data can be stored without a predefined schema.

Internal and External Data

Data can also be classified as internal data, which is data collected and/or controlled by an organization. An example would be personnel data collected and stored by the human resources department. We also have external data, data that is collected from sources outside of an organization. Census data and data gathered from credit reporting agencies are examples of external data.

Data Sources

The different types of data explored earlier are collected through different sources. Primary data sources include data that is collected and processed by an organization and housed internally. Secondary data sources include data that is gathered from sources external to an organization. Keep in mind that internal data can come from a primary or secondary data source and that an organization's data governance framework affects data that is collected from primary and secondary sources as long as they are used by the organization.



The quality of your data has a direct effect on the decisions made long after the models are developed. When data is gathered, it can present quality issues ranging from missing values to inconsistent formats. Data architects and engineers within organizations must clean the data gathered from internal and external sources to ensure that it is usable. Data that is collected from different sources are considered raw data. Raw data should be studied before it is used in an enterprise. Data is used for immediate analysis and model development with the goal of producing automated results or strategic decision-making. Data will move through different stages to ensure continuous use.

At this stage of the data science lifecycle, we are considering data in its raw form. One should also view all data (whether cleaned from its source) as raw data. We want to know how to enrich data to understand it further. Once you have completed this module, you will be able to discuss the techniques used to enrich data through a process called Data Wrangling.

Data Wrangling is the process of cleaning, formatting, and enriching raw data to make it usable for analysis. As mentioned earlier, data wrangling is also a best practice for an organization with a good data management framework. The data architects, engineers, and/or administrators will store data that has been processed to allow for enterprise-wide access and usage.

Data wrangling is a time-consuming process. As a data science team considers all data that has been extracted as raw data, the data wrangling process can assign value to a dataset after the data has been cleaned and transformed. Data wrangling is also part of the data understanding phase of the data science lifecycle and successful data understanding requires a clear understanding of the business and defining the business and analytic objectives and requirements for the analytic solution.

Despite its importance, data wrangling presents some challenges that are common in data science projects.

So far, you might have interacted with datasets from sources such as Kaggle, KDNuggets, or other avenues with “cleaned” datasets. You might also be collecting data from social media using built-in data-gathering tools to generate CSV files. You must consider these datasets as raw data. It is best practice to study the data to determine its quality.

Consider an organization that collects or purchases customer data from a marketing firm. The data from the marketing firm can be sent to the organization by a simple file transfer or through more automated sharing processes. The file from the marketing firm will contain formatted data that fits their data architecture and must be structured to fit the recipient organization's architecture.

A quick search about the data wrangling process will produce multiple definitions and perspectives. You might find that data wrangling is sometimes referred to as feature engineering. In this course, we separate both processes. When you perform data wrangling, you are essentially concerned with cleaning your data. Feature engineering will involve domain knowledge of the data and involves selecting the right features from the data to further improve the performance of your models.

The scikit-learn preprocessing package is widely used by data scientists and analysts for transforming and enriching data. Pandas, Numpy, Matplotlib, and Theano are other libraries in Python that support data cleaning and transformation.

Inspecting Data
Once data has been gathered, you will inspect it to assess its quality. You can inspect data using basic sorting techniques as well as creating visuals. Using visuals such as box plots to identify outliers in your dataset, sorting techniques will expose missing values and show the range of values in the different variables. Once data inspection is completed, you are ready to begin the preparation process.

Outliers

Rather small or large observation within your dataset compared to other values in the dataset is called an outlier. Outliers will affect the performance of your model and, prior to getting to that point, your exploratory data analysis. When you have a large dataset, the outliers are not as noticeable as when you have a smaller dataset. Similar to missing values, you must handle outliers when you identify them in your dataset. You should refrain from removing them from the dataset until a proper investigation is completed. You can “handle” outliers by following these steps:

Construct a Box plot or, as it is sometimes called, a box and whisker plot. This chart is used to graph the five-number summary. The five-number summary is then used to identify an outlier in your dataset. A five-number summary consists of five values: the maximum and minimum values in your dataset, the lower and upper quartiles, and the median. These values are then ordered in ascending order and plotted.

The box of a box plot contains the central 50% of the distribution, from the lower quartile to the upper quartile. The median is marked by a line drawn within the box. The lines extending from the box are called whiskers. They extend to the maximum and minimum, except for outliers, which are marked separately.

Box plots provide a visual summary of the data so that data scientists and analysts can identify outliers and other summary statistics, including the skewness and mean.


In box plots, the whiskers extend to the smallest and largest observations only if those values are not outliers; that is if they are no more than 1.5 IQR beyond the quartiles. Otherwise, the whiskers extend to the most extreme observations within 1.5 IQR, and the outliers are marked separately.

Why highlight outliers? It can be informative to investigate them. Was the observation perhaps incorrectly recorded? Was that subject fundamentally different from the others in some way? Often it makes sense to repeat a statistical analysis without an outlier to make sure the conclusions are not overly sensitive to a single observation. Another reason to show outliers separately in a box plot is that they do not provide much information about the shape of the distribution, especially for large data sets.

In practice, the 1.5 IQR criterion for an outlier is somewhat arbitrary. It is better to regard an observation satisfying this criterion as a potential outlier rather than a definite outlier. When a distribution has a long right tail, some observations may fall more than 1.5 IQR above the upper quartile even if they are not separated far from the bulk of the data.

The z-score

Another way to measure position is by the number of standard deviations that a point falls from the mean. The number of standard deviations that an observation falls from the mean is called its z-score.


The z-score of an observation 
 is a measure of the relative position of that observation within a dataset. You calculate the z-score by subtracting the mean from the value and dividing the result by the standard deviation. By the Empirical Rule , for a bell-shaped distribution, it is very unusual for an observation to fall more than three standard deviations from the mean. An alternative criterion regards an observation as an outlier if it has a z-score larger than 3 in absolute value. If an observation has a z-score that is more than 3 or less than -3, it is an outlier!

Transforming Data
The data gathering process looks different for each data-related project and depends on your business and analytic objectives and your data source(s). The data you acquire during the gathering process will almost always need to be transformed into a usable format to meet the requirements of a data science task

Handling Missing Values

One of the most common data quality issues is missing values in your dataset. This can happen due to human error or system issues during data collection. As you inspect your data and identify missing values, it is important to determine why the dataset has missing values. One should also be aware that a dataset that was extracted from an external source might not provide context on the reason behind the missing values. Even in those cases, a data scientist or data analyst should still investigate the missing values. The reasons behind the missing values will determine the techniques used to handle those values.

In statistics, missing data are classified into three categories. Those categories explain the likelihood of missing data.

Missing completely at random (MCAR) implies that missing data is not related to the data. The probability of data being missing is the same for all observations.

Missing at random (MAR) is the probability that the missing data is the same within certain groups.

Not missing at random (NMAR) means that the probability of data being missing varies for reasons that are unknown.

Imputation

The common strategies that are employed in handling missing values are imputation and omission. Imputation replaces missing values in the dataset with other values. The replacement values are not random. One can replace missing values with the mean value. For example, if you have missing values in the age variable, you can replace the missing values with the mean age across all observations. This method will work if the group is homogeneous. But our dataset may not always contain homogeneous groups. In such cases, you will need to resort to other imputation techniques that we will discuss in the feature engineering unit. Those techniques include hot and cold deck imputation, regression imputation, and interpolation and extrapolation.

Omission

Omission is often the go-to technique when there are missing values. Omission involves excluding the missing values from the dataset. Remember, you will suffer a loss of data if you exclude values instead of finding other missing value-handling techniques. Omission can be done when the number of missing values is small.

Pairwise deletion is a type of omission. This means your analysis will be performed on just the available values, which is a smaller sample size.

Listwise deletion removes all data for an observation that has one or more missing values. This would mean your dataset would have observations with values for all variables.

You can also omit variables with missing values. Such variables need to be ones with little to no importance to your dataset and overall objective. For example, if we are predicting social media usage habits, and our dataset includes a shoe size variable with a missing value, we can likely remove that variable and its values from the dataset.

Subsetting. This process involves extracting portions of a dataset that are relevant to your model or analysis and is used in data wrangling to prepare data for exploratory data analysis. This technique can be used to remove observations with missing values. Subsetting can also involve excluding variables instead of observations. An example is looking at summary measures of three subsets of medical records for diabetes treatments where one subset is for successful treatments, another is for unsuccessful treatments, and the last is for inconclusive treatments.

Outliers

When we discussed inspecting the data, there was mention of visualizing the data to identify outliers. Outliers are unusual values in the dataset. The value is unusual because it “lies at an abnormal distance from other values in your dataset.” We will discuss using exploratory data analysis techniques to identify outliers in a future unit. In general, you should not immediately remove outlier values as they oftentimes can contribute valuable insights to your solution. Investigating the reason behind the outlier value is the first step in handling it.

As you learned previously, there are different types of data, and those types of data have specific data transformation techniques that accommodate them.

Transforming Categorical Data

Categorical data is divided into groups or nominal categories based on a qualitative characteristic. Gender, race, and eye color might be variables in a dataset that is useful in predicting a health challenge. Usually, for processing purposes, such data may need to be transformed into a quantitative format. The following are techniques that are employed to transform categorical variables.

Category Reduction. Categorical variables can have many categories or levels. A variable with levels that are not useful can negatively affect your analysis and model. Some categorical variables will have levels that do not occur. It will be difficult to capture the interactions within those levels. A technique to handle these variables can include collapsing some of the categories or creating an "other" category for the categories with few occurrences.

Creating Category Scores. Ordinal data may need to be transformed into quantitative values for certain statistical techniques. Ranked values are an example. A dataset containing student evaluations would have responses that are ranked by different levels. One can transform that data by assuming equal increments between category scores. Responses to the question: “The instructor provided out-of-class support for the course” could be one of Always, Most Times, Sometimes, Hardly, Never. One can assign a score of 1-5, 1 being the highest and 5 being the lowest, or vice versa. The categorical variable can now be captured using quantitative values.

Creating Dummy Variables. Dummy variables are often referred to as binary variables. This technique allows for categorical data to be transformed into 0s and 1s. A dataset containing customer spending data can have a categorical variable, gender, with two categories, male and female. The gender variable can be converted to binary variables. Please note that there is no order or ranking.

Creating Dummy Variables for more than one category. What happens when you have a categorical variable containing more than one category? Consider a dataset with the variable hair color with data represented as brown, brunette, black, gray, and blonde. The hair color variable can still be transformed into dummy variables using the following steps:

For a variable with 
 categories, one will create 
 dummy variables. So for the example above, we will need 4 dummy variables. Let’s call them black, brown, brunette, and gray. 4 is the number of categories of the variable. You will create 4 dummy variables (5-1).

One can now assign 0 or 1 to each category: for example, the black variable would get a value of 0 if the observation does not have black hair and 1 if the observation has black hair.

Keep in mind that the category that was not included in the creation of dummy variables still exists in the dataset. In this example, a dummy variable for blonde was not created. This simply means that all other categories will be compared to this category. Usually, you select the category with the most frequent occurrence as the category that will not transform into a dummy variable.

Transforming Quantitative Data

Categorical data is transformed into quantitative data so the data can be used for specific statistical techniques. Why would one need to transform quantitative data? If you remember, when data is gathered, it is usually noisy with missing values and sometimes needs to be converted to a structure that fits the data science task. This will ensure that you do not lose data or lose information during the analysis phase. One will also encounter quantitative data that needs to be transformed to allow one to glean insights and be usable with appropriate statistical techniques.

An exampleof a popular quantitative transformation is converting the date of birth to age.

Quantitative transformations are also useful when performing feature engineering. One will extract features from the quantitative data and transform them into formats that can be used by a machine learning model. These techniques will be explored in depth later but right now, let us take a look at the techniques for converting quantitative data during data wrangling.

Binning transforms a quantitative variable into a categorical variable. For example, values for age can be grouped into intervals; that is, one can create the following groups: 15-19, 20-24, 25-29, and 30-34, thereby reducing redundancy in the dataset and making it easier to capture outliers. Binning can also be done using unequal intervals.

Using Mathematics. One can create new variables using mathematical transformations on existing variables. For example, you can use techniques such as standardization, min-max scaling, and logarithmic transformation. We explore these mathematical transformation techniques in a future unit.

Integrating Data
Data integration involves ingesting, transforming, and integrating the transformed data for access. The data is integrated to allow for analytic solution development, i.e., modeling and analysis. A popular example is integrating data into a data warehouse so that OLAP (Online Analytical Processing) servers, DSS (Decision Support) Systems, and other enterprise-wide analytic tools can access the data. The data can also be moved to data marts within the data architecture so that other parts of the enterprise can access the data. When there is a data warehouse, data integration can be done with the assistance of an ETL (extract, transform, and load) mechanism.

Once you have enriched and integrated your data, you are ready to explore it and perform feature engineering visually. You might find that feature engineering is an extension of the transformation process done during data wrangling.

Data Wrangling to Data Exploration

In the next module, we will be taking an in-depth look into data exploration techniques. This is typically referred to as Exploratory Data Analysis (EDA). EDA is used somewhat analogous to descriptive analysis in statistics. The results of an EDA exercise can give insights into the project. This is why it is important to begin the data understanding process with wrangling. At this point in the data science lifecycle, data has been preprocessed for use during the EDA process and beyond. Remember, data wrangling is not just for analytic solutions.

The extent of the data understanding phase shows that data quality can truly make or break an analytic solution. The data wrangling process has now informed the data science team on the state of their dataset and might signal a need to source new data. If new data is sourced, then the data wrangling process is repeated in an iterative fashion.



We've seen that statistical methods are descriptive or inferential. The purpose of descriptive statistics is to summarize data and to make it easier to assimilate information. The exploratory data analysis (EDA) process comprises visualizing data to allow a data scientist or a data analyst to explore datasets to gain insights from the data. EDA uses non-graphical techniques and graphical techniques to explore the data. Non-graphical techniques include using summary statistics to describe the data, and graphical techniques are used to describe the frequency distribution of the dataset. Both techniques can be used to show the skew of the data distribution and the extreme outliers.

Summarizing Data

Summarizing data is dependent on the types of data present in your dataset. It is difficult to describe a large data set in its raw form and use specific techniques to summarize and describe the data, including Describing Central Tendency and Assessing Measures of Spread and Relationships.

One can use the location in the data space, the shape of the distribution, and the spread of the data in a dataset to understand its aggregate properties. some of the concepts below can seem like a review of a first course in Statistics, but one should pay attention to the reason for using these techniques in exploring the data. Furthermore, these concepts are important when using statistical inference to draw conclusions on an unknown population parameter.

Location. During the EDA process, one describes the data using a central value. The Mean, sometimes called the arithmetic average, is one such value and is the sum total of all observations divided by the number of observations in the data. The whole population of data may have a population mean value 
 , or if you are only exploring a (smaller) sample, you can talk about a sample mean 
 
 . In addition to the standard arithmetic mean, there are also other central values, such as the geometric mean and harmonic mean.

The Median is the mid-value of a dataset. To compute a median value, one first sorts the data in ascending order. The median value in a dataset with an odd number of elements is the value in the middle. For example, for the (sorted) set {1, 3, 5, 7, 9}, the median will be 5. On the other hand, the median of a dataset with an even number of element observations is defined to be the average of the two middle values. For example, for the (sorted) set {1, 3, 5, 7, 9, 11}, the median is defined to be the average of 5 and 7 = 6.

If the mean and median of your data set differ greatly, you should check that variable for outliers!

Mode is the value that occurs most frequently in the dataset. A uni-modal variable is one that has just one mode, and a bimodal variable has two modes. If your data has more than two modes, it can be referred to as multi-modal. The mode is quite useful when summarizing categorical variables.

Percentile. You may remember this nifty word from your GRE scores or height and weight data from your health records. The percentile tells you the position of a value in the dataset. If someone is 175cm in height and is in the 10th percentile of height measurement for her gender, it means that among all the height data collected for that gender, she is taller than 10% of those values. The 50th percentile is considered to be the median. Quartiles are values that split the data into quarters.

The are several measures to describe the spread, variability, or dispersion of a dataset

Range of a set of values in a dataset can be calculated by subtracting the minimum value in your dataset from the maximum value. Notice that the range only considers two values and ignores all other values of a variable.

Mean Absolute Deviation is the average distance between each value and the mean of a dataset., that is

 
 

where 
 is the number of values and 
 is the 
 value in the data set.

This measure of dispersion can tell you how values are spread out in a dataset and determine whether the mean is a useful indicator of the values within the data. The larger the mean absolute deviation, the more spread out the data. When working with time series forecasting methods, one uses the mean absolute deviation to measure the performance of a forecasting model. Variance, typically denoted by 
 , is defined as the averaged square deviation of the values in a data set from the mean that is

 
 

Standard deviation, 
 , is simply the square root of the variance. It is the most commonly used measure of the amount of variation or dispersion of a set of values.

A low standard deviation tells you that the values are close to the mean, and a high standard deviation means there is a spread. As one performs exploratory data analysis and even while developing models, the importance of the standard deviation can not be overstated. Despite its mention as a way to summarize data, the standard deviation is also used to “measure the confidence in statistical conclusions” and to draw statistical inference conclusions on data and hypotheses.

Interquartile Range (IQR), similar to the range, does not consider all observations when looking at the spread of values in a dataset. IQR describes 50% of values in your dataset when arranged in ascending order. The IQR is the difference between the values in Quartile 3 and the values in Quartile 1. You can use this measure to identify a value that is an outlier.

Shape. Now that you can explain the measures used to explore data by describing its central value and its spread from the mean and identifying outliers, let us describe the distribution of a dataset and assess whether it is normally distributed. Normally distributed data is useful when making statistical inferences. How can we assess the distribution of our data:

Skewness measures the degree to which the distribution of data lacks symmetry. A dataset with 0 skewness is considered normally distributed. Data does not always have a skewness of 0; however, if you have found skewness to be between -0.5 and 0.5, you can ascertain that your data is symmetrical. If skewness is between -1 and -0.5 or 0.5 and 1, then your data is moderately skewed. If skewness is < -1 or > 1, your data is highly skewed.


Figure 1. Symmetrical Dataset with Skewness = 0 (Source: BPI Consulting LLC)

Kurtosis looks at the outliers within the distribution. This measure of shape will describe the distribution of data by showing whether the tails of the distribution are more or less extreme than the normal distribution.

Covariance describes the linear relationship between two variables in your sample or population data. Covariance can be negative, meaning your variables have a negative linear relationship, zero (0), meaning the variables have no linear relationship, or positive, meaning a positive linear relationship exists between the variables.

Correlation or correlation coefficient describes how strong the association between two variables, x, and y, is in terms of how closely the data follow a straight-line trend. It is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables. The correlation value lies between -1 and 1: 

The correlation equals 1 if 
 for all 
 and equals -1 if 
 for all 
 .

More generally, if the scatterplot of x and y is a straight line, then the correlation is either 1 or -1. If the line slopes upward, there is a positive relationship between x and y, and the correlation is 1. If the line slopes down, there is a negative relationship, and the correlation is -1. The closer the scatterplot is to a straight line, the closer the correlation is to 1 or -1.

A high correlation coefficient does not necessarily mean that the line has a steep slope; rather, it means that the points in the scatterplot fall very close to a straight line.


Figure 2. Scatterplots for Four Hypothetical Datasets.

Figure 2 gives additional examples of scatterplots and correlation. Figure 2a shows a strong positive linear relationship between these variables, and the correlation is 0.81. Figure 2b shows a strong negative relationship with a sample correlation of -0.81. Figure 2c shows a scatterplot with no evident relationship, and the correlation is zero. Figure 2d shows a clear relationship: As x increases, y initially increases but then decreases. Despite this discernable relationship between X and Y, the sample correlation is zero. the reason is that, for these data, small values of Y are associated with both large and small values of X. This final example emphasizes an important point: The correlation coefficient is a measure of linear association. There is a relationship in Figure 2d, but it is not linear.

One important note on correlation is that two variables having an association does not mean there is a causal relationship between them.



Statistics provides methods for planning how to gather data for research studies, summarizing the data, and Making predictions based on the data. Data is typically categorized as numeric or categorical. Quantitative data is represented as continuous or discrete values, while categorical data can be nominal or ordinal values.

Structured Data has organized facts that are presented in fixed formats and are easy to extract. Unstructured Data does not neatly fit in the row, and column structure or cannot be maintained in formats that are uniform.

Unlike structured data, unstructured data can be stored without a predefined schema. New-generation database frameworks, also known as NoSQL databases, have been developed specifically to handle this type of data.

Internal data is data collected and/or controlled by an organization, and external data is data that is collected from sources outside of an organization.

Data Wrangling is the process of cleaning, formatting, and enriching raw data to make it usable for analysis. Data wrangling is sometimes referred to as feature engineering. Feature engineering involves selecting the right features from the data to further improve the performance of your models.

Dummy variables are a technique that allows for categorical data to be transformed into 0s and 1s. A dataset containing customer spending data can have a categorical variable, gender, with two categories, male and female. The gender variable can be converted to binary/dummy variables.

The purpose of descriptive statistics is to make it easier to assimilate information. The exploratory data analysis (EDA) process comprises visualizing data to allow a data scientist or a data analyst to explore datasets. EDA uses non-graphical techniques and graphical techniques to explore the data.

Correlation or correlation coefficient describes how strong the association between two variables, x, and y, is in terms of how closely the data follow a straight-line trend. It is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables.



Raw Data to Features

So far, we have discussed data as an entity in the data science process and how it is transformed during the cleaning/wrangling process, used for exploratory data analysis, and used to draw conclusions with inferential statistics. Now we will focus on the parts of data that can be useful in the model-building process, parts of data that will assist in performing the tasks that you have defined in earlier stages of the data science process, and those tasks that are done to meet our analytic objective. Developing an analytic solution will involve the use of statistical modeling. We must understand that those models consist of formulae that only relate numerical quantities to each other. How then can we build a solution that ranks customer preferences or identifies segments of a customer base that might benefit from a service? How can a mathematical model understand variables that are not numeric?

Features

A feature is a numeric representation of a part of the raw data. The Wikipedia definition of a feature best describes it as "...an individual measurable property or characteristic of an observation". Features are the parts of an observation that are represented in a way that a machine learning model can use. Consider an image classification task. To properly represent the features of your image, they are processed into a numerical format that allows the mathematical model to use them.

When raw data is transformed into features, a data scientist must consider the right features that are useful for the data science task. A good feature is one that is appropriate to the statistical modeling technique and data science task. Features should also provide information, i.e., if you are performing a predictive task, your features should have predictive values.

Transforming or processing features from data is an important task in the data science project life cycle but is often glossed over. The price for badly selected features is a costly one that rears its head when you are training your model. As shown in Figure 1, features will directly affect the models that you develop and the insights gleaned from your models. The snowball effect of badly selected features will end up leading decision-makers down the wrong path. As efficiency and accuracy are key in the data science process, it is important to explore available resources that are developed to guide data scientists on feature engineering techniques for data science tasks and modeling. Note that feature engineering requires both domain and technical expertise.


Figure 1. Feature Engineering and Analytic Solution Building. (Source: Zheng & Casari (2018))

Feature engineering is the process of extracting features from raw data and transforming those features into suitable formats for a machine learning model. Feature engineering leads to higher quality models and better insights for decision-makers. When you think about the diverse machine learning techniques, data science tasks, and contexts in which we apply machine learning, you will see that feature engineering can not be generalized. It is not a one size fits all process. It is dependent on the analytic objective and the data. Feature engineering requires domain knowledge and intuition.

During the feature engineering process, the data scientist will remove features from the data that do not provide task-specific information (e.g., the feature has no predictive value) and also features that introduce redundancy. This is called feature selection.

Numeric Data Types: Even though we defined a feature as a numeric representation of data, raw data that is in numeric form should also undergo feature engineering. This is because the data must meet the assumptions of the chosen model.

Scalar: Single numeric feature, e.g., mass.

Vector: Ordered list of scalars; also defined as an object that has both a magnitude and direction.

Spaces: Vectors exist within a vector space and are also a collection of vectors that can be added or multiplied by scalars.

In machine learning, the input to a model is represented as a numeric vector.



Importance of Feature Engineering

Feature engineering is the process of using domain knowledge to extract features from raw data. Algorithms need specific features in the model development process. Feature engineering will ensure your dataset is compatible with your algorithm, thereby improving model performance. So far, we have highlighted the specialized nature of feature engineering and that there is no one suitable solution. However, there are foundational concepts that are essential to your understanding of feature engineering.


Figure 1. Mapping Raw Data to ML Features. (Source: Google Developer Course)

Feature Engineering Techniques

Imputation

Do you remember this concept from an earlier module? It is useful during the data wrangling process as you cleanse your data and is equally used in feature engineering. Missing values in a dataset can negatively affect the performance of a model. Missing values can be caused by simple human errors, and privacy concerns, among others. How can we fix the problem of missing values? A simple but problematic solution is dropping rows or columns. A preferable solution is an imputation. It would help if you considered a default value for missing values in a row or column. Let us visit how you handle this with numeric and categorical data.

Numerical Data Imputation. If you are not dropping rows and columns with missing data, the numerical imputation method will allow you to replace missing values intuitively. For example, a column with numbers and some with " - " or "NA" can be replaced with a "0". Other methods used include using the median or mean values of that variable.

Categorical Data imputation. In some cases, replacing missing values with a zero will not make sense to the dataset. You can replace values in a categorical column with the “most frequently occurring value,” and you can impute “other” in a situation where there is no dominant value in the categorical column.

Binning

Similar to imputation, binning can be applied to numerical and categorical data. Binning makes a model more robust, but there is a trade-off between performance and overfitting . Binning categorical data will have less of a negative effect on model performance than when binning is performed on numerical data. It is also used to capture noisy data when you have values that have variance.

Example
In the context of image processing, binning is the procedure of combining a cluster of pixels into a single pixel. As such, in 2x2 binning, an array of 4 pixels becomes a single larger pixel, reducing the overall number of pixels.

Handling Outliers

You learned about how to visualize your data to detect outliers in an earlier module. This method is less error-prone. You can use some statistical and visualization methods to detect and handle outliers, including computing the z-score, using percentiles, and visualizing the data distribution of your dataset. These techniques were discussed in the "Exploratory Data Analysis" module.

Log Transform

Log transform, also known as logarithm transform, is used to handle skewed data and make the distribution of data less skewed. It is widely used because of its ease of use, and it decreases the effect of outliers in a dataset. Log transform is not usually applied to values that are less than or equal to zero.

Categorical Encoding

One hot encoding

One hot encoding is a process by which categorical variables are converted into a form that could be provided to a machine learning algorithm to make accurate predictions. This technique replaces categorical variables with different Boolean variables that indicate whether or not a category of the variable is part of the observation. Those Boolean variables are called dummy variables.

One hot encoding is easy to implement; it will retain all information of the categorical variable. This method does not add information that can make a variable more predictive.

Assume that a categorical variable education with labels less than high school and high school. We can generate the Boolean variable “high school,” which becomes one if the person has high school or 0 if the person has less than high school.

Binary Feature Encoding

When you have a variable with multiple categories, one-hot encoding might increase the dimensionality of your data. The binary encoding method can be used to create a smaller number of variables without losing information.

Ordinal Feature Encoding

When you have ordinal data that is useful to your analytic solution, you can transform those features using ordinal encoding. Here we convert string labels to integer values. If you have an ordinal variable with string values that are satisfied, dissatisfied, highly satisfied, highly dissatisfied, not applicable, and somewhat satisfied, ordinal feature encoding will map the values to a corresponding integer. As you can see in the table below, all values are not integers.

Highly Satisfied

1

Satisfied

2

Somewhat Satisfied

3

Not Applicable

4

Dissatisfied

5

Highly Dissatisfied

6

Feature Split

When you split features, the features become easier to bin, and this improves model performance. There are many ways of splitting features, and it depends on the variable. If your dataset contains the variable address, you might split the column by extracting the street address, city, state, and postal code. You run the risk of increasing dimensions; in this case, we employ techniques that assess the value of the extracted dimensions.

Scaling

Some machine learning algorithms need to have scaled continuous features as model inputs. Scaling is not necessary for most algorithms, but it can make continuous features identical with respect to range.

There are instances that require the use of scaled data, including algorithms that use gradient descent

Gradient descent
(definition) An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.
Neural Networks and Linear Regression are some of those examples. The data is scaled before being fed to the model. Algorithms like k-Nearest Neighbors, clustering analysis like k-means clustering, and other distance-based algorithms would need data that is scaled.

Quick thought: How about tree-based algorithms like decision trees and random forests? They are not affected as they are not distance-based.

Normalization. This technique involves values ranging between 0 and 1. Prior to normalization, all outliers in the dataset should be handled.

Standardization. Also known as z-score normalization, it is useful for feature engineering in logistic regression, artificial neural networks, and support vector machine tasks.


https://scikit-learn.org/stable/modules/preprocessing.html




Feature Engineering and Bias

Feature engineering can be performed before the model building process, i.e., during the data wrangling and exploratory data analysis phase, or it can be performed during model building. Later in this course, we will discuss cross-validation, but we must note here that feature engineering can be done during the cross-validation process. Cross-Validation is a model validation technique used to assess how a model will generalize to a new data set. At this time, feature engineering is done during the cross-validation loop.

Feature engineering at any stage can introduce bias to the data. While you manipulate the data, you can unintentionally create a relationship between features that do not otherwise exist. The features that are selected or created during the feature engineering process can shape the insights that are gotten from the model.



Dimensionality

As you develop analytic models or perform exploratory data analysis, you will encounter datasets with a large number of variables. A small dataset can also become quite large post data cleaning -think about when you transform variables by creating new variables, e.g., dummy variables. Considerations for a dataset with a large number of variables include issues with over-fitting and computing costs. We think about the dimensionality of a model when we consider the number of variables used by the model. The mathematician R. Bellman defined the curse of dimensionality as the problem caused by the exponential increase in volume associated with adding extra dimensions or variables to a space. This just means that when there are more features in a dataset, you are prone to more errors. A dataset with a large number of features could have lots of redundancy and noisy data with little benefit to your overall analytic objective. How can you address the curse of dimensionality without losing useful information? We use the technique of dimensionality reduction, sometimes referred to as feature extraction or factor selection. This technique is implemented using mathematical modeling.

Feature Extraction

So far, we have talked about techniques that focus on features of an observation. As you know by now, feature engineering informs the models that you will build, and its techniques involve looking at the features of the data. Now, we will explore a technique that is considered a model-based feature engineering technique.

Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset. You might be asking yourself why we would reduce a dataset when we have talked about the importance of more data for better interpretation and solid performance from models. When you have a dataset with a large number of variables, you have to assess the relationship between those variables, identify variables that might violate the assumptions of your chosen ML model, and generally select the variables that are useful to your task. With PCA, you will be reducing the dimension of your feature space to remove any redundancies or irrelevant features.

You use the PCA technique when you want to ensure variables in the dataset are independent of each other. It is a useful technique to use when there are variables that need to be dropped. There are other techniques for dimension reduction, including Linear Discriminant Analysis (LDA), and those techniques will be mentioned in a future unit as well as in your upcoming Machine Learning courses.

PCA is a linear transformation technique as it finds a low-dimensional representation of your high-dimensional data. PCA involves performing the eigendecomposition on the covariance matrix. It will seek out a “small” number of dimensions in the dataset that are useful to the analytic task. PCA is considered to be an unsupervised technique and will be mentioned in that unit as well.

The following steps are used when performing PCA:

Standardize the data.

Compute the Covariance matrix of dimensions in the data.

Compute the Eigenvectors and Eigenvalues from the covariance matrix. Eigenvector is a nonzero vector that changes by a scalar factor when that linear transformation is applied to it. Meanwhile, an eigenvalue is known as a characteristic value1 or a set of scalars.

Sort eigenvalues in descending order and choose the top k Eigenvectors that correspond to the k largest eigenvalues.

Construct the projection matrix W from the selected k Eigenvectors.

Transform the original data set X via W to obtain the new k-dimensional feature subspace Y



In the last section, we explored Principal Component Analysis (PCA) as a dimensionality reduction technique to provide a low-dimensional representation of data through a linear transformation (a fancy term for multiplying by a matrix). While it is quite useful for tabular data, it is not always the best tool for the job, particularly as it requires us to preserve all pairs of distances between different data points. This leads to potentially poor performance when data tend to be clusters or classes, where we know the distance between neighbors might be much more important than the distance between data points “farther” apart. Let’s consider MNIST, a well-known digit recognition dataset. We can apply PCA to the dataset to get the following visualization:


Source: https://ryanwingate.com/intro-to-machine-learning/unsupervised/pca-on-mnist/

As we can see, while some groups are visible, most of the digits are clumped together, making them really hard to distinguish. Given that MNIST has 10 distinct classes, what we would like is an algorithm that lets us differentiate between local distances and global distances a little better to make classes more visible.

Enter t-distributed stochastic neighbor embedding or t-SNE. t-SNE is a popular statistical dimensionality reduction technique that is primarily used for the visualization of clusters of points in higher dimensions. t-SNE was introduced originally as a stochastic neighbor embedding method in 2002 by Geoffrey Hinton and Sam Roweis, and the t-distribution modification was introduced as an improvement over the original method in 2008 by Laurens van der Maaten and Geoffrey Hinton. Practically speaking, it is a really important technique, as it allows us to achieve non-linear embeddings of our data.

How t-SNE works is that it tries to model the distribution of points in the higher-dimensional space as a set of Gaussian distributions, where we define the probability of some data point i picking another point j to be neighbors through the following equation:

 

(If you’re familiar with deep learning, you might notice the similarities between this method and the softmax function. It’s also important to note that the distance function here is more of a suggestion, and you can use any custom distance function you want with most t-SNE implementations).

After creating this high-dimensional model, we then optimize a low-dimensional embedding, with the lower-dimensional embedding using a t-distribution instead with the following similarity function:

 

By minimizing the distance between these distributions, we can get a good lower-dimensional embedding for our high-dimensional data, thus allowing us to see a “good enough” representation of the potential internal clustering of the data. While we can use multiple distance functions between distributions, we tend to use KL-divergence as it is relatively easy to optimize and relatively easy to intuit. Effectively, it tells us that two distributions are different if they associate largely different probabilities with the same data. By minimizing this, we can ensure that our lower-dimensional embedding roughly approximates the high-dimensional data.

If we go to our MNIST example, using t-SNE gets us the following results, which make the clusters a lot easier to see:


Overall, t-SNE can be a far better visualization tool than PCA. However, it is very important to know the limitations of any new tool you consider for data science, especially data visualization. Given that visualizations can be misleading, it is important to treat a potentially better tool with skepticism.

In the case of t-SNE, the problem lies in the optimization process. Firstly, it is important to note that there are several parameters to the algorithm which need to be carefully considered in order to get good results out of the algorithm. The most important parameter is known as the ‘perplexity,’ and this roughly is the number of neighbors you want to consider as “close” to any data point. Larger datasets require larger perplexity, but too large of perplexity can remove the presence of any potential clusters, as you can see in the following example:


Secondly, note that it is important to not really think too much about the distances between clusters and the relative size of any clusters reported. Given that t-SNE is stochastic, the distances between points tend to mean very little, if anything, and instead, it is important to look at the data in its entirety.



Thirdly, it is important to note that t-SNE is sensitive to data scaling. Applying a standard scalar or another scaling system is important to get interpretable results, as otherwise, certain features will be scaled in a very different manner than any other.

Lastly, and most importantly, t-SNE is random. As the loss function is non-convex, different initializations can lead to different visualizations. Treat t-SNE as an exploratory tool more than anything, and do not make the mistake of trying to use it ahead of classification. Should you want to use a non-linear technique in this manner, consider looking into other tools like UMAP instead.

References:

Hinton, G. E., & Roweis, S. (2002). Stochastic Neighbor Embedding. In S. Becker, S. Thrun, & K. Obermayer (Eds.), Advances in Neural Information Processing Systems (Vol. 15). Retrieved from https://proceedings.neurips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf

Maaten, L.van der and Hinton, G. (2008) Visualizing data using T-Sne, Journal of Machine Learning Research. Available at: https://jmlr.org/papers/v9/vandermaaten08a.html

van der Maaten, L. (2014). Accelerating t-SNE using Tree-Based Algorithms. Journal of Machine Learning Research, 15(93), 3221–3245. Retrieved from http://jmlr.org/papers/v15/vandermaaten14a.html


Feature vectors play a pivotal role in data science, as they convert raw data into numerical forms compatible with machine learning models. This module emphasizes the importance of accurate feature selection and engineering, tasks requiring both domain knowledge and technical skills.

The module introduces fundamental machine learning concepts such as scalar, vector, and spaces, highlighting that even numeric data requires feature engineering to meet specific model assumptions.



Introduction to Natural Language Processing
Natural Language Processing (NLP hereafter) is a subfield of computer science aiming to build systems that:

Enable human-computer communication

Enable/enrich human-human communication

Perform tasks requiring the use of human language faculty, more efficiently and more correctly.

Here are some applications of NLP in these three areas:

Human-computer communication

Dictating email messages

Listening to email messages

With proper pronunciation, tone, and stress!

Using human language to give commands to the operating system

Human-human communication

Real-time text and speech translation

Summarizing meeting conversations

Enabling communication with people with hearing/vision impairments

Improving the efficiency of tasks requiring human language faculty

Correcting typing/grammatical errors

Question answering

Answering e-mails automatically

Searching large document databases

Here are some (and definitely not a comprehensive set of) applications of NLP in a diverse set of areas:

Information extraction (IE), especially “open” IE: Given a text (or a collection of texts), find out who did what to whom, where, when, how, why, with what, in exchange for what, etc.

Document classification: Classifying a document into topic areas (e.g., news, sports, business, entertainment, sports, etc.), classifying an email as spam or not, or as important or not.

Question answering beyond finding the documents relevant to the question and instead delivering the actual answer.

Conversational Agents (e.g., Siri, Alexa, Google Assistant): Holding a typically multi-stage turn-taking goal-driven conversation with a user, going beyond question answering and helping with other tasks such as making appointments, helping with shopping or entertainment options, etc.

Image (and eventually video) understanding: Expressing the pictorial or video content in human language (e.g., image captioning or verbalizing a football match action in a TV program setting).

Text (including text on images) and speech translation with additional applications in video call transcription and translation, real-time lecture translation, generating speech output in the right accent and the right lip rendering for much more natural-looking video generation.

Sign-language translation and scene-to-speech for the visually impaired (which would be akin to video understanding above).

Summarization: Generating a short summary of the salient points of a document or a set of documents.

Opinion and sentiment analysis: Extracting political or personal sentiments from news pieces, tweets, product or movie reviews.

Essay evaluation: Evaluate an essay (say in an SAT exam setting) for proper structure, sentence, and vocabulary use.

Fake news or fake review detection: Automatic fact-checking of news, especially in real-time viral settings on social media; verifying that product, restaurant, or movie reviews in online shopping or recommendation settings are genuine and not generated by bots, etc.

Bird’s Eye View of NLP
The following figure gives a bird’s eye view of NLP and various functions or tasks that partake in building and evaluating NLP applications.


From Eduard Hovy’s “The Past and 3½ Futures of NLP” presentation.

Fundamental functions in NLP
There are three high-level functions that NLP tries to automate:

Analysis (or “understanding” or “processing” …) where the input is language (text or speech), and the output is some representation that supports useful action (e.g., translation or robot movements) in response.

Generation: input is a representation of an utterance and possibly a representation of the context, and the output is text or speech that captures the semantics and the intent encoded in the input representation (e.g., generating the target language sentence in machine translation).

Acquisition: obtaining the representation and necessary algorithms from data (e.g., learning to translate from aligned translated sentences).

The representation that these employs depends on the actual task being solved: they can be explicit such as morphological analyses of words, syntax trees, or part-of-speech symbol sequences of sentences that capture the linguistic structure of words or sentences. They can also be very opaque, like sentence embeddings that a neural machine translation system computes as it analyzes an input sentence.

Relation to Other Fields
NLP is closely related to the following areas of computer science and other fields Machine Learning, Deep Learning, Artificial Intelligence, Statistical modeling, Information Theory, Human-computer interaction, Software Engineering, Linguistics, Ethics, Cognitive Science, Logic, Social sciences, political science, and psychology.

Currently, almost all functions implemented in NLP applications make heavy use of machine learning, especially deep learning, involving transformers, large-scale neural language models, and the like. These all necessitate large-scale data sources such as annotated and unannotated text, speech corpora, and large-scale computing resources to train.


Natural language processing has numerous practical applications within today’s world, such as question-answering systems or language translators. Here are a few of the more well-known applications:

1. Sentiment Analysis

As the name suggests, sentiment analysis is used to identify the sentiments in a fragment of natural language text. Expressions like sarcasm, threat, exclamation, etc., are often very hard for computers to recognize. A use case of sentiment analysis is when companies identify the opinions and sentiments of their customers based on online reviews and feedback.

2. Information Extraction

Information Extraction is the task of extracting pre-specified types of facts from written texts or speech transcripts and converting them into structured representations (e.g., databases). An example is when your email extracts only the relevant data of a meeting invite for you to add to your Calendar.

3. Machine Translation

Machine Translation is the procedure of automatically converting the text from one language to another language while keeping the meaning intact. In earlier days, machine translation systems were dictionary- and rule-based systems, and they saw very limited success. However, starting in the mid-1990s, first due to statistical models based on large parallel corpora and then to deep neural networks coupled with special processing hardware such as GPUs and TPU (Tensor Processing Units), machine translation has become fairly accurate in converting text from one language to another.

4. Natural Language Generation

Natural language generation (NLG) is the process of producing a human language text response based on some data input. This text can also be converted into a speech format through text-to-speech services. NLG also encompasses text summarization capabilities that generate summaries from one or more documents while maintaining the integrity of the information. Natural language generation systems have evolved over time from static text generation with the application of hidden Markov chains, recurrent neural networks, and transformers to enable more dynamic text generation in real-time.

5. Topic Modeling

Topic modeling refers to the task of identifying topics that best describe a set of documents. These topics are not predefined and will only emerge during the topic modeling process, which makes it an unsupervised approach. We will delve into the implementation of one of the popular topic modeling techniques known as Latent Dirichlet Allocation (LDA) in Project 5.



A language processing system will rely on different representation choices for capturing relevant aspects of the language input and output. These representations typically depend on the task and what is needed in downstream processing in the pipeline.

Earlier you were introduced to the NLTK tools. The book Natural Language Processing with Python discusses NLTK tools in detail. We won't go too deeply into it now, but it would be helpful to read the first two sections of chapter 8, Analyzing Sentence Structure .

A typical classical NLP pipeline uses at least the representation levels, as shown in the following figure.


Phonetic and Phonological Representations

Phonetics is the study of speech sounds as physical entities (their articulation, acoustic properties, and how they are perceived), while phonology is the study of the organization and function of speech sounds as part of the grammar of a language. Knowledge of phonetics and phonology is pivotal for applications that require understanding or generating speech data, like digital voice assistants, text-to-speech generators, etc.

For instance, speech recognition systems analyze (representations of) waves of air pressure (originally) generated by a human speaking and classify segments of such waves into abstractions called phonemes. Sequences of such phonemes are then transcribed into orthographic symbols making up words taken into context, usually through language models.

Morphological Representation

Morphology is the study of word structures, especially how morphemes , which are the smallest units of linguistic representation that come together and makeup words that can then be used to satisfy the semantic and syntactic constraints of a sentence. Morphemes can themselves be meaningful words that can appear by themselves in the language (free morphemes) or can be affixes that can only appear when combined with other morphemes (bound morphemes).

In many languages of the world, words typically consist of one or more morphemes, and these morphemes can combine in many different ways to build words (suffixation, prefixation, infixation, interdigitation, etc. A typical morphological takes in an orthographical representation of a word and generates a representation of all possible morphological interpretations of that word. For instance, a word such as books can be segmented into morphemes as book+s, and then this segmentation can be interpreted as either book+Noun+Pl (the plural form of the noun book) or book+Verb+Pres+3PSg (third-person singular form of the present form of the verb (to) book).

Lexeme Representation

A morphological representation does not necessarily capture all the information in a word (or sometimes in a sequence of words). The lexeme representation typically adds additional information to a word representation, such as the sense of the root word (e.g., when we use the word “banks,” – are we referring to “banks on the Wall Street” or are we referring to the “banks of the river”? At this level, we also perhaps conjoin words that work together (e.g., look up or piss off) and treat those as a single lexeme.

Syntactic Representation

As one may already guess, not every sequence of words constitutes a valid sentence in a natural language. Consider, for instance, the following sentences:

I want a flight to Tokyo

I want to fly to Tokyo

I found a flight to Tokyo

I found to fly to Tokyo

The first three look fine with our understanding of valid English sentences, but the last one does not. Furthermore, we sort of know that in the first sentence, “to” goes with “Tokyo,” “a” goes with “flight,” and “to Tokyo” goes with “a flight” and “I” and “a flight to Tokyo” go with “want,” the main verb of the sentence. Such relationships are hierarchical and can be captured with linguistic computational formalisms called grammars.

Grammars assign structure to valid sentences in a language. But at the syntax level, validity is only about the structure and not the meaning of a sentence. For example, the sentence “Colorless green ideas sleep furiously” is a syntactically perfectly valid sentence, but semantically it is nonsense.

The syntactic representation of sentences is hierarchical: two commonly used representations are constituency syntax trees based on grammar expressed using context-free grammar formalism rules and dependency trees based on lexical relationships between words.

For example, the following tree representation captures the structure of the sentence, “A boy with a flower sees a girl with a telecope.” The various symbols, such as NP (noun phrase) or VP (verb phrase), are names of various intermediate structure types as defined by the underlying grammar.


Here the structure is for the interpretation of this sentence where the boy is using the telescope to see the girl.

The sentence can also have the following tree representation:


This is for the interpretation where the girl is carrying a telescope!

This brings out another major issue in NLP: there are usually a multiplicity of representations for almost all inputs (remember the two possible interpretations of “books” above, which need further context to resolve during actual processing). Rerouting such ambiguities at every level of linguistic representation is probably the hardest problem in NLP.

A more recently commonly used syntactic representation relies on dependency relationships between lexical items, forgoing any use of the intermediate structure or phrase types in the trees and representing lexical relations between headwords and dependents, with a label denoting the relation as shown here.


Here “saw” is the main meaning carrier of the sentence. “saw” has the subject “kids” and a direct object, “birds.” “fish” is related to “birds” as a prepositional object which itself is related to “with,” which is a preposition.

Semantic Representation

Loosely speaking, this level represents the “meaning” of a sentence, sometimes compositionally scaffolding the structure of a sentence as described by a syntactic representation. Early approaches to semantic representation have assumed rather discrete representations of entities, properties, and events in a “world model” and have employed formalisms such as formal logic to capture what is called the truth-conditional semantics of a sentence. A sentence such as “Everybody has something they like” would be represented by a logical form such as 
 . The true value of such a sentence can then be computed based on the description of the world model.

A less formal but potentially more useful approach to semantics has been flatter but still hierarchical representations using semantic roles. Such representations assign the same semantic representation to syntactically different sentences if those express essentially the same event. For example, all these sentences:

Warren bought the stock.

Someone sold the stock to Warren.

The stock was bought by Warren.

are describing the same “selling” event where the buyer is Warren, stocks are sold, and the seller is not known or not expressed explicitly, but it is inherent. Thus the semantic representation for these sentences will be the same. There have been many similar approaches proposed along the same lines differing in the types of roles and granularity of how events are represented.

Much more recent approaches to semantic representation, especially in deep learning contexts, rely on embeddings computed by either running the embeddings of individual words through an encoder (e.g., in a machine translation system) or usually by even just adding up the embeddings of individual words to get a representation of the sentence.

Pragmatics

Pragmatics deals with understanding how the context in an utterance is made, or a sentence is used to contribute to the overall meaning and communicative intent and which aspects of a context are relevant to the interpretation of the utterance of a sentence. Such contextual information also includes intonation, physical gestures, and social identity. For example, an utterance such as “Can you pass the salt? “ in a dinner set is really not a question of someone’s ability to pass the salt but is rather interpreted as a gentle request.

Thus pragmatics requires representation of all aspects of the context, including the set of all propositions that all discourse participants in agree on for the purpose of going on with the discourse.

Discourse

A sequence of natural language sentences incrementally describes a local model of entities and the (evolving) relations between them. This model is known as the discourse model, and we, as the understander of the text, interpret linguistic expressions in the sentences with respect to this mental model that the understander of the text builds incrementally as we read, containing representations of the entities referred to in the text, their properties and the relations among them. This mental model already assumes a jointly agreed world model (e.g., everyone “knows” New York City or “Bill Clinto”), and one introduces entities that will be mentioned by naming them the first time they need to be mentioned and then as the text develops uses a variety of linguistic referring expressions to refer to these entities as needed.

Furthermore, not every possible sequence of sentences constitutes a meaningful discourse. Consider the following two sequences of sentences:

Eric is a pathetic programmer. He only knows Java. Worse still, he always optimizes the outermost loop first. However, the incompetence of his managers ensures him a steady, six-figure income.

Worse still, he always optimizes the outermost loop first. Eric is a pathetic programmer. However, the incompetence of his managers ensures him a steady, six-figure income. He only knows Java.

Clearly, only the first of these “makes sense”; the second is not something we are likely to see feel that while we probably understand each sentence, we have a feeling that the whole thing does not “make sense.”

A sentence sequence has to exhibit hard-to-define properties to be interpreted as a discourse: They have to have cohesion and coherence. Cohesion refers to the degree to which two passages of speech/text are “held together” by formal devices like shared words and discourse markers that indicate continuity or lack of continuity. On the other hand, coherence refers to the degree to which passages in a text have “meaningful relationships.”

Representation in Neural Models

Recent work in NLP has been using a representational paradigm based on a real vector representation of words. Such representation represents not only the identity of words (as a lexicon would) but also their semantics by capturing aggregate contexts words appear in to represent word semantics. The idea of such representations is actually quite old and goes back to what is known as the distributional hypothesis, first put forward in the 1950s. This hypothesis basically states that “Words that occur in similar contexts tend to have similar meanings.”

Such representations have been instantiated with the notion of embeddings which can be computed directly from the distributions of words in large amounts of text using a variety of algorithms, such as word2vec or glove embedding algorithms. Recent NLP algorithms that make use of the meanings of words use embeddings. Basic embeddings can be static since the computations rely on the orthography of individual words. Thus words with multiple meanings, such as “book,” “bank,” or “down,” get an embedding that lumps the semantics of all different meanings into one vector. Recent large transformer models such as BERT can compute contextualized embedding from static embeddings as input when a sentence is an input. These contextualized embeddings capture different uses of an ambiguous word and are typically different for each distinct user/meaning of a word.



Language data could refer to data from multiple sources like speech, image, video, and text data. Thus, language processing tasks applied to preprocess this largely depend on the form of data and its source. The rest of this module focuses primarily on text data as the input and output.

1. Tokenization

Tokenization is the process of splitting an input sentence, paragraph, or entire document into a list of tokens, where each token is a linguistic unit in speech or writing. For example, the string sentence "Today is a good day.", when tokenized, would yield ['Today', 'is', 'a', 'good', 'day', '. ']. In general, tokenization may split independent punctuation but may need to keep some punctuation if those are deemed to be part of a token. For instance, in “Dr. Smith…” the token is “Dr.” and not “Dr.” In English, sometimes tokenizers choose to split contracted words, e.g., “John’s” is split as John and “’s.’” The downstream task in the pipeline may choose the interpret the punctuation or ignore them.

Text in other languages may need to go through more complex operations during tokenization, depending on their writing systems. For instance, Chinese is typically written without any spaces between words, and a sequence of characters needs to be split into individual words before further processing using a process known as word segmentation. On the other hand, Arabic text is written without short vowels, and for certain downstream applications, one may need to add these through a process known as diacritization.

2. Stop word removal

Stop words are words that typically belong to a closed class of words in the vocabulary of a language that themselves do not carry meaning but function in a sentence to get the syntactic relations right. Typical stop word lists in English words such as “a,” “the,” “in,” “on,” etc. They are also rather frequently used words by nature and usually interfere with some downstream tasks. For example, applications such as document classification rarely benefit from such words. Thus once tokenization is done, one may need to remove all such words in a process called stop word remove. This can be done by sorting the vocabulary of the text collection by frequency and defining the top 10–100 vocabulary entries as stop words, or alternatively by using one of the many predefined stop words lists available online.

3. Morphological Analysis, Lemmatization, Stemming

Many language processing tasks, such as spelling checking and correction, parsing, and surface generation, either need to extract and process the information encoded in words or synthesize words from available semantic and syntactic information. This is especially necessary for languages with rich(er) morphology, such as German, Hungarian, Finnish, Turkish, and Arabic, to name a few.

Morphological analysis typically segments words into their constituent morphemes, taking into account any orthographical variations in orthography dues to morphology. For instance, while processing a word such as “stopped,” a morphological analyzer would need to know about the root words “stop” and the suffix “-ing” and that under certain phonological circumstances, certain consonants at the end of the root words need top duplicate when the following suffix starts with a vowel (cf. “stops”). It would then represent this word with something like stop+Verb+Past. Similarly, a word like “easiest” would be segmented as “easy+est” using an orthographical rule in English that changes a stem final “-y” to an “-i” in orthography, as there is no difference in pronunciation. The output representation would be easy+Adj+Super. Other more mundane mappings include handling special cases such as generating “go” as the root words for words such as “went” or “gone.”

Morphologically complex languages have many other orthographical processes usually rooted in phonology, such as vowel harmony, consonant and vowel insertions or deletions, or duplications. Morphological analyzers would have to take all of these into account in order to analyze words.

The state-of-the-art tools for morphological analysis rely on the well-established computational formalism of finite state transducers. There are numerous toolkits that take in a description of the root and affix lexicon of a language and compile these into large finite state transducers which take in a word in a language and generate representations for all possible morphological interpretations of a word. A side benefit of finite state transducers is that they a bidirectional and given a morphological analysis. They can produce the actual word.

When full morphological information is not necessary or not available, a “lighter” operation called stemming can be used. Stemming refers to heuristically stripping off known word endings to get to a base word (that itself may not be an actual word) that can be used as a proxy for the word, especially in tasks where morphological details are necessarily needed. Stemming in English, for instance, maps "change," "changing," "changes" to "chang." Porter Stemmer is a popular algorithm used for stemming for English? Stemming has limited applicability in languages whose morphology is more complex than English.

A slightly more accurate version of stemming is called lemmatization which does a more informed version of stemming using an additional lexicon and predicts the actual stem of the word or its lemma – the standardized form to look the word up in a dictionary. In the examples above, it should return “change” as the lemma.

Of course, a full morphological analyzer would do lemmatization in addition to interpreting the affixes and would be a tool of choice for languages with complex morphology.

5. Part-of-Speech Tagging

Morphological analyzers typically assign words a morphological interpretation in addition to a lemma. Such information includes the category of a word’s morphological interpretation and any additional syntactically (or semantically) relevant information, such as whether the word is a common or a proper noun, whether it is a singular or plural noun, or what the tense of a verb is, etc.

For English, such morphological information has been coded as part-of-speech tags. The Penn Treebank part-of-speech tag set has been established as the de facto part-of-speech coding scheme for English. More recently, the universal dependencies project has established a smaller common tag set for many languages, including English.

The Penn Treebank convention assigns short symbols to words as a part-of-speech category. For example, “books” would get both VBZ (third person present tense verb) and NNS (plural common noun), while “went” would be VBD (past tense verb), and “gone” would get VBN (past participle verb). Many words are ambiguous with respect to part-of-speech (such as “books” earlier). For example, the word “word” has 6 possible part-of-speech categories:

Adverb (RB) “...up and down Florida…”

Particle (RP) “ ..keep the ball down…”

Preposition (IN) “..down the center”

Adjective (JJ) “..down payment..”

Verb (VBP) “we down five glasses of beer every night”

Noun (NN) “they fill the comforter with down”

In general, almost every word in an English sentence will have multiple POS tags. For example. In the rather artificial sentence,

He can can the can.
the first “can” is a modal verb (MD), the second “can” is a tenseless verb (VB), and the third “can” is a singular noun (NN).

The task of determining the contextually correct POS tag for a word in a sentence is called part-of-speech-tagging (POS tagging).

POS tagging for English is typically done with a sequence-to-sequence mapping approach, with the sequence of the words going in and the sequence of POS tags coming out. Such approaches are trained with manually tagged data from the Penn treebank.

Early such approaches for POS tagging for English were based on Hidden Markov Models, which were trained with standard training data and applied to unseen data, with additional heuristic provisions for handling previously unseen words (e.g., googling was not in the Penn Treebank). The tagging model of transition and emission probabilities learned can then be used with the Viterbi algorithm to select the most likely sequence of tags for the words in an input sentence.

Later more sophisticated but computationally more expensive approaches, such as Conditional Random Fields, were used. These days any machine learning approach for sequence-to-sequence transformation can be used, including recurrent neural networks, etc., provisions in certain tasks like statement disambiguation , sentiment analysis , etc.

6. Named Entity Recognition

While processing text, it is often useful to identify phrases from the text that represent named entities: people, locations, organizations, geopolitical entities, product brand names, etc. Named Entity Recognition (NER) seeks to identify the span and the types of these named entities and group them. Although they are not necessarily named, referrals to dates, times, and other kinds of temporal expressions and prices can be considered under the same umbrella.


Figure 1. Named Entity Recognition with spacy .

NER is another example of sequence-to-sequence transformation. We represent the named entities and their spans using a scheme known as BIO encoding. For each named entity category, we have two symbols, one with B and one with I. So, for example, B-PER labels a token that begins a person-named entity, while I-PER labels any token of a person's name (of a length greater than one) that is inside a named entity. O labels any word that is outside a named entity. Thus with k different categories of named entities, there is a set of 
 labels.

In the figure earlier, any token that is not colored gets the O label. In the last colored blue colored named entity, “The” would get B-ORG label, and “Justice” and “Department” would get the I-ORG label.

The sequence-to-sequence transformation then takes in the sequence of tokens and uses a classifier to assign one of these labels to each of the tokens. Of course, sophisticated named-entity recognizers also employ other sources of information such as POS tags and capitalization information for the tokens, any phrase markers, etc. Any machine learning approach that can perform classification can be used to implement NER, as in Figure 2.


Figure 2: NER as a classifier – From “Jurafsky and Martin, Speech and Language Processing, 2nd Edition.”

A training set of labeled named entities is used to train a classifier which then performs NER on new sentences.

NER systems are evaluated by three metrics: precision, recall, and 
 as shown in the figure below.


7. Parsing

Parsing is the process of assigning syntactic structures to a sentence. As we saw earlier, the syntactic structures are represented with either constituency trees or dependency trees. A constituency parser would be based on typically context-free grammar and produces one or parse trees for an input sentence. The parse trees are relative to the grammar, and different grammars would produce different structures. The Cocke-Younger-Kasami (CYK) algorithm based on dynamic programming can produce a chart in which all parses of a sentence are encoded in 
 time 
 being the number of tokens in the input sentence. Extracting all individual parses may take substantially more effort as there may be an exponential number of trees for some edge cases. While the CYK algorithm requires context-free grammar in what is called the Chomsky normal form and may seem restrictive, any context-free grammar can be converted to a Chomsky-normal form grammar with a larger number of intermediate categories at the expense of obtaining structures that may not be linguistically sensible.

Thus if the input sentence is “A boy with a flower sees a girl with a telescope.” the parser would generate the following two parse trees:



but it would not be able to tell which of these parses is the “correct” one.

Manually building grammar for parsing is not a feasible task. Treebanks, like the Penn Treebank discussed above, can be used to build statistical parsers by (1) inducing a large-scale grammar and (2) associating probabilities with the rules in the grammar. Thus the CYK algorithm about can be augmented to produce the most likely parse of an input sentence in the same amount of time.

For dependency parsing, there are several parsing methods. While it is not necessarily the most accurate method, transition-based dependency parsing is the most commonly used technique. Using a stack essentially makes one pass over the input words and decides in 
 time on the dependency relations between lexical items.

Fundamentally a transition-based parse is a classifier that, at any point in time, looks at what is on the stack and what is left in the rest of the input to decide on action toward building a dependency parse tree. Such a classifier can be trained by using training data obtained by transforming a dependency treebank into a sequence of parser actions.

Other dependency parsing algorithms typically employ more sophisticated algorithms. Graph-based methods typically label a graphical representation of the sentence and assign weights to each arc representing word-to-word relations and then extract the maximum spanning tree to represent the most likely parse.

_____ focuses on the study of the formation of words & analyzing their constituent parts.

Morphology ++

Pragmatics

Semantic Analysis

Syntactic Analysis

Correct. Morphology is the study of word structures, especially regarding morphemes, which are the smallest units of language.

Consider the following statements:

A. The main goal of stemming is to reduce the words to a base form which is not the goal for lemmatization.

B. Stemming generally uses a crude heuristic process that chops off the ends of words, while lemmatization depends on morphological analysis of the words.

C. Named Entity Recognition (NER) must be performed before performing Part of Speech (PoS) tagging.

D. Tokenization is used to generate a more efficient representation of text data.

E. Stopword removal is the process of removing frequently occurring words that are not relevant for a Language Processing task.

Which is (are) true about Language Processing Tasks?

B and E

A is incorrect. The main goal of both lemmatization and stemming is to reduce a word to a base form.

B is correct. Stemming often uses a crude heuristic to chop off the word ends to reduce it to a base form, but lemmatization requires morphological analysis to get the base form of the word.

C is incorrect. PoS and NER recognition can both be performed independently.

D is incorrect. Tokenization is done to analyze the smaller components(tokens) of text data, and efficient representation is not its primary goal of it.

E is correct. Stopword removal is the process of filtering out words that are not inherently meaningful and hence, are not relevant to the task.



Textual data cannot be used directly as model inputs as models typically require numerically represented features. Applying the text processing tasks mentioned in the previous section helps streamline textual data into a form that can be easily constructed to numerical features using any one of the methods, like bag-of-words, term frequency, word embeddings, etc., based on the use-case.

1. Bag-of-Words

Bag-of-Words is a primitive feature construction method that can be employed for simple problems to obtain quick results. It translates the entire text corpus into a vector with word counts. It also assumes that the vocabulary is fixed and the size of the vocabulary determines the size of the vector, with each entry in the vector representing how many times a word occurs in the document. Since only a small number of the words in a vocabulary would be used in a document, such a representation would be very sparse, with many of the entries in the vectors being 0.

2. Term-Frequency

Instead of representing the entire corpus as a one-dimensional list of numbers indicating word counts, term frequency takes into account the word frequencies for each member document in the corpus.

Consider the corpus = [ “Jack ate an apple”, “an apple on the table”, “Jack likes the apple” ]

Bag-of-words will create a one-dimensional vector for this entire corpus:

Jack

ate

an

apple

on

the

table

likes

2

2

2

3

2

2

1

1

However, the term-frequency matrix will have a row corresponding to each of the three documents:

Jack

ate

an

apple

on

the

table

likes

Document #1

1

1

1

1

0

0

0

0

Document #2

0

0

1

1

1

1

1

0

Document #3

1

0

0

1

0

1

0

1

3. Term-Frequency: Inverse-Document Frequency

Term-frequency Inverse-Document Frequency (tf-idf) treats frequency counts for each document of a corpus distinctly like in Term Frequency (Tf). The Tf method discussed earlier assigns an importance value to all the words purely based on their frequency. Hence, features corresponding to words that appear more frequently, like stopwords, get assigned a large value in comparison to words that could potentially warrant higher importance in the corpus. Therefore, tf-idf is a way to “normalize” these high-frequency values. In tf-idf, for any word not in the corpus, we can either ignore it or consider its frequency under another/foreign word column .

Each word has an inverse document frequency associated with it. Hence,

 

Intuitively, the IDF of a word that appears in fewer documents is higher. (Now, using the above formula, what is the IDF of a word that appears in every document? What could be the highest IDF value a word could get?)

Finally, tf-idf for a word j in document i is calculated by multiplying the IDF score for word along j with the Tf for word j in document i :


4. Word Embeddings

All the above feature construction methods are easy to visualize and understand. However, it represents individual words as dimensions and thus tends to suffer from the curse of dimensionality. Word embeddings are representations of words in a meaningful low-dimensional space whose dimensionality is a fixed number independent of the word count in the corpus. Intuitively, Words that are placed closer in this space are expected to be similar in meaning. For instance, the position of “Seattle” is closer to “Boston” than it is to “talk” in the below illustration of word embeddings.


Figure 1. Word Embeddings (Source: IBM Research Blog )

As discussed in an earlier module, word embeddings can be generated by training a model from scratch or through pre-trained models like BERT (introduced later in the course), which brings down the training time significantly. Due to the advantages of word embeddings over other methods in capturing context and minimizing memory used for feature representation, it is increasingly used for deep learning and advanced NLP tasks, some of which will be discussed in the next section.



While methods like Bag-of-words and term frequency are simple yet highly effective techniques, they don’t take the context of relative positivity between words into consideration. For example, “good food and terrible service” and “terrible food and good service” mean completely different things, although frequency-based methods would model them to be the same. Language models take into account this additional relationship between words that helps represent language data more accurately.

Probabilistic language models are a category of language models that are constructed by calculating n-gram probabilities (an n-gram being an n-word sequence, n being an integer greater than 0). An n-gram’s probability is the conditional probability that the n-gram’s last word follows the particular n-1 gram (leaving out the last word). For instance, the Bi-gram (that is, n=2) model for the phrase “good food and terrible service” would require modeling conditional probabilities of every two consecutive words. With n=2, each word is modeled with one preceding word, like, P(word = “food”/”good”), P(word = “service”/”terrible”).

 

Where the probability 
 of a token 
 given the preceding token 
 is equal to the probability of their bigram 
 , divided by the probability of the preceding token.

Given a sentence in a language, a language model will use these probabilities to assign an overall probability to the sentence, which can be interpreted as a useful measure of the plausibility of that sentence in the language (but not necessarily of grammaticality.) For example, the sentences “Big blue skies look appealing.” and “Colorless green ideas sleep furiously.” have the same grammatical structure, but to a speaker of the language, the first is a much more plausible sentence than the second one – a sentence she can say someone could use.

Such probabilities can be estimated from large-scale text corpora using maximum-likelihood estimation. Various smoothing methods are used to estimate probabilities for n-grams that have not been observed in the training data. Such language models are useful in many language processing tasks, such as contextual spelling correction, part-of-speech tagging, etc.

These days people build (classical) language models using well-established toolkits:

SRILM Toolkit: https://www.sri.com/engage/products-solutions/sri-language-modeling-toolkit

CMU Statistical Language Modeling Toolkit: http://www.cs.cmu.edu/~dorcas/toolkit_documentation.html

KenLM Language Model Toolkit: https://kheafield.com/code/kenlm/

Each toolkit provides executables and/or API and options to build, smooth, evaluate and use language models.




The Bayes Theorem describes the probability of an event based on prior knowledge of conditions related to that event. If you want to assess the risk of a person developing macular degeneration, the Bayes theorem supports accurately assessing that risk based on a certain age range instead of making assumptions.


Bayes Rule (Source: https://www.psychologyinaction.org)

Additional Reading: Bayes Theorem

Additional Reading: Overview of Bayesian Statistics

Bayesian Inference is applied when the Bayes theorem seeks to update the probability for a hypothesis as more information becomes available. It is used in sports, medicine, and law, among other fields. Bayesian Inference derives the posterior probability as a consequence of a likelihood function and a prior probability. It is not the only updating rule, but it is widely used.

Naive Bayes (NB) Method

Named after Reverend Thomas Bayes, naive Bayes is a simple classifier that can be applied to categorical predictors. When classifying observations using NB, the classifier computes the probability for all possible classes given all the observed evidence and then classifies the observation as belonging to the class with the maximum posterior probability. When the problem calls for predicting the probability that an observation belongs to a class, we can use this method. Naive Bayes is based on applying the Bayes theorem and assumes that all predictors or observed features are independent. Although this is a naive assumption, naive Bayes performs quite well for real-world applications. A fruit that is green, round, and 18cm in diameter can be considered to be a honeydew melon. The NB classifier will assume that all these features independently contribute to the probability that the fruit with these features is honeydew melon. Naive Bayes can perform well with a small training dataset for estimating the right parameters for a classification task. A downside to this model outside of its naivety is that studies have been conducted, showing it does not perform as well as methods like random forests. NB is said to be a good classifier, but as an estimator, its probability outputs are not as strong. When model complexity is not important, NB can be used for high-dimensional data. This is because when the dimension of a dataset is large, data points are more likely to be further apart than in cases with low-dimensional data.

NB is not considered the go-to algorithm for estimating the probability of an observation's class as it is biased in its results, but it is quite useful for ranking and classification tasks. Assume that you introduce a new observation to your model, and this new observation has a categorical feature that has not been observed in the training dataset. NB will compute a zero probability to that record. Let's put this in a real context: if your response has diabetes, and a predictor category is past pregnancy. Now assume that your training dataset has all observations with past pregnancy =0. All new observations with past pregnancy =1 will be classified as not having diabetes.

There are other Bayesian Methods that can be used in Data Science, these are explored in machine learning and applied statistics courses.

Have you come across the term "lazy learning"? This is a method in which training data is generalized and is most useful for large datasets that will be updated continuously. In such a case, a model typically depends on (or queries) a small number of attributes in the dataset. An application of a lazy learner is a recommendation system. A recommendation system relies on certain variables such as ratings, pricing, and country of origin and will continuously update as information on new movies or new items in shoppers' preferences is available.

The opposite of a lazy learning method is an eager learning method. An eager learner usually requires less space than a lazy learner. An eager learner will learn immediately and sometimes for a long time but will take a shorter time to classify data. A lazy learner will typically learn for a short time but take a longer time to classify data.

A well-known lazy learner is the k-Nearest Neighbor (kNN) method. This method can be used to solve both classification and regression problems. kNN is considered rather simple yet useful and is one of the first algorithms or methods that entrants to data science will learn (kNN is also simple to implement in Python or R). kNN will find a predefined number of training samples closest in the distance to a new point or a new observation and predict the label for the new observation. kNN, however, can also suffer from the curse of dimensionality. This method will perform best when data is rescaled. It is best practice to normalize applicable data to the range of 0,1 and to standardize the data if it has a Gaussian distribution .

kNN will perform its task just in time as it does not learn a model but keeps its training observations in an explicit form, so it spends less time learning and more time classifying or predicting.

There are three steps involved in making predictions with kNN: Compute the Euclidean distance of the new observation to previous classified observations, identify the nearest neighbor(s), and then perform the classification.

Reading: k-Nearest Neighbors: From Global to Local

The kNN method begins by identifying the observations (or the k records) in the training dataset that are similar to a new observation that should be classified. The neighboring observations can be used to classify the new observation into its class. That class should be its predominant class. Since this method is considered a non-parametric method, it will take information from similarities between the predictor values of the observations in the dataset. The kNN method uses distance computations to determine the distance between each new observation and the observations in the training dataset. The most popular distance measure used in kNN is the Euclidean Distance. The formula for the Euclidean distance between observations (
 and 
 ) is shown below:


Euclidean Distance Function. (Source: Sayad (2010))

There are other distance measures, including Manhattan (distance between vectors using the sum of their absolute differences) and Minkowski distances. Euclidean distance is used because it is computationally cheap (keep in mind that predictors should be standardized before implementing the Euclidean distance function). Euclidean distance will not work with categorical variables unless they are converted into binary dummy variables. An alternative distance measure, in this case, would be the Hamming Distance , which can be used as long as you do not have more than two (2) classes.

Once distances are computed, the class that a new observation will be assigned is based on the classes of its neighbors. Now, you can see why kNN is considered a similarity function.

How do you determine the number of neighbors to assess, so that a new observation or data point can be classified correctly? You can, for example, determine that the new data should be classified in the same class as its single closest/nearest neighbor, i.e., k = 1. Can you guess what will happen in this situation? You are right if you guessed that you would face overfitting. You can mitigate this by using a k that is greater than 1. If you assign k = n, i.e., n being the number of observations in the dataset, there will be over-smoothing, and all new data will be assigned to the majority class. Values of k are historically between 3 and 10 and usually an odd number to avoid ties. Sometimes k can be chosen using cross-validation, and the validation dataset will validate the best k.

Although kNN is helpful for predicting a categorical response, it is also effective for predicting continuous value responses just like one would with a linear regression model. The main difference between predicting a categorical response and a continuous value is that the algorithm will use the average (usually weighted) response of the neighbors of the new data point to determine the accurate prediction. The weight will decrease as the distance increases from the data point at which the prediction is required. The best k for a classification task is assessed using the overall error rate but in this instance, the best k is determined using the root mean square (RMS) error.




When we want to study the hidden structure of data and identify different groups within that structure, we use the Cluster Analysis technique. Once groups are constructed, it is safe to assume that data points within each group have similar features and are very dissimilar to data points in other groups. Cluster analysis is looking to define structure within a dataset.

There are different types of clustering techniques. We will briefly define the general idea at this point and fully explore it in an upcoming module.

K-means clustering is a widely used clustering technique that computes the distance between data points in a group and the center of the group. The number of clusters (k) is decided before the process begins, and K-means clustering can only be applied to numerical variables. This is because it solely uses Euclidean distance as a similarity measure to form clusters.

It is not uncommon to begin by randomly selecting a number of observations from the data as the initial cluster center, the remaining observations will then be assigned to the nearest cluster center. The algorithm continues to assign and reassign observations to their closest clusters by computing the cluster centroids (the middle of a cluster). The reassignment is done to minimize dispersion within clusters.

Hierarchical clustering connects data points to form clusters based on their distance and is also known as connectivity-based clustering. Each data point is considered its own cluster at the start of the process, and then the algorithm groups clusters based on similarity until true clusters are formed. This is also known as agglomerative clustering.

There is another approach of hierarchical clustering that puts all data points in one cluster and then separates them based on dissimilarity until different clusters are formed. This is called divisive clustering.

Hierarchical clusters are formed and represented using a dendrogram (shown below). The y-axis of a dendrogram marks the distance where clusters merge, and data points are placed on the x-axis.


Example of a Dendrogram. (Source: Mathlab).

Similar to regression and classification techniques, clustering output should be evaluated. Evaluation methods differ based on the kind of clustering technique used to meet your analytic objective. The next sections will focus on the different types of clustering techniques and the evaluation techniques that apply to each technique.


Unsupervised Learning Techniques
When we want to identify patterns in a dataset from unlabeled data, we use unsupervised learning to perform this task. Unsupervised learning is also referred to as self-organizing. We had already seen an example of unsupervised learning when we studied Principal Component Analysis while discussing feature engineering. We will also look further into the different types of cluster analysis techniques.

You can categorize data according to characteristics using a technique called Cluster Analysis. If you think about how we reason and learn as human beings, we make sense of events, people, and things by placing them in groups. You have memories that are characterized as happy and sad, or people categorized into close friends, acquaintances, and mentors, among others. You might similarly consider clustering data to identify those with similarities as a method of exploring data. Applications of cluster analysis include market segmentation - the segmenting of customer data based on certain criteria, including transaction history. The different clusters created from the segmentation exercise are useful for targeted advertising or the application of customized marketing strategies that will elicit positive responses and increase sales and engagement.


Clustering (Source: www.pyarmy.com)

Types of Clustering
Hard Clustering divides data into a number of groups, and data points can only belong to one cluster. All clusters are independent of each other.

Soft Clustering groups data into clusters, but a data point can belong to more than one cluster to a degree.

Overlapping Clustering allows data to belong to more than one cluster.

Hierarchical Clustering organizes data in a hierarchical manner so that the hierarchies are represented by a dendrogram.

Reading: An Expansion on Clustering .

k-Means Clustering
This method involves identifying the number of clusters k that the dataset will be grouped into. The data in each cluster will share similarities to the other data points within its cluster. Assume you have k = 5. Cluster 1 will contain data that is homogeneous but quite dissimilar to the records in cluster 5. The data within clusters adhere to distance measures to ensure that dispersion is minimized. k-Means Clustering technique abides by a number of distance measures, but the most popular is the Euclidean distance. Let us look at how clusters are created using this technique:

A set of k means is chosen. These are meant to capture the mean of all the observations within the cluster, in general.

Each data point is then assigned to the cluster with the nearest mean.

After a pass, using the points assigned to a cluster, new means are computed.

The last two steps are repeated.

The algorithm converges when the assignments to cluster no longer change. The algorithm is not guaranteed to find the optimum.

How do we decide k? Similar to kNN, there are empirically studied recommendations for the best k to select. You can also select k based on previous knowledge (this is hardly the case with this unsupervised task). You can use different values for k and then compare the results obtained from each value of k. It is good practice to also run the k-Means cluster method by using different values for k based on the number of clusters that are expected from the data in order to see how the sum of distances reduces with increasing values of k.

k can also be chosen by calculating the Within Cluster Sum of Squares (WCSS). This is the sum of squares of the distances of each data point in relation to the centroids in the data points cluster.

Assuming that we have 1000 observations in a dataset, and we have decided that k = 1000, the WCSS should be zero (0). This is because all the observations are considered centroids, and there is technically no distance between the observation and the centroid within the cluster since it is the sole data point in its cluster. This is certainly not a computationally sensible way to cluster data. Think about a dataset with over 100,000 observations. Also, think about the information to be gleaned from the cluster analysis; you will lack useful information.

When you randomly initialize with a range of k values for the 1,000 observations mentioned above, i.e., between 2-10. You can use the Elbow method to find out the optimum value for k. The Elbow method produces a graph that shows this optimum value at the "elbow" of the line, as shown below. You select k as the WCSS decreases; the figure below shows that after 5, the decrease in WCSS is quite small.


Elbow Method

Reading: k-Means Clustering-sklearn .

Additional Reading: K-Means Clustering Algorithm

K-Means Clustering and k-Nearest Neighbors have been known to cause confusion for data scientists who are new to the field. After all, we are discussing similarity measures and distances to an observation to classify or cluster into a class. The main difference is that one is an unsupervised technique, and the other is supervised. kNN is a supervised classification method that involves labeled data that is used to train a model to accurately predict the class of a new observation according to its closest or neighbor data points.

K-means does not provide a labeled dataset to the model for learning purposes. K-means will partition the data into a number of clusters. kNN works best with data that is of the same scale, but k-means does not need the same scale data to perform well. Remember when you learned about kNN being a lazy learner? K-means is an eager learner. It is slow to train, but it tends to deal with noise in the training dataset better than a lazy learner.




Unsupervised Learning Techniques
When we want to identify patterns in a dataset from unlabeled data, we use unsupervised learning to perform this task. Unsupervised learning is also referred to as self-organizing. We had already seen an example of unsupervised learning when we studied Principal Component Analysis while discussing feature engineering. We will also look further into the different types of cluster analysis techniques.

You can categorize data according to characteristics using a technique called Cluster Analysis. If you think about how we reason and learn as human beings, we make sense of events, people, and things by placing them in groups. You have memories that are characterized as happy and sad, or people categorized into close friends, acquaintances, and mentors, among others. You might similarly consider clustering data to identify those with similarities as a method of exploring data. Applications of cluster analysis include market segmentation - the segmenting of customer data based on certain criteria, including transaction history. The different clusters created from the segmentation exercise are useful for targeted advertising or the application of customized marketing strategies that will elicit positive responses and increase sales and engagement.


Clustering (Source: www.pyarmy.com)

Types of Clustering
Hard Clustering divides data into a number of groups, and data points can only belong to one cluster. All clusters are independent of each other.

Soft Clustering groups data into clusters, but a data point can belong to more than one cluster to a degree.

Overlapping Clustering allows data to belong to more than one cluster.

Hierarchical Clustering organizes data in a hierarchical manner so that the hierarchies are represented by a dendrogram.

Reading: An Expansion on Clustering .

k-Means Clustering
This method involves identifying the number of clusters k that the dataset will be grouped into. The data in each cluster will share similarities to the other data points within its cluster. Assume you have k = 5. Cluster 1 will contain data that is homogeneous but quite dissimilar to the records in cluster 5. The data within clusters adhere to distance measures to ensure that dispersion is minimized. k-Means Clustering technique abides by a number of distance measures, but the most popular is the Euclidean distance. Let us look at how clusters are created using this technique:

A set of k means is chosen. These are meant to capture the mean of all the observations within the cluster, in general.

Each data point is then assigned to the cluster with the nearest mean.

After a pass, using the points assigned to a cluster, new means are computed.

The last two steps are repeated.

The algorithm converges when the assignments to cluster no longer change. The algorithm is not guaranteed to find the optimum.

How do we decide k? Similar to kNN, there are empirically studied recommendations for the best k to select. You can also select k based on previous knowledge (this is hardly the case with this unsupervised task). You can use different values for k and then compare the results obtained from each value of k. It is good practice to also run the k-Means cluster method by using different values for k based on the number of clusters that are expected from the data in order to see how the sum of distances reduces with increasing values of k.

k can also be chosen by calculating the Within Cluster Sum of Squares (WCSS). This is the sum of squares of the distances of each data point in relation to the centroids in the data points cluster.

Assuming that we have 1000 observations in a dataset, and we have decided that k = 1000, the WCSS should be zero (0). This is because all the observations are considered centroids, and there is technically no distance between the observation and the centroid within the cluster since it is the sole data point in its cluster. This is certainly not a computationally sensible way to cluster data. Think about a dataset with over 100,000 observations. Also, think about the information to be gleaned from the cluster analysis; you will lack useful information.

When you randomly initialize with a range of k values for the 1,000 observations mentioned above, i.e., between 2-10. You can use the Elbow method to find out the optimum value for k. The Elbow method produces a graph that shows this optimum value at the "elbow" of the line, as shown below. You select k as the WCSS decreases; the figure below shows that after 5, the decrease in WCSS is quite small.


Elbow Method

Reading: k-Means Clustering-sklearn .

Additional Reading: K-Means Clustering Algorithm

K-Means Clustering and k-Nearest Neighbors have been known to cause confusion for data scientists who are new to the field. After all, we are discussing similarity measures and distances to an observation to classify or cluster into a class. The main difference is that one is an unsupervised technique, and the other is supervised. kNN is a supervised classification method that involves labeled data that is used to train a model to accurately predict the class of a new observation according to its closest or neighbor data points.

K-means does not provide a labeled dataset to the model for learning purposes. K-means will partition the data into a number of clusters. kNN works best with data that is of the same scale, but k-means does not need the same scale data to perform well. Remember when you learned about kNN being a lazy learner? K-means is an eager learner. It is slow to train, but it tends to deal with noise in the training dataset better than a lazy learner.


When your output variable is a continuous value, you are able to make predictions using the widely known regression analysis. The input variables for a regression task can be categorical, discrete, or continuous data. So far, we have read about getting qualitative responses or output using classification techniques. Regression techniques return a quantitative response to a task. It is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s).

Thought: The delineation above does not mean that all supervised techniques will either return qualitative or quantitative responses. Some techniques that we will explore in the next modules can return both types of responses; those techniques include kNN, among others.

Regression is one of the easier techniques to implement. We perform regression analysis because it can highlight the impact of independent variables on a dependent variable. For example, one can tell the effect of changes in temperature and terrain on the outcome of a football game. Regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model. Regression is used for forecasting tasks as well. When the goal is to infer relationships between the values of variables x and y, one can again use regression techniques.

If the independent variables are highly correlated, we can say that the variables are multicollinear. If the correlation between two independent variables is 1 or -1, then you have perfect multicollinearity. One can detect multicollinearity when there are large changes in the estimated regression coefficient when an independent variable is added or removed.

A regression model will have certain components, including the independent variables, often denoted as X and the dependent variable Y. A regression model also accounts for random error 
 ; the random error is not found in the dataset. Instead, it is the difference between an expected outcome and an actual observation. It is usually an unpredictable occurrence that you can not account for in your dataset. Then, you have unknown parameters β. Your goal with a regression model is to estimate the function f(X, β) with the best fit to the data. The function f should be specified when performing regression analysis. This will ensure that you are deciding on the right regression methods to use.

When performing regression analysis, you might encounter data that has outliers. This is not handled during the data understanding phase. It can affect the results of your regression analysis.

Let us explore the different types of regression techniques in this section with the goal of exploring each technique further in the subsequent sections.

Linear Regression
This regression technique is used to model the relationship between independent variable x and dependent variable y. When you have two or more independent variables, you will represent them as the vector 
 , and k is the number of inputs. The model is said to be linear because the output is expected to be a linear combination of independent variables.

There is the simple linear regression model that allows for predicting a response based on one predictor variable. Most times, you will be predicting a response with multiple predictor variables. Single linear regression does not allow for multiple predictor variables, so instead of training multiple simple linear regression models for each predictor, you use the multiple linear regression method to account for multiple predictors.

This model can also be used for classification if you replace the gaussian output with a Bernoulli distribution.

Let us represent a regression model as:


The regression function for multiple linear regression is:


Y is a straight-line function of each independent variable X. The slopes of the individual straight-line relationships of
 with Y are the constants 
 , also known as the coefficients of the variables. One can interpret this to mean 
 is the change in the predicted value of your dependent variable Y per unit of change in 
 , with other things being equal. Consider 
 as the intercept (a prediction that your model will make if all the independent variables had zero values). You must also account for the random error $\epsilon$ in the equation.

You estimate 
 , and 
 using the Least Squares method. This method will minimize the sum of squared residuals (a residual is a difference between an observed value and the fitted value given by a model). The least squares method can be linear or ordinary, or nonlinear. Ordinary Least Squares choose the parameters of a linear function of a set of independent variables by the principle of least squares. Non-linear least squares will fit a set of observations with a model that is non-linear in unknown parameters; that is, it will approximate the model by a linear model and refine its parameters by iterations.

The Performance of a regression model can be assessed using the coefficient of determination or 
 . 
 measures the proportion of the variation in the dependent variable that is predictable from the independent variable(s). So, the larger the 
 , the better the model can explain the variation of the response with various predictors.


Ordinary Least Squares Source3

Reading: Four Principal Assumptions . These assumptions justify the use of linear regression models for prediction modeling. These assumptions should be met to avoid producing misleading analytic solutions and insights.

Polynomial Regression
When the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in x, this is called a polynomial regression. Pay attention to the figure below. You will note that using a linear regression line to fit the data would result in a high value for the error.


Trying to fit a simple linear regression line. Source4

Now refer to the image below to see the outcome when you fit a polynomial line through the data points. The polynomial regression provides a better view of the relationship between the y and x variables. So a polynomial regression can fit a broader range of functions. However, it is sensitive to outliers, and those outliers can affect the result of a polynomial regression analysis.


Polynomial regression with lower error. Source4

Stepwise Regression
When you have a regression analysis task, you might have multiple independent variables (and, in reality, you will), and you will need a method that fits the regression model with the most significant predictors. Stepwise Regression will increase the prediction power of a model with a minimum number of predictors. The process of fitting the model with the predictors is done automatically without human intervention. There are two techniques for stepwise regression:

Backward elimination tests the effect that each variable has on a model by deleting it. The deleted variables are those that have the "most statistically insignificant deterioration of the model fit." This technique should not be used if the number of predictors is more than the observations in the dataset.

Forward selection is the reverse of backward elimination. Variables are added to assess model fit and included if the variable shows a significant improvement to the fit.

We also have the mixed selection technique, which can be considered a hybrid selection method with the backward elimination and forward selection techniques.

Model Accuracy

Stepwise regression is prone to overfitting issues, and one way to guard against this is to check how significant the least significant variable will be based on chance. Model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or holding-out sample. You can check the extent to which a model fits the data with the residual standard error (RSE is the standard deviation of error 
 ), i.e., the average amount that the response will deviate from the true regression line. A large RSE means the model was not a good fit for the data, and the 
 is independent of your response variable, unlike the RSE.

 is calculated using the total sum of squares which is the total variance in Y, and RSS is the discrepancy between the data and an estimation model.

Selecting the Right Regression Method
The goodness of fit of a model will show how the model fits the data that it is trained with, and it will highlight any lack of balance between observations in the dataset and those that will be introduced to the model (new values). To select the right method, one can use the different metrics below:

AIC (Akaike Information Criterion). One chooses the model with the smallest AIC as the best model. The AIC puts more emphasis on the model performance on a training set and will tend to select more complex models.

BIC (Bayesian Information Criterion). A model with the lowest BIC is considered the best model. BIC is related to the AIC and is appropriate for models that fit under the maximum likelihood estimation. Unlike the AIC, BIC penalizes complex models.
AIC and BIC are proportional

 will increase as more dimensions are added to the dataset (this is considered a weakness of this metric). A value of 0 means that a model does not explain any variability, and 1 means the model explains full variability.

Adjusted 
 addresses the issue highlighted with n independent variable that has a strong correlation with the dependent variable increases the adjusted 
 and decreases it when a variable without a correlation to the dependent variable is added. When you have a model with more than one variable, the adjusted 
 is a suitable criterion to use.

Mallow's Cp is used to assess the fit of a regression model that has been estimated using ordinary least squares. The goal is to find the best model involving a subset of these predictors. Note that you want a small Cp.

We will continue to learn more about regression analysis in an upcoming module, and you are encouraged to locate the materials in the additional reading section to strengthen your knowledge of Regression analysis.

Additional Reading: Elements of Statistical Learning



Let us assume that you have a medical dataset that contains observations with features including patient age, weight, height, sex, and race, and you have been tasked with identifying whether a patient is diabetic or not. It would take a long time for you to review each observation and compare their feature value and symptoms to classical symptoms of diabetes. Using a data science approach, you can assign a diagnosis to each observation based on the historical data for that diagnosis. You would have a classification problem. Classification works with an existing dataset that has labeled outcomes and seeks to label the outcomes of a new, previously unseen dataset. Below, you will find the different types of classification problems. Later in the course, we will explore methods that can be used to solve classification problems.

Binary Classification

Classification tasks that are binary will classify observations in a dataset into two defined categories. The observations are grouped based on the presence of characteristics unique to one of the two categories. An example would be making a decision on a credit card application (i.e., approve/deny).

Multi-Class Classification

Multi-class classification also referred to as multinomial classification, classifies observations into one of three or more classes. Each observation can only be classified as one of the multiple classes. That is, an observation can not be labeled as belonging to more than one class. For example, if our task is to classify images with a single fruit in each, a classifier would classify each new image into one type of fruit, e.g., one of orange, pineapple, peach, and mango.

Multi-Label Classification

Unlike multi-class classification, which assumes that each observation belongs to one class, multi-label classification allows for observations to be classified under multiple classes hence the term multi-label classification.

A quick thought: Can you think about a scenario where observations can belong to multiple classes at once (thereby leading them to be labeled under those classes)?

Multi-label classification can be applied to, for example, classifying textual data. One important such application is document classification, the task of determining the topic of a document. It is conceivable that a document on politics can also be considered a business or an economics document – though perhaps not that strongly. Or, if you watch movies, you know that some movies can belong to multiple genres, e.g., Romantic Comedy, Romantic Drama, and Thriller Comedy, etc. Let us stick with this example and conceptually define how a multi-label classification task would pan out.

You are tasked to classify movies based on their plot. We can assume that we have defined our analytic objective, defined our requirements, and we have gathered and prepared our data. When you classify the observations in this dataset, you might find Movie A will belong to Romance and Comedy. Let us now look at the different multi-label classification techniques and see how they can handle problems with multi-labels without causing a dimensionality issue to your dataset and jeopardizing the performance of your model.

Multi-label Classification Techniques

Multi-label classification does not have constraints on the labels that observation can have, and this makes it difficult to learn. Using the OneVsRest Technique, the classifier makes the assumption that labels are mutually exclusive and there is no consideration for correlations between classes.

Similar to OneVsRest, the Binary Relevance technique trains a separate single-label binary classifier for each class, i.e., for each class, an observation will either be predicted as belonging to that class or not. This technique ignores any correlation between classes.

The algorithm for your classification task can also adapt the algorithm to perform multi-label classification. A popular example is using a multi-label version of the k-Nearest Neighbors (kNN), a supervised learning technique we saw before that makes the assumption that similar data points are always close together.

Example: scikit-multilearn for MLkNN .

You can transform your task into a multi-class task by training all unique class combinations on one multi-class classifier.

X

Y1

Y2

Y3

X1

0

1

1

X2

1

1

0

X3

1

1

0

X4

1

0

1

Here we see that observations X2 and X3 belong to the same classes. This technique will transform our task into a single multi-class task and give a unique class to all possible combinations in your training data set.

X

Y1

X1

1

X2

2

X3

2

X4

3

So far, we have seen that OneVsRest, Binary Relevance, and Label Powerset techniques do not consider correlations between classes. The Classifier Chains technique will build a chain of binary classifiers to take into account any correlations between classes. The number of classifiers that are constructed equals the number of classes, i.e., if we have classes: comedy, drama, and romance, we will have three classifiers as well C1:C3.

We should mention Logistic Regression in this section because it is an important classification technique. You will learn more about it in the next module. Logistic regression uses a logistic function to model the probability of a class or event. Some questions you can answer with logistic regression are: Will you pass or fail a course, will you develop high blood pressure based on certain attributes, or will 18 to 35-year-old college-educated men from Pennsylvania vote for the Democratic or Republican presidential candidate in the 2024 presidential elections? The logistic regression model can have independent variables that are of diverse data types, but the response is categorical.



Tree-based methods are considered to be among the simpler methods for prediction and classification. Trees can be built using both numerical and categorical variables, and the tree method is rated highly as an interpretable method. Certain data science practitioners and thought leaders favor the simplicity of tree-based models because they can be seen to mirror an "If-Then" statement and are easily digestible to an individual with a growing statistics knowledge.

We will explore the different tree-based methods starting with one of the most popular methods: Decision Trees. Using a very simple example, let us build a decision tree: Decision Trees: scikit-learn .

A decision tree consists of a root node, leaf nodes, and branches. In decision sciences, it is an effective visualization that is easy to interpret, in data mining and machine learning, it is used to model predictions. The end goal of a decision tree method is to predict the value of a target variable based on several predictors. When you have a decision tree model with an outcome response containing a categorical value, you have a Classification Tree. When your outcome or target variable is a continuous value, you have a Regression Tree.

Additional Reading: Decision Trees for Decision Making

Building Classification Trees

Building a classification tree involves recursive partitioning and pruning. Both concepts are used to ensure the model has a low error rate and that overfitting is not an issue.

Recursive Partitioning creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset. C4.5 is one of the popular algorithms that employ recursive partitioning. It generates models that have more sensitivity and tend to be more accurate. Partitioning is done by repeatedly splitting and creating subsets until the tree is pure; that is, observations belong to a single class. Recursive partitioning splits each node on the decision tree to create decision rules that are easily interpretable, but overfitting can be an issue.

Another technique for building decision trees is the Chi-square automatic interaction detection (CHAID). This is used for both classification and prediction and can be used to capture the interaction between variables. It is most useful when you have a large dataset. Let us assume that you have received a credit card offer from Capital One as a preselected customer. CHAID can help Capital One's marketing firm to predict how your age, income, and credit score will affect your response to the interest rate offered.

Measures of Impurity. You can measure impurity using entropy and the Gini index. The Gini index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen. It varies from 0 to 1. 0 indicates that all elements are members of a class, while 1 denotes that elements are distributed (randomly) across various classes. It is best practice to select the feature with the lowest Gini index as the root node. Entropy is a measure of uncertainty within a model. Decision trees will always seek to minimize entropy.

Reading: Gini Index and Impurity Measures

Pruning. If you have dabbled in horticulture, you will be familiar with the term pruning. You prune a plant so that it grows without obstacles, but you can also prune a plant to redirect the growth and shape of the plant. You can think about pruning decision trees in a similar light. It is one of the solutions to avoid overfitting the training dataset. Once you have a large decision tree, you will prune the weakest branches to reduce the complexity of your model and improve accuracy. Pruning can be done using two techniques.

Cost complexity pruning will generate a series of trees. The tree is created by removing a subtree and replacing it with a leaf node with a value chosen as in the tree-building algorithm. The best tree is chosen by generalized accuracy, measured by a training set or cross-validation.

Reduced error pruning is done by replacing each node with the node's most popular class. However, that replacement is temporary unless it does not negatively affect the prediction accuracy. It is an efficient technique for pruning.

Application: Decision Trees and NLP: A Case Study in POS Tagging .

When a full tree is built, it will result in a fully grown decision tree that represents the maximum number of splits that the CART method will make to identify pure subsets. Full trees tend to overfit and do not do best at generalizing well to new cases. Solving this requires pruning the tree. The least complex tree with the smallest validation error is called a Minimum Error Tree. The least complex tree with a validation error that is within one standard error of the minimum error tree is called a Best Pruned Tree.

The validation dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree. After pruning, the tree will generalize new cases well. Misclassification rate is a performance measure for classification trees and is used to identify the tree that has the lowest error or the minimum error tree.

Building Regression Trees

We already indicated that decision trees are more explainable than linear regression models. A smaller tree can easily be interpreted by someone who is not in the field, and trees can use qualitative variables without the need to create dummy variables. The impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal nodes. The predictive accuracy of CART models is not as robust as other methods. Regression tree performance is evaluated using the root mean square error (RMSE).

Random Forests, Bagging, and Boosting can be used to improve this prediction accuracy and performance. We will learn about those next.

Ensemble Techniques
Bagging reduces variance in a decision tree method. This is achieved by averaging a set of observations and directly applied by producing multiple training data sets from the entire dataset, then using those training datasets to build a model for each set, and then averaging the results retrieved from each model. This is likely to produce a model with low variance. Bagging will reduce overfitting issues and works quite well with high-dimensionality data. Out-of-Bag Error Estimation measures the prediction error of models that use bagging. It is also used to validate models created using random forests. It is computed on data that was not used in the analysis of a model, unlike the validation metrics.

Additional Reading: History of Random Forest Algorithm

Random Forests. This is an extension of bagging and makes some changes to bagged trees. When there is overfitting with decision trees, random forests will remedy this issue. Similar to bagging, a random forest will perform well because it consists of a large number of decorrelated trees (the focus is on the low correlation between trees). A Random Forest will build several decision trees and then merge them for better accuracy and predictive value. It is used for classification and regression tasks and it searches for the best feature within a random subset of features in a dataset. The random forest method will also evaluate the importance of features and scale the results of this assessment to show the importance of features. This is useful for feature engineering as you can eliminate the features that do not contribute to your task without losing information. Random forests create random subsets of features and combine those subsets, which prevents overfitting. The number of features to be included can be derived by calculating the square root of the number of predictors. The downside to the random forest method is that it can be computationally slow in making predictions (but not slow to train).

Boosting. Similar to bagging, boosting can be used to improve the predictive accuracy of certain methods including decision trees. It differs somewhat from bagging as the trees built with this model are dependent on a prior tree (each tree depends on or fits the residual of the trees that preceded it). Each tree is created iteratively and the output of each tree is assigned a weight that is relative to its accuracy. This ensures that the overall predictive accuracy estimate of that method is improved.

Overfitting can occur in boosting if the number of trees becomes too large. When you take your machine learning class, you will learn more about the techniques that are used in Boosting, including one of the most popular: Adaptive Boosting (AdaBoost). AdaBoost is used to improve the performance of models. It is sensitive to outlier data, but on the upside, it is considered the best out-of-the-box classifier when used with decision trees. This is because the information that is collected by the AdaBoost algorithm about the training data is then fed into the tree algorithm so that the model can accurately classify observations that would have otherwise been difficult to classify. AdaBoost will select features in the dataset that will improve the model's predictive power, which is helpful for reducing dimensionality and improving computation time.

Ensemble methods were represented as an extension of the tree method; take note that they are also used for other methods.



A data science pattern that can be used to solve different data science tasks, from machine translation to information retrieval, is Ranking. Learning-to-rank is a technique used to train a model for ranking tasks. We do not always want to predict the probability of scenarios. We just might want to rank things. Ranking is used to solve information retrieval problems, including collaborative filtering, sentiment analysis, and document retrieval. The learning-to-rank technique is applied in supervised learning to rank results according to relevancy. When you are building a model using this approach, you must decide on the features used but also on the adequate relevance criteria.

Assume that you have a set of documents and that users pose queries to retrieve documents matching a query ranked based on a measure of relevance to a query. We can use one of the following approaches:

Pointwise Approach is used under the assumption that each we can compute a numerical score that captures how much a document is relevant to a query. Once we know these scores, we can rank the documents. Thus the learning-to-rank problem can be cast as a regression problem — given a (training set of ) query-document pair, learn to predict a relevance score. Ordinal regression and classification algorithms can also be used in a pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values.

Pairwise Approach seeks to reduce the average number of wrongly ordered rankings in comparison to the ideal expected result (also known as the ground truth). Ranking using the pairwise approach becomes a classification or regression task. Every pair of documents is classified by a binary classifier which determines which one of the pairs is more relevant to the query. Then, based on these pairwise rankings, a global ranking is produced, minimizing the number of out-of-order pairs in the final list.

Listwise Approach reviews the list of documents and produces an optimal ordering. It tries to directly optimize the value of one of the above evaluation measures, averaging over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to the ranking model's parameters.

Microsoft Research has developed three algorithms that all use pairwise ranking:

RankNet uses gradient descent to update the weights or model parameters for a learning-to-rank task. This algorithm seeks to minimize the number of wrong orderings among a pair of results of a ranked list.

LambdaRank uses a cost function to train a RankNet which results in speed and accuracy improvements.

LambdaMART uses Multiple Additive Regression Trees (MART is an implementation of the gradient tree boosting methods for regression and classification) and LambdaRank to solve a ranking task.



This data science pattern is different from what you have studied in this course as it makes assumptions about an algorithm and the data that is used to construct it. Active Learning pattern posits that if an algorithm or learner can choose the data, it will learn from, it will perform better than an algorithm that does not choose its own data, and it will perform better with less training. Active learning is sometimes referred to as query learning. The learning methods you have used so far when you sample and gather data and transform it to train a model are considered the traditional methods. When you have a large data set that is unlabeled (as is typical), active learning can be a useful technique for labeling.

Active learning presents Scenarios that allow a learner to query the labels of observations in a dataset.

Membership Query Synthesis is a scenario that enables a learner will generate an observation that is similar to one or more in the dataset. Once it is created, the new observation can then be labeled by the oracle (an information source or teacher).

Stream-based Selective Sampling scenario involves unlabeled data points or observations that are evaluated by the algorithm as to whether these points should be labeled by for training or discarded. 

Pool Based Sampling, as shown in the figure below, assumes that you have a pool of unlabeled data, and observations are collected from the pool according to an informativeness measure (certainty that a classifier has when classifying data points). The informativeness measure is applied to all observations in your dataset, and then the observations that have the most important measures are selected. The selected observations are then labeled.

Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).


Pool Based Active Learning Cycle-Source: Settles Active Learning Survey1

Query Strategies

How does the algorithm decide on the most informative measures? Let's highlight some of the strategies used to evaluate the informativeness of unlabeled data.

Uncertainty Sampling is an approach that allows the active learner to query the observations about which it is not able to label.

Query-by-committee involves using a group or committee of models that have been trained on a labeled dataset, but the catch is that these models have competing hypotheses. Each model in the committee will vote on the labels. Identify the query that all voting models disagree on that becomes the most informative query.

Expected Model Change would use an approach that selects the observation that would introduce the most change to a current model if its label was known.

Expected Error Change involves labeling the data points that would reduce the model's out-of-sample error (a measure of how accurately your learner can make predictions on new data).



Overview: Errors at All Phases
A quick survey of both scholarly and practitioner literature shows that errors can make or break the analytic development process and render your analytic solution of little use to your client. Errors can start from the business understanding phase when a data science team does not set the appropriate analytic objectives due to misunderstanding the business context and needs. This misunderstanding to bloated costs and scope creep (changes, continuous or uncontrolled growth in a project’s scope, at any point after the project begins). A data science team might also encounter errors during the data understanding phase, such as issues with data that is not prepared adequately or, even worse, collecting data that is not relevant to the analytic solution. Errors in the data understanding phase can occur due to inexperience within the data science team and an attempt to deliver a solution prematurely.

Errors in Model Understanding
The model understanding phase also presents errors that we should explore as we are learning about the different data science patterns and techniques that can be used to solve data-related problems. Errors in this phase may show up when training and validating models. Errors will reveal if the expected performance of a model will be sufficient for deploying to production. The models below are errors that you will encounter throughout your career as a data scientist. As we learn about different techniques, we will further explore how to assess models based on certain error estimates.

Types of Model Errors
Training Error is derived by computing the classification error of a model on the exact data that was used to train the model. The training error is defined as the average loss that occurs at the end of the training process. It should be noted that the training error will usually be lower than the test error.

Test Error is derived by computing the classification error (the loss) of a model on the test set. It is important as it gives insight into the number of errors to expect when making future predictions, and it is used for model selection. No part of the training data set should be part of the test data, as this can affect the accuracy of the test error. As seen below, the test error may decrease as the model complexity increases up to a certain point and then may start increasing.


Reducible Error is the error resulting from a mismatch between the ground truth and the model estimation, or the estimate of the true relationship between x and y. The reducible error is the element that we can improve. It is the quantity that we reduce when the model is learning on a training dataset, and we try to get this number as close to zero as possible.

Irreducible Error is the noise term in the true relationship that cannot fundamentally be reduced by any model. When x can not determine y because there are other predictors that might improve the prediction error, you can incorporate those variables. The irreducible error is the error that we can not remove with our model or with any model. The error is caused by elements outside our control, such as statistical noise in the observations.

As we explore the different methods, we will dive deeper into calculating the error rate for the models trained using those methods.

Model Bias
The bias is a measure of how closely the model can capture the mapping function between inputs and outputs and measures the average accuracy of the model arising from erroneous assumptions in the learning algorithm. A high bias can cause an algorithm to miss the relevant relationships between features and target outputs (underfitting). The bias is always positive.

Low Bias indicates that assumptions regarding the functional form of the mapping of inputs to outputs are weak.

High Bias indicates that assumptions regarding the functional form of the mapping of inputs to outputs are strong.

Model Variance
The variance of the model is the amount the performance of the model changes when it is trained on different training data. It is an error from sensitivity to small fluctuations in the training set and measures the average consistency of the model. High variance can cause an algorithm to model the random noise in the training data rather than the intended outputs (overfitting). The variance is always positive.

Low Variance indicates that changes to the training dataset cause small changes to the model.

High Variance indicates that changes to the training dataset cause large changes to the model.

Model Noise
How big is the data-intrinsic noise? This error measures ambiguity due to your data distribution and feature representation. You can never beat this. It is a property of the data.

Variance refers to the amount by which 
 would change if you estimated it using a different training data set. Bias refers to the error that is introduced by approximating a real-life problem, which may be complicated by a simpler model. For example, real life does not present scenarios that have a simple linear relationship. This means linear regression will present some bias in the estimate of f.

When you decompose bias-variance, you will analyze an algorithm's ability to predict outcomes for data that your model has not seen. The bias-variance tradeoff is encountered while working with some supervised learning techniques. The premise is that your model will adequately learn the training data, and it should properly generalize well to new data.

As seen in the figure below, a supervised learning method that can represent training data well but experiences overfitting is considered a high variance method. A method with high bias will not adequately learn the training data, and this leads to underfitting. High variance models are typically more complex, and those with high bias tend to be simpler.


Bias-Variance Diagram.

The Bias-Variance Decomposition
Let us look at a very well deconstructed mathematical representation of Bias-Variance by IBM's Aditya Prasad :

Note that the bias and variance of an estimator are mathematically related to each other and also to the performance of the estimator. Let us define an estimator’s error at a test point as the “expected” squared difference between the true value and the estimator’s estimate.

Whenever an expected value is referenced, this means the expectation over all the possible models, trained individually over all the possible data samples. For any unseen test point 
 , you will have:


Referring to 
 and 
 as f and g, respectively and skipping the conditional on X:

 
 
 


So, the error of the estimator at an unseen data sample 
 can be decomposed into the variance of the noise in the data, bias, and the variance of the estimator. This implies that both bias and variance are the sources of error in an estimator.

Reading: Bias-Variance Tradeoff .

The Trade-off!
​​The bias and the variance of a model’s performance are connected.

Ideally, we would prefer a model with low bias and low variance, although in practice, this is very challenging. In fact, this could be described as the goal of applied machine learning for a given predictive modeling problem. Reducing the bias can easily be achieved by increasing the variance. Conversely, reducing the variance can easily be achieved by increasing the bias. This relationship is generally referred to as the bias-variance trade-off. A model will present a high error when there is high bias and also when there is overfitting, or there is high variance and low bias. The model can not generalize to new or unseen data. We want a model that is balanced between bias and variance to ensure the error is minimized.


The variation of Bias and Variance with the model complexity.

This is similar to the concept of overfitting and underfitting. More complex models overfit while the simplest models underfit.


As shown in the figure above, an ideal and balanced model is one that has a low bias and low variance. You can work against overfitting (high variance) with dimensionality reduction techniques. This way, the model is simplified. Trade-offs can be optimized using a technique that will be discussed on the next page, Cross-Validation. The figure below gives you a visual representation of bias-variance with the training dataset.


Bias-Variance and Training-Test Data.
When evaluating the performance of a model, there are methods that allow for your model to be fit multiple times with different subsets of a dataset. Model Assessment and Model Selection are key concepts of importance to every data scientist. You will assess your model to see its performance and select the most fitting model. How then can you test and validate your model to ensure that real-world data can be introduced to it?

There are multiple scenarios where you will not have access to a large enough dataset to estimate the test error rate of a model. However, you can not use this as an excuse not to test and validate your model. You can employ a method called holding out. With holding out, you are using a subset of the observations in your training dataset to be used to validate your model. This process will allow you to predict the responses to the observations used to validate the model. This approach is called the Validation Set Approach, and the data that was used during holding out is called the Validation Dataset. Similar to the results from fitting the model with the training data set, you will assess the error rate of the validation set approach using the mean squared error (MSE), which will provide an estimate of the test error rate for quantitative outputs.

Consider that the test error rate for the validation data set will depend on the observations included in the validation data set and not on the training data set. The validation data set test error rate might be overestimated when this approach is applied to statistical methods that require a large number of observations.

k-Fold Cross-Validation

k-Fold Cross Validation

You should think about k as the number of groups that are formed as a result of splitting your dataset. Implementing k-fold cross-validation is straightforward. The dataset should be shuffled randomly and split into groups according to the chosen value of k. Each group will be used as the held-out validation dataset, while the others will be used as a training dataset. Your model will be trained with the training dataset and then evaluated with the held-out dataset. k-fold cross-validation is not costly to implement as other cross-validation techniques. It can be applied to most learning methods. You should assess your model's bias by calculating the mean of all error estimates. The model's variance is assessed by computing the standard deviation of all the error estimates. The lower the value for the bias and variance, the better, and this means your model is balanced.

Selecting k is not a random process, an inappropriate k will lead to a model that has a high bias or high variance. Remember, you want a balanced model with low bias and low variance. Using a fixed value of k=10 has been empirically tested to show that the resulting model will be a balanced model (low bias-low variance). k=10 and even k=5 yield test error rates that do not suffer from bias-variance issues.

Leave One Out Cross Validation (LOOCV)
This technique involves splitting the dataset to use one observation for validation and the rest of the dataset for training. The LOOCV technique presents less bias as it does not overestimate the test error rate as the technique continues to fit the model with as many observations as are in the dataset. There is no randomness in the dataset split. It is costly to implement (think about applying this technique to a large dataset), although it usually provides a reliable and unbiased estimate of model performance. A viable solution involves using polynomial regression to make the cost of this technique similar to that of fitting a single model, which, dues to mathematical convenience, can implement LOOCV with a single training session on all of the data.

LOOCV can be used with any kind of predictive modeling.


Leave One Out Cross Validation (LOOCV)

LOOCV will have a higher variance than the k-fold CV because, with LOOCV, models are trained on almost identical sets of observations, and this means that the outputs will be positively correlated with each other. With k-fold CV, when k is less than n, the output of your models is not as correlated as is the case with the LOOCV models.

Using Cross Validation with Regression and Classification Problems
Classification Problems: When Y is qualitative, we use the number of misclassified observations as a measure of the model's test error.

Regression Problems: When Y is quantitative, the MSE is used to measure the test error.




The ultimate goal of any supervised machine learning problem is to find a model or function that predicts a target or label and minimizes the expected error over all possible inputs and labels. Minimizing error over all possible inputs means the function must be able to generalize and make accurate predictions on unseen inputs. In other words, the fundamental goal of machine learning is for the algorithm to generalize beyond the training sets.

Regularization is a general approach to help select a balanced model to trade off between a high bias and a high variance. This ideal goal of generalization in terms of bias and variance is a low bias and a low variance which is near impossible or difficult to achieve, hence, the need for the trade-off to minimize the model's total error.

There are three popular regularization techniques, each of them aiming at decreasing the total size of the parameters of the model (e.g., coefficients in a regression):

Ridge Regression, which penalizes the sum of squares of the coefficients (L2 penalty).

Lasso Regression, which penalizes the sum of absolute values of the coefficients (L1 penalty).

Elastic Net, a convex combination of Ridge and Lasso.



For selected units in this course, we will have paper reading modules that provide exposure to foundational research papers. The goal of these modules is to familiarize you with the styles of data science literature and the contexts in which advances in data science are introduced. We understand that reading technical papers can be challenging and time-consuming if you don’t have prior experience. To facilitate your learning, we have included both the original paper and our synthesis of the paper’s key points below. Our expectation is that you can acquire a good understanding of the paper’s message by skimming through the original article and then reading our synthesis.

Beyond the specific content of each paper, we also encourage you to pay attention to the synthesis structure introduced below, which contains key questions that one should ask while reading through a scientific publication. You may find this outline useful in a future seminar course or in your own research projects.

[Required Reading] Paper: Briscoe, E., & Feldman, J. (2011). Conceptual complexity and the bias/variance tradeoff.Cognition,118(1), 2-16. (Requires CMU credentials to access)

Who are the paper's authors? Why are they qualified to write this paper?

The first author is currently a Senior Research Scientist and the Chief Scientist of the Aerospace, Transportation, and Advanced Systems Laboratory with the Georgia Tech Research Institute. She conducts research and development projects that focus on behavioral and data science/analytics applications in various problem spaces, including computational social science, technology emergence and prediction, social network analysis, insider threat detection, terrorism and radicalization, business intelligence, and psychological profiling. She received a Ph.D. in cognitive psychology from Rutgers University in 2008.

The second author received his Ph.D. in 1992 from the M.I.T. Dept. of Brain and Cognitive Sciences and has been at Rutgers ever since. His main research interests are in visual perception, especially perceptual organization and shape, and in categorization and concept learning. In both these general areas, his focus is on mathematical and computational models of human mental function. In categorization and concept learning, he is similarly interested in how the mind organizes groups of objects into coherent collections and hierarchies. In experimental work, he has found that human learners, given a set of objects to be learned, tend to form categories that are as simple as possible. This idea opens up an enormous set of research questions about what perceptual features form the basis for categorization, how these features are selected in order to reduce representational complexity, and how these goals relate to the structure of the natural world.

Who is the audience of the paper?

The paper is targeting machine learning researchers and practitioners who build machine learning systems that interact with the end-user.

Why is the paper’s topic relevant at the time of its writing?

There are two popular psychological theories on how humans perform categorization. Exemplar theory states that people store the attributes of observed examples, called exemplars, along with their category labels in memory, and categorize a new object with the label of the most similar exemplar. For example, people would categorize an object as a bird if it’s similar to any type of bird that they have come across, e.g., parrot, sparrow, penguin. In contrast, prototype theory states that there is a central representation of each category, and people compare a new object against these central tendencies to determine its category. With the same bird classification task above, based on the prototype theory, one would label an object as a bird if it possesses the common, “average” features of a bird, e.g., two legs, two wings, and lay eggs. Overall, the key difference between the two theories is whether a new object is compared to real instances (exemplars) or an abstract central representation (prototype) of a category.

While prior researchers have often regarded these theories as fundamentally disparate, the authors instead suggest that they can be viewed as two extremes on the same continuum of bias-variance and that the way humans actually perform categorization lies somewhere in the middle of this continuum. Their attempt to connect psychological theories of human cognition to statistical machine learning concepts of bias and variance presented a novel perspective at the time of the paper’s writing (keep in mind that back in 2010, statistical ML was not as popular as it is nowadays, especially to those in non-technical areas such as psychologists).

What is the paper’s contribution? Which research gap is it trying to address?

The paper presents a number of conceptual and empirical contributions:

The characterization of exemplar theory as the low bias, high variance extreme, and prototype theory as the high bias, low variance extreme on the bias-variance continuum.

A class of experiments to evaluate human learners’ position on this continuum when the complexity of the training data varies has not been systematically attempted before.

The proposal of a locally regularized model that had the best fit for human performance and therefore constitutes a reasonable explanation for how humans perform categorization.

Summary of the paper’s experiments and findings.

The experiment reported in the paper has the following phases:

Generate data at different levels of complexity. The authors first constructed five bivariate Gaussian mixtures, p1(x, y), p2(x, y), …, p5(x, y), where pK(x, y) consists of K components (Equation 1). In this way, K ranges from 1 to 5 and denotes the complexity of the underlying distribution. For each K, the authors then generated ship flag images, each with a pre-defined label – either belonging to a pirate ship (positive) or a friendly ship (negative) – and having two quasi-continuous features, the width of the inner black rectangle and the orientation of the sword (Figure 4). These two features were generated from either the distribution with pdf pK(x, y) for positive images or 1 - pK(x, y) for negative images.

Obtain human categorization of the generated data. 13 undergraduate students were recruited for the study. Subjects were shown a sequence of flags that may belong to either a pirate ship or a friendly ship, and the flag features were sampled from one of the five Gaussian distributions in step 1. They were then asked to learn the categorization by first attempting to classify each flag on their own, then seeing feedback on the correctness of their answer, and then repeating these steps with the next flag.

Build models that represent the exemplar approach, prototype approach, and locally regularized context approach. The exemplar model is called GCM and is denoted in Equation 5. The prototype model is denoted in equations 6 and 7. The locally regularized model assumes the same functional form as the exemplar model, but the sensitivity parameter c (whose high value corresponds to more “exemplar-like” and the low value corresponds to more “prototype-like”) is modulated locally, i.e., its value is set independently at each partition of the feature space.

Fit the models to subject data. The fitted parameters are optimized to fit the ensemble of each subject’s responses. This helps answer the question: as the complexity K varies, which models best reflect the human performance in this flag classification task?

Fit the models to concept data. The fitted parameters are optimized to maximize the likelihood of the training examples observed so far at each point in the experiment. In other words, the ground truth labels of the flags are used in this process instead of the human classifications like in the previous step. This helps answer the question: as the complexity K varies, which model’s performance is more closely correlated to human performance?

The important findings from the experiment are as follows:

Human subjects are proficient at categorizing simple concepts (K = 1), but their performance declines as the complexity of K increases, approaching random guessing at K = 4 or K = 5.

When fitting models to subject data, the exemplar model has a better fit than the prototype model across all complexity levels. At larger complexity levels (K = 4 or K = 5), the two models converge in performance, largely because they were fitted on human categorizations that were just random guesses.

When fitting models to concept data, the prototype model’s performance decreases much faster than human performance, whereas the exemplar model’s performance does not decrease fast enough to match human performance at higher complexity levels.

The locally regularized model, which represents a middle point in the bias-variance continuum, consistently fitted subject data better than the exemplar (low bias, high variance) and the prototype (high bias, low variance) model.

Circling back to the question of whether humans perform categorization by the prototype approach (compare a new object to an abstract prototype of each candidate category) or the exemplar approach (compare a new object to existing instances of each candidate category stored in memory), this paper’s finding suggests that humans adopt a middle ground. Humans don’t assume there is only a single prototype for each concept but do not keep in memory a large number of exemplars for each candidate prototype either. Instead, they treat concepts as mixtures of several sub-concepts, each represented by a partition of the feature space with its own localized sensitivity parameter c.

What are the implications of the paper’s findings?

From a cognitive standpoint, the paper shows that evaluation of human learning should be conducted at different levels of conceptual complexity. While theoretically disparate models, such as the exemplar model and prototype model, may have a similar fit with human learning on simple data, they quickly diverge at higher levels of complexity. Complexity should be systematically varied over a range of levels to reflect a comprehensive picture of general human learning.

From a machine learning standpoint, there remains the open question of whether machine learning should follow the process of human learning. While there have been attempts to connect the two, for example, with neural networks that replicate the neural structure of the brain, the similarities are shallow at best. Deep neural networks typically require a very large amount of training data, which is very different from how humans learn. In recent years, however, more attention has been paid to making machine learning more human-like, for example, by learning from a limited number of samples (few-shot learning and no-shot learning) or by increasing robustness to adversarial attacks. This paper shows yet another way that human learning can be connected to machine learning – while the bias/variance trade-off originates from statistical learning, it can also be used to explain the way humans perform categorization by balancing the performance accuracy and the number of sub-concepts that they can reasonably hold in memory.



Deep learning applications have been widespread in recent years with the increasing availability of data and compute resources. Deep learning is an area of machine learning that draws inspiration from how the human brain functions as a model of computation. Like most machine learning algorithms, deep learning also involves a transformation of data from an input to an output: for example, using speech samples as inputs to predict the speaker or taking some text in one language and translating the text into another language. As the problem becomes more and more complex, classical machine learning approaches fail to adequately learn such transformation or mapping of input to output from the data. Deep learning algorithms overcome this using many successive transformations of input data, thus the deep in deep learning.

At a very high-level abstraction, we can view neurons as aggregating signals from their inputs and sending processed signals to their outputs. Such input signals can be visual signals detected by the retina or acoustic signals detected by the ear. Again at an abstract level, the neuron can be considered computing a weighted summation of the inputs weighing each input by a synaptic weight and doing a final non-linear transformation on the sum.

Inspired by this, neural networks consist of neurons that are connected to each other via weights. Each neuron receives weighted inputs from neurons in the previous layer, these are summed and passed through a nonlinearity function, commonly called the activation function, and the resulting output is passed on to neurons in the next layer, again, connected with some weight. The deep nature of deep neural networks refers to having a very large number of layers of such connected neurons.

In this module, we discuss some of the key concepts in deep learning.

The nonlinearity introduced in neural networks is key to their learning capabilities and what allows these models to approximate more complex functions. Activation functions are responsible for performing the nonlinear transformation on the weighted sum of inputs received from the neurons in the previous layers. Depending on the magnitude of the continuous value generated by an activation function, the neuron can be considered as “activated” or “inhibited,” thus affecting the transformations in the subsequent layers.

Following are some common activation functions:

Sigmoid
The sigmoid function transforms an input to a value between 0 and 1. Assuming the 
 represents the weighted sum of the inputs, the logistic function, which is an example of a sigmoid function, computes

 

It is especially useful when we want to think of the output in terms of probability. The sigmoid function has some disadvantages in being used in intermediate layers of a deep neural network and is hence mostly used in the output or the final layer. One of the disadvantages of the sigmoid function is that for very small or very large input values, the gradient approaches zero, which slows down the learning process.

ReLU
ReLU stands for Rectified Linear Unit. It is a piecewise linear function that will output the input directly if it is positive. Otherwise, it will output zero. It has the advantage of being faster to compute than some of the other activation functions and does not get saturated at high input values as opposed to the logistic function, which saturates at 1. ReLU is a common default choice for activation functions. However, the ReLU activation function does have the disadvantage of dying out for negative values, as the ReLU function is 0 when the weighted sum of inputs is negative. This results in stalling of the training when the weights in the network always lead to a negative output.

LeakyReLU
Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training and hence is considered a hyperparameter. The small slope for negative values prevents the stalling problem encountered in the ReLU activation.

Tanh
Tanh, the hyperbolic tangent function is a shifted version of the sigmoid function where its range is between -1 and 1. The mean of the activations that come out of the function is closer to having a zero mean therefore, data is more centered, which makes the learning for the next layer easier and faster. The tanh function shares the same disadvantage as that of the sigmoid, where the gradient becomes close to zero for very large and very small values, thus slowing down gradient descent.

Softmax
The softmax activation function is used in neural networks when we want to interpret the outputs of a multi-class classifier as a probability distribution. This makes it easier to assign an input to the one class with the highest probability when the number of possible classes is larger than two. The softmax ensures that the sum of outputs for each class is equal to 1 for a given input.


Objective / Loss Functions
An objective function quantifies how well or badly a model is performing. Typically, objective functions in deep learning are defined such that a lower value is better. Because of this nature, they are often called loss functions. There are many functions that could be used to estimate the error of a set of weights in a neural network, and they often depend on the choice of the activation function in the final layer of the model. A function with a smooth, differentiable, and high-dimensional curve that the optimization algorithm can reasonably navigate to perform iterative updates to network weights is a desirable choice.

Following are some of the commonly used loss functions based on the problem at hand:

Regression Problem: A problem where the model predicts a real value, the last layer in the model consists of a single node with a linear activation function, and the Mean Squared Error (MSE) can be used as a loss function.

Binary Classification Problem: A problem where an example has to be classified into one of two possible classes, the final layer in the model consists of a single neuron with a sigmoid activation, and a Binary Cross Entropy function can be used as a loss function.

Multi-Class Classification Problem: A problem where the input has to be classified into one of more than two possible classes, the final layer in the model consists of the same number of neurons as the output classes and a softmax activation. The Cross-Entropy function can be used as a loss function in this case.

For a deep learning problem, once a loss function has been defined, an optimization algorithm is used to update the network parameters (weights and biases) based on the obtained loss value. The loss is usually the sum of the loss values obtained for each example in the training dataset and is minimized by an optimization algorithm. An optimization algorithm iteratively calculates the next point using the gradient at the current position, then scales it by a learning rate and subtracts the obtained value from the current position. This is known as “making a step” and refers to the update in network parameters mentioned above. The value is subtracted in the case of a minimization objective, which is the most common case in deep learning and can be added for a maximization objective.

Following are some common optimization algorithms along with their advantages and disadvantages:

Gradient Descent
Gradient Descent is the most basic but also one of the most used optimization algorithms. It is used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm. Gradient descent is a first-order optimization algorithm that is dependent on the first-order derivative of a loss function. It calculates which way the weights should be altered so that the function can reach a minimum. Through backpropagation, the loss is transferred (propagated!) from one layer to another, and the model’s parameters, also known as weights, are modified depending on the losses so that the loss can be minimized.


Advantages

Easy to compute

Easy to implement

Disadvantages

Susceptible to getting stuck in a local minima

Convergence is slow as updates are calculated after calculating the gradient for the entire dataset

Computation for entire dataset requires a large memory

Stochastic Gradient Descent (SGD)
Stochastic Gradient Descent (SGD) is a variant of Gradient Descent where model parameters are updated more frequently as opposed to one single update. Model parameters are updated after the computation of loss on each training example chosen in a random order, hence the title stochastic.


Advantages

Converges in lesser time because of frequent updates

Lesser memory requirements for calculating updates

Less likely than Gradient Descent to get stuck in a local minima

Disadvantages

High variance in parameter updates due to high frequency in updates

Learning rate needs to be correctly chosen and adjusted for effective training

Mini-Batch Gradient Descent
To overcome the issues in Gradient Descent and Stochastic Gradient Descent, Mini-Batch Gradient Descent performs loss calculation and parameter updates for a given batch. A batch is a fixed-sized subset randomly sampled from the training dataset. Thus, the dataset is divided into multiple batches, and parameter updates are calculated after processing each batch.

Advantages

Frequent updates and lesser variance as compared to SGD

Moderate amount of memory requirements

Disadvantages

Learning rate needs to be correctly chosen and adjusted for effective training

Learning rate is constant for all parameters which might not be desirable

Susceptible to getting trapped in a local minima

SGD with Momentum
The addition of momentum to SGD addresses the problem of high variance in parameter updates due to frequent updating. Historical parameter updates are multiplied by a momentum term and added to the current calculated update. SGD oscillates between either direction of the gradient and updates the weights accordingly. However, adding a fraction of the previous update to the current update will make the process a bit faster and smoother.

Advantages

Reduces the high variance in model updates by SGD

Faster convergence than Gradient Descent

Disadvantages

The momentum hyperparameter needs to be additionally tuned

Learning rate needs to be correctly chosen and adjusted for effective training

AdaGrad (Adaptive Gradient Descent)
The adaptive gradient descent algorithm uses different learning rates for each iteration. The change in learning rate depends upon the difference in the parameters during training. The more the parameters change, the more minor the learning rate changes. This modification is highly beneficial because real-world datasets contain sparse as well as dense features. So it is unfair to have the same value of learning rate for all the features.

 

 

Advantages

Reduces the need to manually modify learning rate

Tends to have faster convergence than Gradient Descent and SGD

Disadvantages

The learning rate might be decreased aggressively and monotonically, resulting in a very small learning rate

RMS (Root Mean Square) Prop
RMSProp addresses the issue of varying gradient values. Some gradients might be quite large, and some might be quite small. In this case, the monotonically decreasing learning rate, as in the case of AdaGrad, might not be ideal. The algorithm focuses on accelerating the optimization process by decreasing the number of function evaluations to reach the local minima. The algorithm keeps the moving average of squared gradients for every weight and divides the gradient by the square root of the mean square. As a result, if there exists a parameter due to which the loss function oscillates a lot, the update of this parameter is penalized.

Advantages

Requires lesser tuning than other optimization algorithms

Faster convergence

Disadvantages

The initial learning rate needs to be set manually and needs to be carefully chose as the suggested value does not work for all tasks

AdaDelta
AdaDelta is an extension of AdaGrad, which tends to remove the decaying learning rate problem. Instead of accumulating all previously squared gradients, AdaDelta limits the window of accumulated past gradients to some fixed size w. An exponentially moving average is used rather than the sum of all the gradients in this case. AdaDelta uses two state variables to store the leaky average of the second moment gradient and a leaky average of the second moment of change of parameters in the model.



 


Advantages

Elevates the learning rate decay problem in AdaGrad

Disadvantages

Computationally expensive

Adam (Adaptive Moment Estimation)
Adam works with momentums of first and second order to update the learning rate, but unlike RMSProp, which only uses the momentum of the first order. Also, instead of maintaining a single learning rate through training as in SGD, Adam optimizer updates the learning rate for each network weight individually. The Adam optimizer is known to combine the benefits of RMSProp and AdaGrad.

 
 

Advantages

Rapid convergence

Rectifies vanishing learning rate and high variance

Disadvantages

Computationally expensive

Equations Reference: Link

Convergence Behavior of Various Optimization Algorithms

(https://awesomeopensource.com/project/Jaewan-Yun/optimizer-visualization)


(https://awesomeopensource.com/project/Jaewan-Yun/optimizer-visualization)




Following are some of the key deep learning architecture/algorithms:

Artificial Neural Networks (ANN):
Artificial neural networks (ANNs) are composed of node layers containing an input layer, one or more hidden layers, and an output layer. An input node or a node in a hidden layer is connected to nodes in a subsequent hidden layer or an output layer with a weight. Each node in a hidden layer or an output layer typically computes the weighted sum of its inputs and passes the result through an activation function. The general name for such an ANN architecture is a multi-layer perceptron.


Figure 1. Deep Neutral Networks - Multiple Hidden Layers. Source: https://www.ibm.com/cloud/learn/neural-networks

Convolutional Neural Networks (CNN):
A CNN is a multilayer neural network that was biologically inspired by the animal visual cortex. The architecture is particularly useful in image-processing applications. The first CNN was created by Yann LeCun, and his architecture focused on handwritten character recognition, such as postal code interpretation [1] . As a deep network, early layers in a CNN recognize features (such as edges), and later layers recombine these features into higher-level attributes of the input. The LeNet CNN architecture is made up of several layers that implement feature extraction and then classification (see the following image). The image is divided into receptive fields that feed into a convolutional layer, which then extracts features from the input image. The next step is pooling, which reduces the dimensionality of the extracted features (through down-sampling) while retaining the most important information (typically, through max pooling). Another convolution and pooling step is then performed that feeds into a fully connected multilayer perceptron. The final output layer of this network is a set of nodes that identify features of the image (in this case, a node per identified number). You can train the network by using back-propagation.


Figure 2. LeNet CNN. Source: https://developer.ibm.com/articles/cc-machine-learning-deep-learning-architectures

The use of deep layers of processing, convolutions, pooling, and a fully connected classification layer opened the door to various new applications of deep learning neural networks. In addition to image processing, CNN has been successfully applied to video recognition and various tasks within natural language processing.

Recurrent Neural Network (RNN):
The RNN is one of the foundational network architectures from which other deep learning architectures are built. The primary difference between a typical multi-layer network and a recurrent network is that, rather than completely feed-forward connections, a recurrent network might have connections that feed back into prior layers (or into the same layer). This feedback allows RNNs to maintain memory of past inputs and model relationships in time. The key differentiator is feedback within the network, which could manifest itself from a hidden layer, the output layer, or some combination thereof. RNNs can be unfolded in time and trained with standard back-propagation or by using a variant of back-propagation that is called back-propagation in time (BPTT).

RNN architectures suffer from vanishing and exploding gradient problems. To overcome these issues, LSTM and GRU architectures have been developed and are described below:

Long Short-Term Memory (LSTM) Networks: The LSTM was created in 1997 by Hochreiter and Schmidhuber [2] , but it has grown in popularity in recent years as an RNN architecture for various applications. The LSTM departed from typical neuron-based neural network architectures and instead introduced the concept of a memory cell. The memory cell can retain its value for a short or long time as a function of its inputs, which allows the cell to remember what's important and not just its last computed value.

The LSTM memory cell contains three gates that control how information flows into or out of the cell. The input gate controls when new information can flow into the memory. The forget gate controls when an existing piece of information is forgotten, allowing the cell to remember new data. Finally, the output gate controls when the information that is contained in the cell is used in the output from the cell. The cell also contains weights, which control each gate. The training algorithm, commonly BPTT, optimizes these weights based on the resulting network output error.

Gated Recurrent Unit (GRU) Networks: GRUs were proposed in 2014 as a simplification of the LSTM. This model has two gates, getting rid of the output gate present in the LSTM model. These gates are an update gate and a reset gate. The update gate indicates how much of the previous cell contents to maintain. The reset gate defines how to incorporate the new input with the previous cell contents. A GRU can model a standard RNN simply by setting the reset gate to 1 and the update gate to 0.

The GRU is simpler than the LSTM, can be trained more quickly, and can be more efficient in its execution. However, the LSTM can be more expressive and, with more data, can lead to better results.


Figure 3. Recurrent Neural Networks (RNN). Source: https://clay-atlas.com/blog/2020/06/02/machine-learning-cn-gru-note/

 Reset Answers
References
1
Y. LeCun. (n.d.). Backpropagation Applied to Handwritten Zip Code Recognition. AT&T Bell Laboratories.
2
S Hochreiter, J. S. (1997). Long short-term memory. Neural Computation, 9 (8), 1735–1780.


Introduction to Computer Vision
Computer vision is the study of how to equip computers with (super) human-level perception, or more specifically, how to analyze or manipulate pixel values in a meaningful way. While computer vision models take input feature matrices and output scalars or vectors, much like the standard machine learning paradigm, the fact that their inputs are images (2D or 3D matrices) presents several interesting challenges.

Through the course of this module, we will be building upon the Deep Neural Network concepts introduced in the previous module. The power of neural networks, particularly multilayer perceptrons, lies in the ability to automatically extract a hierarchy of features at different levels of abstraction for classification tasks. As a result, neural networks preclude the need for feature engineering as standard machine learning methods require. However, one of its downsides is the presence of too many parameters and the inability to incorporate particular input structures required to model data from specific domains.


Figure 1. Rivian Pickup Truck.

Let us consider the image analysis task, where we would like to figure out what is happening in a given image or a sequence of images. In this image of a pickup truck (Figure 1), we get important signals regarding the objects within the truck, such as wheels, headlights, doors, and so on. The spatial proximity of the key features helps us understand what is happening in this image. This is the task of image understanding in computer vision. The input to all the computer vision models is the raw pixels in the images, which are just matrices of numbers representing the intensity levels of various spatial locations. From the computer's view, an image (Figure 2) is just a big matrix with a number (or tuple of numbers) at every pixel (Figure 3).


Figure 2. What a person sees.


Figure 3. What a computer sees.

A straightforward approach would be to flatten the image's pixel values and feed them as inputs to a multilayer perceptron. However, by doing so, we lose the valuable signal captured by the spatial structure of the image. Therefore, while learning useful visual features in computer vision, the key idea is to preserve and use the spatial structure of the image. One way is to use some spatial filters to extract a spatially adjacent set of pixels in the image and then feed those image patches to a multilayer perceptron. This idea sparked the need for a mechanism for weighting those extracted patches from the image to highlight their relative importance. In addition, it is sensible to use spatial filters of varying sizes to extract features at different resolutions. The algorithm we described was formalized as the convolutional neural network (CNN).

In this module, we will introduce the basic CNN architecture and classic architectures such as LeNet, AlexNet, VGGNet, and ResNet, which you will be implementing during the Computer Vision task in one of the Projects during the course. We will also brief the current state of CNN research and a few contemporary applications of computer vision.


Computer Vision Applications and the three “Rs” of Computer Vision
Jitendra Malik , a computer vision pioneer, proposed the 'Three R's' as the classic problems of computational vision: reconstruction, recognition, and (re)organization. You might have come across many computer vision applications in your daily life, which are most likely attempts to solve one of the three Rs. In this section, we brief a few common applications of computer vision.

Optical character recognition (OCR)


Figure 4. LeNet 5.

Optical character recognition or optical character reader (OCR) is a technology to convert images of text into actual text. The text images can be typed, handwritten, or printed into machine-encoded text such as a scanned document, a photo of a document, or an image that contains the text. Figure 4 shows the results of probably the first OCR task, LeNet. OCR is a commonly used method to digitalize printed text to reduce storage size and enable editability and searchability.

Object Recognition

Object recognition is a computer vision technique for identifying objects in images or videos. It might be relatively easy for a human to look at a photograph or watch a video and spot people, objects, scenes, and visual details. However, it is not as straightforward for a computer not only to recognize the items but also to understand what the items mean to the level of understanding of a human. Object recognition is a key technology behind driverless cars (Figure 5), enabling them to recognize a stop sign or to distinguish a pedestrian from a lamp post. It is also valuable for disease identification in biological imaging, industrial inspection, and robotic vision applications, just to name a few.


Figure 5. Object recognition technology from Tesla’s Autopilot and full self-driving capabilities.

Face Recognition


Figure 6. Apple’s Face ID.

Facial recognition is a specific case of object detection where the primary object is a human face. While similar to object detection as a task, where features are detected and localized, facial recognition performs not only detection but also recognition of the detected face. Facial recognition systems search for standard features and landmarks like eyes, lips, or a nose and classify a face using these features and the positioning of these items. The facial recognition system is used as an ID verification process to authenticate users for security purposes, such as Apple's Face ID (Figure 6), Clear airport security service, and the United States driver's license photo database.

Image augmentation applications on various social media services, such as Snapchat, use an algorithm to detect faces and perform augmentation to swap faces in an image for entertainment (Figure 7).


Figure 7. Snapchat’s Face Swap.

Vision-based Biometrics


Figure 8. Sharbat Gula, photographed in 1984 and 2002.

Sharbat Gula, one of the students in an informal school at the Nasir Bagh refugee camp in Afghanistan, was identified 18 years later from a photograph taken in 1984 when she was 12, using an analysis of her iris pattern image (Figure 8). Daugman (2004) computed IrisCodes from both of her eyes from the photograph in 1984 and 2002, respectively, and matched them using a Hamming Distance to confirm that the images are of the same person. You can read more about the story here .

Did I get this?
You are tasked with building a foot traffic monitoring system for a retail store. The system should be able to:

- Maintain a count of how many people are in the store.

- Detect when people are entering and exiting the store.

- Record where, within the store, people are moving.

- Maintain a count of how many people are sitting and how many people are standing in the store.

Consider the following Computer Vision tasks:

A. Optical Character Recognition

B. Face detection

C. Object detection

D. Pose estimation

E. Object tracking

Which task(s) would the system need to handle?


Correct. C is correct. The system will need object detection to detect people in the store. D is correct. The system will need pose estimation to detect who is standing and who is sitting. E is correct. The system will need object tracking to track where people are moving within the store and when people enter and exit the store.


Types of Layers In a Convolutional Neural Network
The CNN architecture incorporates a number of layer types as follows:

The input layer accepts a 3D matrix of size W1×H1×D1

The convolutional layer accepts a 3D matrix of size W1×H1×D1 and has four hyperparameters: the number of filters K, the spatial extent F, the stride S, and the amount of zero padding P. It then outputs a 3D matrix of size W2×H2×D2 where
 
 

The pooling layer accepts a 3D matrix of size W1×H1×D1 and has two hyperparameters: the spatial extent F and the stride S. It then outputs a 3D matrix of size W2×H2×D2 where
 
 

The fully connected layer is identical to a fully connected network layer. It accepts a K-dimensional vector and outputs an l-dimensional vector, where l is the number of nodes in this layer (if the input is a 3D matrix, this matrix is flattened to become a vector).

Classic Architectures
LeNet

LeNet is perhaps one of the first successful applications of CNNs was made in 1998 by Yann LeCun et al. They proposed a CNN architecture called LeNet for the task of document recognition. In particular, the task they considered was to recognize handwriting. The architecture of LeNet (Figure 9) contains two convolutional layers separated by two pooling layers, and then finally, two fully connected layers to perform the eventual classification. The convolutional filters used were of size 5x5, with a stride of size 1, whereas the pooling layers were 2x2 with a stride of 2.


Figure 9. Data flow in LeNet. The input is a handwritten digit, the output a probability over 10 possible outcomes.

AlexNet


Figure 10. Data Flow of AlexNet

AlexNet was the first successful application of CNNs to the ImageNet dataset and is considered a breakthrough in the application of deep learning to computer vision. It was the first CNN-based winner of the ImageNet challenge. It achieved an error rate of 15.3 percent on ImageNet in 2012, which was state-of-the-art at that time. The architecture of AlexNet (Figure 10) contains five convolutional layers with max pooling and three fully connected layers before making 1,000 class prediction problems via the softmax function. AlexNet contained eight layers and was also the first to use the fast and efficient Rectified Linear Unit (ReLU) activation functions and used extensive data augmentation. The original AlexNet uses 11x11 convolutional filters with a stride of size 4. Figure 11 shows the first layer of the AlexNet convolutional filter.


Figure 11. Image filters learned by the first layer of AlexNet.

VGGNet

Several follow-up CNN architectures improved on AlexNet by using even smaller convolutional filters and even deeper networks. A noteworthy successor to AlexNet was the architecture called VGGNet from Oxford University. It cut the error rate of AlexNet on ImageNet in half as it got an error rate of just 7.3 percent. VGGNet was twice as deep as the AlexNet as it had 16 layers and used smaller convolutional filters of size 3 by 3. In total, VGGNet had a staggering 138 million model parameters. The main rationale for using smaller filters and more layers is that the stack of smaller filters has the same receptive field as some larger ones, but more layers allow us to incorporate more non-linearities and potentially fewer model parameters overall.


Figure 12. Comparison of AlexNet and VGGNet.

VGGNet improves over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3X3 kernel-sized filters one after another (Figure 12).

ResNet

He et al. (2015) introduced the Residual Network or ResNet as "shortcut connections" that allow layers to be skipped. ResNet researchers showed that a 56-layer neural network has both higher training as well as a higher testing error compared to a 20-layer network. One reason for this surprising finding is that the valuable predictive signal attenuates as it passes through many layers and the associated activation functions. The solution to this problem and the key idea behind a ResNet is to fit the residual value of the signal instead of the actual desired mapping. Doing so allows us to train a staggering 152-layer residual network with an error rate of just 3.57 percent on the ImageNet dataset, which is actually better than human-level accuracy on this task.


Figure 13. Comparison of Normal CNN layer and Residual layer.

Figure 13 compares how a ResNet and a regular CNN operationalize a residual layer. The left figure shows a standard layer in a CNN where it tries to fit the actual desired mapping H of x. A residual layer, in contrast, fits the residual F(x) = H(x) - x. Architecturally, it is acquired using a residual or a short-circuit connection, as shown in the right figure. It is common to use residual connections after every couple of convolutional layers, as seen on the architectural diagram of ResNet-18 in Figure 14.


Figure 14. ResNet-18 architecture.

The Current State of CNN research

Now that we have seen several milestone architectures for CNNs, let's now see the current state of CNN research. In recent years, the trend has been to go deeper as extra layers of non-linearities give significant accuracy boosts. In addition, recent algorithms use smaller filters as they can have similar receptive fields as some of the larger filters but simultaneously are parsimonious in terms of model parameters. Further, it is also becoming increasingly common to residual connections in state-of-the-art CNN architectures these days. Figure 15, taken from a 2017 paper by Canziani et al. , shows the accuracies of different models on the ImageNet dataset. It is a remarkably rapid area of research with successive innovations, and most modern-day architectures achieve accuracies better than humans on the ImageNet dataset.


Figure 15. Complexity comparison of deep neural network models. Source: Canziani et al. (2017)

Did I get this?
You are given an input image of shape (height = H, width = W, n_channels = C). You are using a convolutional layer L with spatial extent 1, stride 1, and no padding. The choice of the number of filters in the convolution layer and whether to use a subsequent pooling layer (parameters defined by you) is left up to you. Which of the followings is (are) correct in terms of change in dimensions of the input image?

After the convolutional layer L, the image becomes smaller (the layer output is a 3D matrix with smaller width and height).

The convolutional layerL in this example (with spatial extent = 1, stride = 1, and no padding) will lead to a change in all three dimensions of the input image.

You can use a pooling layer to change the H and W dimensions of the input image but cannot change the C dimension of the image with just the pooling layer. ++

You can use just a pooling layer to change all three (H, W, C) dimensions of the input image. 

After the convolutional layer L, the image width and height remain the same, but its depth can be changed.++


Correct. A pooling layer can only change the H and W dimensions of the image and not the channel dimension.

Correct. The depth of the output image is equal to the number of filters, which can be set to a value different from C.


What property of the image data makes Convolutional Neural Networks (CNNs) a good model choice?

Spatial proximity of the image features.++

Visual nature of image data.

Feature engineering is not required for CNNs.

CNNs have fewer parameters than fully-connected Neural Networks.

Image data is high-dimensional.



At the beginning of a data science process on any task, it is important not just to understand the dataset and the tools available but also the hardware resources available to you for the task at hand. Most data science tasks rely on using statistical or numerical techniques to process the data and/or optimize some discrete/continuous optimization task. In doing so, you’ll need to assess whether a task is feasible with the hardware you have and what hardware to use to make the most of the computational budget you’re given to solve the analytic objective.

In this chapter, our goal will be to give a brief tour of what hardware resources one should look at when deciding what hardware to get for a specific task. While it will not be overly exhaustive, our goal here is twofold:

First, you will want to spend time to make sure that the tools you use actively use the computational resources you are given. Taking the time to understand if and when you can scale in any cloud computing resources you are using can save you time and money in performing any computational task, which then can be a boon as you assess different statistical techniques and their performance in your analytic objective.

Second, you will want to assess how you are using the various libraries common to data science. Compared to more traditional single-threaded and multi-threaded applications, data science libraries tend to require users to focus on operations, unlike those more traditionally used, such as conditional indexing and matrix multiplication. In this chapter, you’ll find motivation for learning these libraries to the depth necessary to perform these tasks, as they form the basis for highly performant and highly readable data science code.

We discussed the various types of memory, from fastest and most expensive to slowest and least expensive, along with principles for their efficient use, in an earlier section:

Efficient Memory Access
Besides memory, the other main aspect of any hardware system you will need to assess is the processors inside the system. In data science, you will see a variety of processors being used, but they tend to split into three main categories, from least expensive to most expensive: CPUs, GPUs, and DSAs.

CPU
The primary processor on a system, the Central Processing Unit, is used on most systems for the majority of complex calculations unless the application developer specifically invokes another processor. They can handle less parallelism than GPUs but are more able to handle longer sequences of branching statements with ease.

GPU
Also known as the Graphics Processing Unit, this is an additional processor present in systems to help manage graphics and other calculation-intensive operations where there is significant data parallelism, i.e., where we can split the data into chunks and process each chunk separately. While most systems nowadays have an integrated GPU of some kind, in data science we tend to focus on systems that have a separate GPU, which has performance in mind. As data science applications and graphics applications require similar data-parallel computation, these tend to be much faster in some tasks than CPUs.

DSA
Also known as Domain-Specific Architectures, this category ranges from Google’s TPU to Intel’s Crest [1] . These are purpose-built systems to solve computationally expensive modeling problems, like those found in neural networks. These tend to bring the largest performance gains for data science but are further limited in what they can do, as they are built to solve specific problems and can be difficult to program directly.

Choosing processor types
While it is tempting to use the most efficient DSAs to try to squeeze as much performance as possible out of the newest systems, remember that, in the data science process, you will need to budget cost as well as time. It might be useful to use a DSA or a GPU, but you should remember to do the following when deciding whether to use either chip for a project or not:

If you have access to trial usage of the DSA/GPU for your project, check that the tools you are using utilize the accelerators at all. As these chips require separate programming APIs to utilize, the tools that you use them for might not be compatible out of the box or at all.

Additionally, before setting and forgetting your system, check the usage patterns of the code you are able to run.

If the code is relatively I/O, Network, or Memory intensive, it might not make sense to use an accelerator chip, i.e., a GPU or a DSA, in your system. Instead, it might pay to try to use multiple processors or multiple computers together to solve the work in question.

If the code is computationally expensive, check to see if the memory usage aligns with the memory limits of your GPU/DSA. As these chips have their own memory, keeping to such limits can ensure that your code runs smoothly.

Lastly, and most importantly, use profiling tools on your code. While such tools can be difficult to use at first, they provide the best way to see immediately where the potential slowdowns are and can give you ideas about where you need to optimize your code. If you do not want to use a profiler, you could even use just a simple timer in your program and count the time taken to run some hot sections accordingly.


You are tasked with making a hardware choice for the end-to-end training of a model. The training procedure for this model uses matrix computations and frequently retrieves data from a database service on the cloud. What would be the correct hardware choice to accelerate the overall training procedure?

GPU to handle training and multiple CPUs to handle network reads.++

A single GPU to handle the entire training procedure.

Multiple TPUs to handle training and network reads.

Choice of hardware does not matter in this case as the training characteristics would mean a similar performance across all hardware choices.

Multiple GPUs to handle training and network reads.

Correct. Since there is a heavy interaction between the model and the network (for reading from the cloud), using accelerators such as GPUs and TPUs may not yield the desired speed-up as opposed to using just the CPU.

In data science, you will work with large amounts of data with a variety of tools in interpreted environments. The sheer amount of data in today’s data science systems requires a lot of experience to manage the underlying hardware, and doing so efficiently and effectively is usually prohibitively expensive. Additionally, managing the security, fault-tolerance, and data governance of the systems you are working on can be incredibly difficult to do at an IT level.

In situations like these, especially when you might not know the scale of the system you want to build, cloud computing can be a great boon to your efforts. By purchasing Infrastructure as a Service (IaaS), Platform as a Service (PaaS), or Software as a Service (SaaS), you can choose to rely on another company to manage some aspect of the underlying hardware/software issues and focus on data science. The main benefits of cloud computing are the following:

You are able to trade the cost of buying all of the hardware for your projects and managing them, for whatever cost the cloud provider asks you to pay. In practice, these costs will be based on your usage rather than the hardware itself, which can allow for savings in the long run.

You do not need to worry as much about capacities. As the main cloud providers have large data systems in place, you do not need to worry about infrastructure capacity or about ensuring that systems cache as intended. Instead, you can make use of the servers that are present to accomplish your goals.

You can purchase these services for potentially less than what you might have thought possible. As you share the cloud with other users, you are able to pay less for more power which, along with specialized chips only available in some cloud instances, can make the cloud much cheaper for the processing power given than other systems.

For all of these reasons, the cloud is becoming the place to do data science, especially for large and unstructured data. As a result, however, you will need to understand the cloud and use it accordingly to manage your data. In particular, you will need to pay attention to the types of storage and processing you are purchasing from the cloud. In general, there are many ways to use the cloud, but the most important factor to consider when using the cloud to host data is the structure/usage patterns of the underlying data.

For unstructured data that has high-data temperature -- i.e., data that you need to read and write a lot -- you should try to use a block-based storage system. Systems like these work like network hard drives, which can make them incredibly useful when you wish to perform large-scale processing tasks and data cleaning tasks.

For large files that are low-write but high-read, an object-based storage system might be more beneficial. Object-based systems store files as key-value pairs, allowing people with the write key and access permissions to download the files accordingly. They are best for Binary Large OBject files (BLOBs) or when you might want to distribute a data file to many people to allow them to process it on their own systems.

For structured data that you might need to write to constantly, you might want to consider a relational database system. These systems enforce a lot of constraints on the underlying data but, in doing so, ensure that each operation has Atomicity, Consistency, Isolation, and Durability properties. These systems use SQL, or Structured Query Language, as a basis for every operation available and are especially important when you want consistency in your data.

Alternatively, if your data is not structured enough to fit in the tight schema requirements of a relational database, one can try a non-relational database system. These systems don’t need to adhere to the requirements of an SQL schema structure and thus to the consistency provided, but gain added flexibility and scale. Such databases allow you to store a wider variety of data and scale it up to more computers much easier than a traditional database system.

Each of these systems will be useful for different use cases, and you will need to carefully consider your use case before deciding what to use. Besides these hosting concerns, you will also need to consider the following carefully:

Your Budget: As cloud resources are based on what you use, it pays to monitor your resources carefully and constantly check your usage limits. Most cloud providers have methods to monitor your usage, which can help you both understand and automatically stop extraneous usage accordingly.

Your Security: While cloud providers maintain the security of the infrastructure for you, you still need to keep in mind good practices for the security of the software you are running. Checking if your systems are vulnerable to the latest Log4J or SQL injection attack is not your main priority as a data scientist, but knowing best practices and discussing the architecture with experts is a huge priority, especially if the data in question is confidential user data.


You’re actively working on a codebase that runs small ML jobs on the cloud and wanting to store and write the processed data as you actively work on the system. Now you want to start processing much larger ML jobs. What is the best option for cloud storage, assuming you are already working on a cloud instance?

A No-SQL database.

A relational database, like AWS’s RDS.

Object-based storage, like AWS’s S3.

A personal data cloud system, like Google Drive.

Block-based storage, like AWS’s Elastic Block Storage.++


Correct. Block storage is as close to a local drive you can work with as possible, so thus, this is the best choice for something akin to that on the cloud.

Throughout the course of this module, you have learned a lot about the underlying hardware that makes or breaks data science operations. While choosing underlying hardware goes a long way to ensuring that your system works within the budgets you allocated for yourself, you should also be prepared to optimize your code accordingly. Given that times for jobs can range from overnight to multiple days, even a meager 10% speedup can provide more time to tune your model and extract bigger insights into your problem.

Even if you have experience optimizing C or C++ code, data science code can be much trickier to optimize. To better understand why, let’s consider some notable factors at play.

Interpreted Languages
Most languages in which data science is done are interpreted rather than compiled. When you interpret code, you do not have the same ability to statically optimize the code as a compiled language. As a result, some of the automatic optimizations you might expect to happen will not, and this will result in slowdowns compared to C++.

Usage of Matrix-Manipulation as a Primitive
For most data science stacks, you will need to use matrix-manipulation libraries instead of implementing every bit of an algorithm from scratch. These libraries provide fast implementations of specific operations in a pre-compiled binary, ensuring that the actual code is much faster than what is possible in the interpreted language as is.

Large-Software Stack
Sometimes, the code you are looking at will have layers upon layers of code underneath it that could be the source of performance issues. Even simple data science projects will have libraries for linear algebra operations linked in, along with the tree of libraries your project requires. This added complexity can make it difficult to understand where potential problems are and make the relationship between code and performance harder to reason about.

Despite these factors, however, there are principles that can be followed to actively improve performance. These are not going to be surefire ways to optimize code, but they tend to lead to better performance more often than not.

Vectorization
Many data science operations require the use of some matrix or data-frame manipulation library. Vectorization is the process of writing your code in the language of that library. This involves reducing the number of imperative programming constructs you use, from if-statements to for-loops, and increasing the number of functional programming constructs you use, such as reduce and map functions. The general goal of this step is to specify what you want the library to do rather than how you want the library to do it, as the libraries you work with can then optimize the performance accordingly.

In particular, your goal should be to at least do the following:

Remove for-loops and replace them with maps: For-loops in interpreted code are much slower than for-loops in pre-compiled code. If you can replace a for-loop with a map function or a function without any side-effects that take in each element of the matrix as input, apply some operation without any side-effect, and return that element, your code will speed up accordingly.

Use conditional indexing instead of if-statements: Like the above tip, moving branching code from your interpreted environment to the pre-compiled environment will generally be faster. If you are able to create a function that associates some element of a matrix with a Boolean without side effects, using conditional indexing to express what you want makes it easier for the library to perform its job for you.

You could also look at trying things like stride manipulation or pivoting, but the main goal of vectorization is to utilize the resources involved as effectively as possible. The more effectively you can use the library you have access to, the faster you will be able to make your code.

Multiprocessing or Multithreading
If your operation deals with processing large amounts of data, it might pay to make your code friendly to multiprocessing or multithreading libraries. Here, you will create either separate copies of your program, called processes, or separate execution environments that share data, called threads. In doing so, you can likely parallelize disk operations that might be the main bottleneck of your program.

Alternatively, you could also try switching your programming model to use something like Spark or Hadoop Map-Reduce. These tools utilize the “Map-Reduce” framework for computation. In essence, you deconstruct the pipeline you wish to parallelize into separate phases, consisting of the following phases:

Mapping: Here, you separately process each line of a data file, and apply some operation to turn it into a key, value pair.

Reducing: Here, you take all of the data for a given key, and process the values together, producing some output to then map again.

While it can be challenging to construct the pipeline in this manner, it is necessary to learn how to rephrase the calculations you want to go into these varying programming frameworks, and it is part of your job. If you take courses on cloud computing or ML on Large Datasets later on, you’ll be exposed to these tools and be forced to grapple with these concepts in more detail than we have time here to cover.

These frameworks can automatically parallelize the job with those functions given, allowing you to process more data faster.

Delegating to a Compiled Language
If the above steps are not enough, you could write sections of your code in a compiled language and then create functions that use that code in your interpreted language. While this is generally not advised unless you know the code is the main bottleneck, it can provide large speedups at the cost of technical complexity.


You are working with sklearn to build predictive models on customer data to predict customer behavior for a large fortune 500 company. You are getting frustrated due to the slow performance of sklearn on your laptop and want to get better results as quickly as possible.

Of the following, what is your best course of action to get better results, given time and money?

Re-write the entire codebase to use MapReduce instead. This will take a significant amount of time (> 100 hours) and require $5 per hour to run due to the custom nature of your job but it will allow you to scale to high amounts of data.

Get yourself access to a TPU cluster, and apply your code there. This will cost about $4.3 per hour of computing time and will also take 8 hours or so to set up properly.

Get yourself access to Colab Pro Plus and apply your code there on a GPU device. This will cost about $50 a month but will only take 2-4 hours to set up, and you will not have to worry about computing time costs.

Find a library other than sklearn to use.++

Get yourself a new laptop with the fastest available consumer-grade CPU and the largest RAM. This will cost $1500 and take 8 hours to properly set it up for your task at hand.



Correct. While you could get marginally better results with a better CPU or better hardware, it is likely that you will need to use some sklearn replacement or use another software stack to improve performance much more. An example could be ‘scikit-learn-intelex.’



MapReduce is a good example of a paradigm that takes advantage of the memory hierarchy in order to achieve an effective and efficient result. It was developed to implement parallel processing of large amounts of data in data centers. We offer a small explanation and a concrete example to illustrate. It would take a whole course in itself to teach how to use MapReduce at an expert level in your work.

Data centers have scaled to larger and larger amounts of data by using large numbers of consumer-grade and sometimes unreliable hardware. In order to perform well in this type of context, MapReduce aims to develop algorithms that take advantage of the strengths of data centers while coping well with their weaknesses. Data warehouse compute clusters have a few characteristics that make it challenging to develop parallel processing code:

Unreliability: Any machine might go offline or be sluggish at any moment, so the algorithm needs to be fault-tolerant.

The data are distributed across the machines, and moving data between machines is inefficient, so algorithms should avoid moving data.

As we have discussed, memory size and speed form a memory hierarchy, from the CPU's cache to RAM to disk. As we work outward from the processor to each layer of the memory hierarchy, each memory stage is cheaper but slower than the previous stage. The network occupies the lowest level of the memory hierarchy: moving data across a data center is much slower than moving data inside a single machine. So for an algorithm to be efficient in a data center, it's best not to move large amounts of data across the network.

The MapReduce framework allows the programmer to design algorithms for distributed data processing using a divide-and-conquer approach. The name MapReduce comes from two common concepts in functional programming: Map applies the same function to every element in a list, and Reduce uses a function (like 
 , adding two numbers together) to turn a list into a single value. The key ideas are that by breaking the large-scale processing into map and reduce tasks, the problem can be decomposed into tasks that can be executed in parallel without requiring communication between the tasks or moving large amounts of data. The system can then manage how these tasks are allocated to the hardware. We will use the term “worker” to describe the hardware layer, which could be a physical or virtual machine, or multiple subprocesses on the same machine.

image
Figure 1: MapReduce Processing

Figure 1 illustrates how MapReduce works. The data are separated into multiple chunks. The same function is applied, or mapped, to each chunk. Each map task is a small, granular operation which applies to a small piece of data. A master controller assigns these map tasks to workers. Because each worker applies the map task to the data it has locally, its data does not have to be moved across the network. The map tasks can be run in parallel, with each piece of data on a different worker, including on different workers in the data center. Each map task transforms its data into a set of key-value pairs. The controller also assigns reduce tasks to process each key-value pair. The reduce tasks combine the outputs of the map tasks into a single result for the entire dataset.

A common problem used to illustrate how MapReduce works is counting the words in a collection of documents -- for instance, counting all the occurrences of the word "Pittsburgh." Each map task is assigned a specific chunk and outputs a set of key-value pairs. For this example, the key is the word and the value is how often the word occurs in the chunk. Across the map tasks, multiple pairs may be produced with the same key: one document might contain the word &quot;Pittsburgh&quot; six times, and another might contain it only once. Each reduce task takes as input a group of key-value pairs with the same key and combines them. For the word count problem, this would mean adding all the counts for "Pittsburgh" to arrive at one final value for the entire dataset.

So far, we have explained how the divide-and-conquer approach works. Note that each map task takes a chunk of data as input and outputs a key-value pair. The data are given to each map task, and computing the result in the reduce tasks requires only tracking and combining the key-value pairs. If we think about the word count problem, the key-value pairs are much smaller than the input documents, and we can see that we are moving much less information by moving only key-value pairs and not the original documents. This addresses one of our goals, reducing communication of data between machines inside the data center during the data-intensive portion of the processing.

The other problem to solve is fault tolerance. Notice that the map and reduce tasks are not tightly coupled, unlike a sequential problem where everything needs to happen in a specific order. The individual map and reduce tasks do not depend on each other, and if one needs to be done twice or restarted, the other tasks do not need to wait for it to complete in order to start. If a worker fails to start or is running slowly, the master controller can re-assign the work to another worker, as long as it keeps track of the tasks and the key-value pairs. Although the master controller seems like a single point of failure, it can also be restarted and run on a different worker if it crashes.

Computation in data centers requires addressing fault tolerance on unreliable hardware and reducing communication between workers. The MapReduce framework addresses both.

Machine Learning Terminologies
Machine learning involves formulating a hypothetical mapping from the input features to the output space. It is often the case that many different implementations of the mapping could work (for example, classification can be carried out by logistic regression, support vector machines, or k-nearest neighbors), but the best mapping depends on the underlying data distribution and available training data. Model selection is a systematic process of identifying this best mapping and builds upon the following concepts:

A model is a set of assumptions you make about your data, which in turn defines the hypothesis space over which learning performs its search

The model parameters are the numeric values or structures that are derived from the learning process.

The model hyperparameters are the numeric values or structures that impact the learning process but are not selected by the learning process.

The learning algorithm specifies the way in which model parameters are updated or derived from the input data.

With these definitions, model selection can be considered the process of identifying the learning algorithm, hyperparameters, and associated pre-processing and post-processing steps that yields the best-fitting model for your data. The table below shows an example of two candidate models for binary classification.

Component

Model 1

Model 2

Hyperparameters

Learning rate 
 , regularizer 
 , number of iterations 

Learning rate 
 , regularizer 
 , number of iterations 

Learning algorithm

Gradient descent over the logistic loss function with L2 regularization

Gradient descent over the logistic loss function with L2 regularization

Pre-processing

None

Normalize the data to have zero mean and unit variance

Post-processing

None

None

Here both models involve using regularized logistic regression to perform classification, but have different hyperparameter and pre-processing components, which in turn reflect different assumptions about the underlying data. For example, Model 1’s higher regularizer value corresponds to the assumption that the dataset may contain outliers which the model should not overfit to (recall that higher regularizer enforces lower variance at the cost of potentially higher bias). On the other hand, Model 2’s pre-processing step is suitable for datasets where the feature values have different scales and need to be normalized prior to gradient descent. In what follows, we will introduce ways to compare a set of candidate models to select the best one; however, we should first discuss what “best” means.

Prediction versus Inference
Prediction is the process of developing models from available data to predict outcomes from new and unseen data. Here the focus is on generalization, and predictive models are evaluated against data they have not been trained on, using the standard performance metrics (e.g., MSE for regression, F1 score for classification). An example prediction problem is that of predicting the number of hospital beds needed in the event of a surge in Ebola cases using historical data from past outbreaks. Prediction accuracy is important in this case because it can help inform resource allocation to hospitals in case a new Ebola outbreak takes place.

Inference is the process of identifying relationships between independent variables (input features) and dependent variables (outcome values). Here the focus is on interpretability. Inference models are evaluated on both their goodness of fit and simplicity. An example inference problem is inferring people’s political inclinations based on their demographic information. Model interpretability is important here because knowing which factors have the largest influence on political inclinations can help a politician strategize his/her campaign for an upcoming election.

Due to their differing priorities, the best prediction models are typically very different from the best inference models. Prediction models are fitted on only the training set, tend to be complex with many features, and have good validity but low interpretability. In contrast, inference models are fitted on the entire dataset, prioritize retaining only the most salient features, and have low validity but good interpretability. Another way of viewing this difference is via the focus on accurately predicting unseen data (prediction) or explaining the underlying data generation process (inference).

To replicate the setting of performing prediction on unseen data, we can reserve a random portion of our dataset for testing and only use the remaining data for training. The model selection procedure can then be expressed as follows:

Input: candidate models M1, M2, …, Ml

Procedure:

Split the dataset into train set and test set.

For each candidate model Mi:

(i) Train Mi on the train set.

(ii) Evaluate Mi’s performance on the test set

Output: the model Mj with the best performance on the test set.

If hyperparameter tuning is also part of the model selection process, the train set can be further split into a train subset and validation subset.

Input: candidate models M1, M2, …, Ml and hyperparameter space S.

Procedure:

Split the dataset into train subset, validation subset and test set.

For each candidate model Mi:

(i) Pick the hyperparameters that give the best performance on the validation subset when Mi is trained on the train subset. We call these the best hyperparameters.

(ii) Retrain a new model Mi using the best hyperparameters on the combined data from the train subset and validation subset.

(iii) Evaluate Mi’s performance on the test set.

Output: the model Mj and the associated best hyperparameters with the best performance on the test set.

Here we note that step (i) can be performed by iterating through all possible hyperparameter values in the space S (grid search) if S is finite and computational resources are not a problem. Alternatively, we could sample the hyperparameter values uniformly from S (random search). When there are multiple hyperparameters, random search is preferred because it allows us to explore distinct values for each hyperparameter at each trial.

While the above procedure is sufficient to demonstrate the prediction of a model’s validity, it relies on only two random splits, which may skew the model selection outcome if we get a bad split (e.g., if most of the outliers happen to be in the test set). A more rigorous alternative is to perform k-fold cross-validation as in the inner loop of the model selection procedure.

Input: number of folds K, candidate models M1, M2, …, Ml and hyperparameter space S.

Procedure:

Split the dataset into train set and test set. Split the train set into K folds.

For each candidate model Mi:

(i) Pick the hyperparameters that give the best cross-validated performance for Mi on the train set. We call these the best hyperparameters.

(ii) Retrain a new model Mi using the best hyperparameters on the train set.

(iii) Evaluate Mi’s performance on the test set.

Output: the model Mj and the associated best hyperparameters with the best performance on the test set.

Did I get this?
You are tasked to build a model on a very noisy text dataset for an organization. What would your strategy to obtain the trained model look like? (Assume you need to select the most probable and reasonable approach)

Looking at the task in hand, select the most appropriate model. Then you proceed to find the model parameters and finally report the computed hyperparameters back to the organization.

Since it is a text dataset, and you know that the dataset has a lot of noise, you decide to use a very low learning rate. You then decide on the training algorithm and report back the model’s parameters back to the organization.

Looking at the task at hand, select the most appropriate model. Then you proceed to find the hyperparameters and finally report the computed model parameters back to the organization.++

Since it is a text dataset, you narrow down all the parameters that could be used in this scenario. You then proceed to select a model to train and report the results back to the organization.

Correct. The correct order is finding a Learning algorithm —> Hyperparameter —> Model Parameter.

When building a model, which of the following is a reasonable order for determining the model components?

Hyperparameter —> Parameter —> Learning algorithm.

Learning algorithm —> Hyperparameter —> Parameter.

Learning algorithm —> Parameter —> Hyperparameter.

Hyperparameter —> Learning algorithm —> Parameter.

Parameter —> Learning algorithm —> Hyperparameter.

Correct. You need to know the learning algorithm to decide which hyperparameters are needed (e.g., logistic regression needs the learning rate and the number of iterations while the decision tree needs the max depth). Once you have the learning algorithm and hyperparameter, you can fit the model on training data to obtain the param values.


In inference, models are trained on the entire dataset to derive the relationships between independent and dependent variables. Thus, there is no longer the notion of a train-test split. Instead, model selection is based on probabilistic metrics that reward goodness of fit but also penalize model complexity, with the goal of acquiring the most reasonable model that is sufficiently simple/interpretable. We introduce a number of popular metrics below.

Akaike Information Criterion (AIC). Derived from frequentist statistics, the AIC score of a model M is computed as

(2K-2LL(M))/N
where KM is the number of parameters in h, LL(M) is the maximum log-likelihood of M on the dataset, and N is the size of the dataset. For regression, LL(M) is the mean squared error, and for binary classification, LL(M) is the logistic loss. A model with a smaller AIC value is considered better for inference.

Bayesian Information Criterion (BIC). Derived from Bayesian statistics, the BIC score of a model h is computed as
K * log(N) - 2LL(M)

where the variables KM, N, and LL(M) are defined similarly as in AIC. A model with a smaller BIC value is considered better for inference. It can be shown that BIC is proportional to AIC, although the former penalizes complex models more heavily. For small training datasets, it may select models that are too simple.

Minimum Description Length (MDL). Derived from information theory, the MDL score of a model M is computed as
threat

Where L(M) is the number of bits required to represent the model h, and L(D|M) is the number of bits required to represent the model predictions on the dataset. A model with a smaller MDL value is considered better for inference.

The AIC formula penalizes models with high degrees of complexity in what way?

The maximum log-likelihood LL(h)

The constant 1

Log N

LL(h) / N

The number of parameters K

Correct. AIC formula penalizes models with high levels of complexity via the number of parameters K.

Metrics are employed to objectively evaluate the performance of machine learning models. When selecting a metric, you should always have the end goal of the machine learning application in mind.

In practice, we are usually interested not just in making accurate predictions but also in using these predictions as part of a larger decision-making process. Before picking a machine learning metric, you should think about the high-level goal of the application, often called the business metric. The consequences of choosing a particular algorithm for a machine learning application are called the business impact. Despite being called “business impact”, not losing track of the end goal is important in any scientific domain. This can be the high-level goal of avoiding traffic accidents or decreasing the number of hospital admissions. It could also be getting more users for your website, or having users spend more money in your shop. When choosing a model or adjusting parameters, you should pick the model or parameter values that have the most positive influence on the business metric. Often this is hard, as assessing the business impact of a particular model might require putting it in production in a real-life system.

In the early stages of development, and for adjusting parameters, it is often infeasible to put models into production just for testing purposes, because of the high business or personal risks that can be involved. Imagine evaluating the pedestrian avoidance capabilities of a self-driving car by just letting it drive around, without verifying it first; if your model is bad, pedestrians will be in trouble! Therefore we often need to find some surrogate evaluation procedure, using an evaluation metric that is easier to compute. For example, we could test classifying images of pedestrians against non-pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate. This closest metric should be used whenever possible for model evaluation and selection. The result of this evaluation might not be a single number—the consequence of your algorithm could be that you have 10% more customers, but each customer will spend 15% less—but it should capture the expected business impact of choosing one model over another.

This section covers a number of common metrics used in classification, regression, and clustering problems. It is imperative that you understand which metrics are suitable for which use cases because your choice of metric (which should be decided prior to model training) will have an impact on the entire subsequent pipeline. or example, you may get different best models in the model selection process with different metrics.

Confusion Matrix
A Confusion Matrix (contingency table) shows how well a classifier performs compared to the ground truth labels. It is often used in a binary classification setting and has the following components:

True Positives (TP) is the number of positive data points that are correctly predicted as positive.

True Negatives (TN) is the number of negative data points that are correctly predicted as negative.

False Positives (FP) is the number of negative data points that are incorrectly predicted as positive. This is also called the Type I error in statistics.

False Negatives (FN) is the number of positive data points that are incorrectly predicted as negative. This is also called the Type II error in statistics.

A trick for remembering these definitions is that the second term denotes what is predicted, and the first term denotes whether this prediction is correct (true) or incorrect (false). For example, false negative means the negative label is predicted but it is false (i.e., the ground truth label is positive).

Ground truth 

Ground truth 

Prediction 

TP

FP

Prediction 

FN

TN

In a multi-class classification setting with 
 categories, a similar confusion matrix with 
 rows and 
 columns can be constructed, where the entry at row 
 and column 
 denotes the number of instances that have ground-truth label 
 and are predicted as having label 
 . In this case, the metrics TP, TN, FP, and FN are computed for each individual category. For example, if there are three categories A, B, and C, then the TP, TN, FP, and FN values for class A can be computed by treating A as positive and B, C together as negative.

Accuracy
Accuracy is the number of correctly classified instances, divided by the total number of instances in the dataset.

 

​​This is an intuitive measure, but should only be used when there is an even distribution of ground truth labels. If the dataset is imbalanced (e.g., there are many more negative than positive data points), accuracy can easily be inflated even by simple models. For example, if a dataset has 90% negative instances and 10% positive instances, a naive model that always predicts negative labels can already achieve an accuracy of 0.9.

Recall
Recall, also known as sensitivity or true positive rate, denotes the fraction of all positive instances that are correctly classified as such:


Recall is used when you want to optimize your model to detect positive instances as best as possible, potentially at the cost of many false positives. For example, cancer detection models may aim for high recall values because predicting healthy people as having cancer (false positive) is less costly than predicting people having cancer as healthy (false negative).

Specificity: 1- false positive rate


It can be interpreted similarly as recall, but in this case, you prioritize detecting negative instances as best as possible.

Precision
Prediction denotes the fraction of positive predictions whose ground truth label is also positive:

 

Prediction is suitable when optimizing the confidence of the positive predictions in your model. For example, if the government decides to cover the health care cost of anyone who has cancer, they will choose a cancer prediction model with high precision, so that money is not wasted on false positives.

F1 Score
F1 score is the harmonic mean of precision and recall:

 

Like accuracy, F1 provides a general measure of model performance without a bias for or against a certain type of error. The differences between accuracy and F1 score are as follows:

Accuracy is good for optimizing true positive and true negative, while F1 is good for optimizing false positive and false negative.

Accuracy is good for balanced datasets (with even distribution of the ground truth labels) while F1 is good for imbalanced datasets.

Matthews Correlation Coefficient
Matthews Correlation Coefficient is a correlation coefficient between the observed and predicted binary classification. A value of +1 means a perfect prediction, 0 indicates that the classifier did the same job as you would if you randomly guessed the label, and finally -1 means the classifier misclassified all observations. MCC is symmetric, meaning that no class is more important than another (if you switch the positive and negative labels, the value of MCC is unchanged).Resource: MCC in skikit-learn .

 

Logistic Loss (Log loss)
If your model outputs a probability value 
 that an input data point has a positive label, it can be evaluated by the logistic loss


where 
 is the ground truth label. The logistic loss is a value between 0 and 1; the lower the loss, the better your model is. In contrast to the metrics introduced so far, the logistic loss is differentiable and often used as the target loss function during model training.

ROC Curve: Receiver Operating Characteristic
The ROC curve is a chart that shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. Alternatively, it can be considered as expressing the sensitivity as a function of the false-positive rate. Area Under the ROC Curve, otherwise known as AUC, measures the entire area underneath the ROC curve, and it is the measure of the classifier's ability to distinguish between classes. It also provides a measure of performance across different thresholds. The AUC measures how well predictions are ranked and the quality of the prediction. AUC might not be useful for certain scenarios, such as it does not tell you much about the "cost of different errors," instead of giving similar weight to all errors. The general interpretation of the chart is that the higher the AUC, the better the model is at its task of distinguishing between classes, e.g., the model has predicted observations that are apples as apples and observations that are not apples as not apples.


According to the visualization below, what is the recall if we want the iACP model to have the (1- specificity) of at least 40%?

Correct. Looking at the plot, where the false positive rate on the x-axis is 0.4, the true positive rate on the y-axis is about 0.75.

Regression tasks will predict the state of a target variable based on other input variables. As a quick reminder, the target variables in these tasks are continuous values. Let us discuss the metrics that are used to evaluate the outcome of regression tasks:

Coefficient of Determination (R-squared)
R-squared, also known as the Coefficient of Determination, is the proportion of the variance in the outcome variable that can be predicted using the predictor variables. It tells you how well-observed outcomes are replicated by a model based on the proportion of total variation of outcomes explained by the model. When interpreting R-squared in a simple linear regression model, it is considered to be the square of the sample correlation coefficient between the outcomes and predictors (r2). If R2 is 0.5, this would mean that 50% of the variation in the dependent variable is explained by the predictor variables. A good model has a high R2.

When there are multiple regressors, then R2 is the square of the coefficient of multiple correlations ("correlation between the variable's values and the best predictions that can be computed linearly from the predictive variables"). This metric will provide an indication of how well new data will be predicted by the model.

Adjusted R-squared
Adjusted R2 is calculated by dividing the residual mean square error by the total mean square error (which is the sample variance of the target field). The result is then subtracted from 1. It identifies the percentage of variance in the target field that is explained by the input or inputs. Adjusted R2 is always less than or equal to R2. A value of 1 indicates a model that perfectly predicts values in the target field. A value that is less than or equal to 0 indicates a model that has no predictive value. In the real world, adjusted R2 lies between these values.

A weakness of R2 is that it cannot determine whether the coefficient estimates and predictions might be biased. The R2 will typically increase when a predictor is added to a model, and as you add more predictors, your model will likely overfit and result in a high R2. Adjusted R2 attempts to correct this overestimation. Adjusted R2 will only increase if newly added predictors improve the model more than expected and decrease when a predictor or predictors improve the model less than expected. It tells you the percentage of variation explained by predictors that will have an effect on the outcome. Basically, adjusted R2 will calculate R2 from the predictors that have a significant impact on the model. Adjusted R2 is best used to compare models with different numbers of predictors.

Mean Squared Error (MSE)
Mean Squared Error (MSE) is a measure of the quality of an estimator or a predictor (depending on context). A value closer to zero is always best. Mean squared error refers to the unbiased estimate of error variance: the residual sum of squares divided by the number of degrees of freedom.

Mean Absolute Error (MAE)
Mean Absolute Error (MAE) is the measure of errors between paired observations and is computed as the average of all absolute errors (the absolute error is the absolute value of the difference between a predicted value and the actual value). This metric is used to measure accuracy. You use the Mean Absolute Percentage Error (MAPE) to compare predictions and interpret whether the size of an error is small or large. The MAPE is a model evaluation technique that clearly interprets the relative error.

Root Mean Squared Error (RMSE)
Root Mean Squared Error (RMSE) gives weight to large errors since it squares the errors before computing the mean. The RMSE is computed by first determining the residuals (difference between the actual and predicted y values). Residuals are squared, and the squares are averaged. Finally, the square root of the averaged squares will result in the RMSE. An easier way to think about the formula is: "square root of (1-r2) multiplied by the standard deviation of y.


In regression, which of the following metrics should we maximize to ensure that we have a better model?

R-squared

Adjusted R-squared

Mean Squared Error

Mean Absolute Error

Root Mean Squared Error
Correct. Larger values of Metric 1 and 2 indicate a better fit model.

The previous page focused on the metrics for evaluating supervised learning problems. The presence of labeled data makes it somewhat straightforward to train and test the model's performance. Now, we will focus on metrics that can be used when labeled data is not present. There are two approaches to evaluating clustering. The Internal and External evaluation approaches. The internal approach involves summarizing the clustering task to a single quality score, while the external approach compares the clustering to a ground truth classification; ground truth is empirical evidence or data that is provable. Clustering can also be evaluated by an expert.

Internal Evaluation
Internal Evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score. The cluster with the best score is seen to be the best. Internal evaluation although useful, can have its drawbacks. It gives insight into how one clustering technique performs against another but it can not speak to the validity of the results in the clusters. A sound example from Wikipedia illustrates this: k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means nor of an evaluation criterion that assumes convexity, is sound.Let's look at internal evaluation techniques that are used to assess the quality of clustering methods:

Silhouette Coefficient
The Silhouette Coefficient shows how similar a data point is to its cluster compared to other clusters. It is calculated using the mean intra-cluster distance and the mean nearest cluster distance for each data point. A silhouette coefficient is 1 is best and -1 is seen to be the worst and means that the sample is in the wrong cluster, when the silhouette coefficient is close to 0, there is a presence of overlapping clusters. For an excellent description and details on how to compute it, see https://en.wikipedia.org/wiki/Silhouette_(clustering).

Dunn Index
Dunn Index is also used to evaluate clustering techniques and is very similar to the Silhouette coefficient. It is only dependent on the data within the clusters. A good clustering is one with a higher Dunn index. When using this evaluation technique, you want to be aware of a high computational cost when you have a large number of clusters. The Dunn index is computed by calculating the distance between each data point in a cluster and others in different clusters. The minimum of the pairwise distance is used to determine minimum separation (min.separation). The compactness of a cluster is measured by computing the distance between the data in the same cluster (max.diameter). Finally, the Dunn index will be: min.separation/max.diameter

If the data set contains compact and well-separated clusters, the diameter of the clusters is expected to be small and the distance between the clusters is expected to be large. Thus, the Dunn index should be maximized.

See https://en.wikipedia.org/wiki/Dunn_index for more details.

External Evaluation
External Evaluation measures the results from a clustering task based on data not used for the clustering task. Benchmarks are set from a set of pre-classified data. External evaluation techniques need ground truth data to evaluate clustering.

Rand Index
Rand Index tells you how similar a cluster or clusters are to a set benchmark. This is similar to a classification evaluation technique. You can calculate the Rand index as:

(TP + TN)/(TP+FP+FN+TN)

Purity
Purity is considered a no-frills technique that assigns each cluster to a class (usually one that occurs often in the cluster), the number of correctly assigned observations is divided by the overall number of observations to determine accuracy. Purity close to 1 is best and close to 0 is not optimal. A large number of clusters can lead to a higher purity. There is a tradeoff between the quality of clustering and the number of clusters when using purity as a metric. The normalized mutual information (NMI) can be used to measure and compare the quality of clustering between different clusterings with a varying number of clusters.

Jaccard Index
Jaccard Index is used in cluster analysis evaluation. It is defined as "the size of the intersection divided by the size of the union of the sample sets." The Jaccard distance measures dissimilarity between sample sets.

F-Measure
F-Measure is simply computed as the 
 
 . You might remember it from the classification metrics, it is also known as the F1 score.

Dice Index
The Dice Index, also known as the Sorensen-Dice index or Dice Coefficient, can assess the similarity of two samples. It ranges from 0 to 1. The dice index is a semi-metric version of the Jaccard index and gives less weight to outliers in a dataset. It is used to measure the lexical association score of two words.

As a data scientist, model interpretation means more than one thing to you and your clients, and in most cases, it will mean different things to both parties. A data scientist is interested in understanding the results of a task and how it can assist the client and their end-users in making decisions. A great resource by Marco Ribeiro explains end-user empowerment as the secret weapon to building trust in a model. The example given is of a doctor using a model to predict whether a patient has the flu or not. There is a middle "man" between the prediction and the explanation of the prediction. This explanation is what the decision-maker (doctor in this case) will use to make the decision on the right diagnosis and treatment.

Interpretability is important to data science and machine learning because it directly affects human decision-makers and their understanding of the predictions made by models. It is not enough to trust the predictions of a model based on prescribed metrics (which we cover in the next module). Instead, it is often important to know what is predicted and why the prediction was made—understanding the why will make the problem clearer and affect problem-solving for future challenges.

Doshi Velez & Kim (2017) have explained in great detail some of the reasons why interpretability is important, the most important being the ever-growing and unsatisfied curiosity of humans (and, by extension, our thirst for learning). Bias identification is another reason why interpretability is important. Why does a model grant loans to one person and not to another with similar credit scores and income? Detecting bias can also lead to better acceptance. Finally, the data scientist and machine learning engineers can debug and audit models when those models are easily interpretable.

Interpretability is not needed if a model does not have an impact of much significance or if the context in which it is applied has been extensively investigated (although this does not help with detecting bias. The studies conducted can still be laden with bias).

The next module is an overview of the assessments or metrics that typically concern you as the data scientist. These metrics are useful tools in deciding whether a model will be considered trustworthy.

Reading: Should you trust that model?

The authors of the above article proposed a technique to explain the predictions and usefulness of any machine learning model. They have tested this technique with a number of classifiers, including neural networks for text and image classification.

Local Surrogate Models"explain individual predictions of black box models."

Shapley Value is concerned with explaining a prediction by assessing the importance of features to the task.

Additional Resource: Sara Hooker: The Myth of the Perfect Model

Accuracy versus Interpretability
Throughout this course, you have learned about understanding your client's needs and developing and implementing the right analytic solution to meet those objectives. At this stage, we want to think through the interpretability of models. This will be helpful for fixing issues with the model and explaining why a model produced its results.

Interpretability is a very important research area in data science and machine learning. We want to explain why a model produces certain results and what happens when there are changes within a model, also known as explainability. Interpretability ensures that a data scientist can measure the effects of any trade-offs within a model.

Let us turn our attention to the accuracy of a model and how its results can proffer better solutions and decisions. As you know by now, errors can be the difference between a useful solution and a solution that will lead to loss of money and with how data science solutions are integrated into everyday life, lives. Accuracy can be defined as the measurement used to determine the best model for a task. If the model can properly generalize to new data, it will produce better results (such as predictions).

There are certain sectors that are restricted by laws and standards in their use of certain techniques in the banking and education industry. Some of these restrictions protect the consumers’ data and ensure that bias is not introduced into the decision-making process. As you read on the last page, the more we learn more about interpretability and employ interpretability strategies, these issues might become a thing of the past. Accuracy is very important in these sectors, and as we have learned, accuracy will typically lead to less interpretability. The big question is, "how can we retain interpretability while improving accuracy?"

Hall (2016) has recommended the following steps:

Train black-box models and use them as benchmarks.

Use different regression techniques.

Use black-box models in the deployment process.

Train small interpretable ensemble models.

Create nonlinear predictors using black-box techniques.

Explain black box models better using variable importance measures.

You are classifying the topic labels of a math problem based on its text; the topic labels are from the common core standards (a US-based standard for K-12 education). An example of one of the problems could be "a rectangular garden has an area of 20 square foot and width of 4 feet; what is its height?" The corresponding label would be "Rectangle Area." As a starting point, you have attempted bag-of-words features with logistic regression, and your model has a good performance (95%+ F-1 score). Your colleague has asked why your model has performed so well. Which is a reasonable response?

The model performed well because bag-of-word and logistic regression typically go together, and they can achieve good performance in text labeling problems.

The model performed well because the model choice is logistic regression.

The model performed well because the label is already included in the features.

The model performed well because you have paid attention to pre-processing your data and it is performing as expected.

The model performed well because text labeling problems can usually be solved with sufficient training data.

Correct. The model performed well because the label is already included in the features. There is a one-to-one mapping between feature and label. This will cause your model to perform well, but really it is not learning anything. It is not learning anything because most math questions have labels in their question, e.g., "find the area of or find the sum of..." you can tell these are Area problems, Addition or Sum problems, etc.


[Required Reading] Paper: Poursabzi-Sangdeh, F., Goldstein, D. G., Hofman, J. M., Wortman Vaughan, J. W., & Wallach, H. (2021, May). Manipulating and measuring model interpretability. InProceedings of the 2021 CHI Conference on Human Factors in Computing Systems(pp. 1-52). (Requires CMU credentials to access)

Who are the papers' authors? Why are they qualified to write about this topic?

The first author is a Senior Program Manager who studies AI and ethics in research and engineering. The remaining authors are affiliated with the Computational Social Science research group at Microsoft, which studies how to help laypeople (e.g., users of Microsoft products) make sense of numerical data.

Who is the audience of the paper?

The paper is targeting ML researchers and practitioners who build machine learning systems that interact with the end-user.

Why is the paper’s topic relevant at the time of its writing?

Machine learning models are increasingly used to aid decision-making in high-stakes domains and to influence people’s everyday decisions. However, people are reluctant to use these models due to concerns about their underlying mechanisms and fairness. In response, a prolific line of research on machine learning interpretability has emerged. This paper is one such research work.

What is the paper’s contribution? Which research gap is it trying to address?

The paper contributes a novel perspective that interpretability is a latent property that cannot be directly measured. However, it can be influenced by measurable properties and has a measurable influence on people’s behavior.


This perspective addresses the existing lack of consensus on the definition of interpretability in current literature. In addition, the paper reports an unintuitive result that clear models with fewer features are not better than complex or black-box models in their ability to help people make beneficial decisions or detect errors.

Summary of the paper’s experiments and findings.

The overall experimental procedure is to various factors that may influence a model’s interpretability and measure their effect on people’s behaviors, with a focus on the following aspects:

RQ1: How well can people simulate a model’s prediction?

RQ2: To what extent do people follow a model’s prediction when it’s beneficial for them to do so?

RQ3: How well can people detect when a model has made a mistake and correct it?

The primary task in each experiment is to predict the price of apartments in New York City, with the help of a linear regression model. The study participants always have access to all 8 features in the dataset, but they may see a clear model (with explanations on how the prediction is derived) or a black-box model (with no such explanations). In addition, the models shown to the participant may have 2 or 8 features, thereby allowing for a 2 x 2 experiment setting (clear vs black-box and 2-feature vs 8-feature).


For each of the 12 apartment data points used in the study, participants were first shown its configuration (i.e., feature values) alongside the model (whose internals were either clear or black box) and were asked to guess what the model would predict for the apartment’s selling price. They were then shown the model’s prediction and asked for their own prediction of the apartment’s selling price. For RQ1, the authors measured the difference between people’s guesses of the model’s prediction and the actual prediction result. For RQ2, the authors measured the extent to which people deviated from the model’s prediction in their guess for the apartment’s ground-truth selling price. RQ3 used the same metric as in RQ2 but only applied to the last 2 apartments which had unusual configurations (such as 3 bathrooms squeezed into 726 square feet) and which the ML models made prediction mistakes.

Based on the paper’s findings:

A clear model with a smaller number of features was easiest for participants to simulate.

There were no significant differences in the participants’ trust of the models across the 4 experimental conditions. Participants did not trust the clear model with 2 features more than the black-box model with 8 features.

When participants see unusual examples, they are less likely to correct inaccurate predictions made by clear models than by black-box models. In other words, too much transparency can be harmful, possibly due to cognitive overload.

What are the implications of the paper’s findings? What can an outsider like us expect to learn from this paper?

There are two important takeaways from the paper:

Machine learning interpretability is not purely a computational problem. An interdisciplinary approach is needed, and a human-centered focus is likely the key.

Intuition alone is not sufficient to interpret models. More empirical studies that cover a wider range of domain models, factors, and outcomes are needed.



In the research paper by Poursabzi-Sangdeh et al. (2021), which of the following findings in experiments 1 and 2 were not replicated in experiment 3?

(Simulation) The finding was that people who saw the clear, two-feature model could more closely simulate the prediction of the model shown to them than those in other conditions.

(Deviation) The finding was that people who saw the clear, two-feature model did not follow the model prediction more than those who saw the black-box, 8-feature model when it was beneficial for them to do so.

(Detection of mistakes) The finding was that participants who saw the clear models were not able to identify the mistakes on unusual data points as well as those who saw the black-box models.

Finding 1

Finding 1 and 2

Finding 3

Finding 2

Finding 1, 2, and 3

Correct. According to the third paragraph in section 5.2, this finding is different from what was shown in experiments 1 and 2.

Which of the following is a novel perspective on model interpretability, as proposed in the paper Manipulating and measuring model interpretability by Poursabzi-Sangdeh et al. (2021)?

Interpretability is a combination of simplicity, transparency, simulatability, and trustworthiness.

Interpretability is a latent property that cannot be directly measured.

Interpretability means different things to different stakeholders.

Interpretability can be defined based on human intuition.

None of the above.

Correct. The paper contributes a novel perspective that interpretability is a latent property that cannot be directly measured. However, it can be influenced by measurable properties and has a measurable influence on people’s behavior.

Among the following findings of experiment 1, which finding(s) contradicted the hypotheses of Poursabzi-Sangdeh et al. (2021) when they designed the experiment?

(Simulation) The finding was that people who saw the clear, two-feature model could more closely simulate the prediction of the model shown to them than those in other conditions.

(Deviation) The finding was that people who saw the clear, two-feature model did not follow the model prediction more than those who saw the black-box, 8-feature model when it was beneficial for them to do so.

(Detection of mistakes) The finding was that participants who saw the clear models were not able to identify the mistakes on unusual data points as well as those who saw the black-box models.

None of the three findings contradicted the initial hypotheses

Findings 2 and 3 contradicted the initial hypotheses

Findings 1 and 2 contradicted the initial hypotheses

All three findings contradicted the initial hypotheses

Findings 1 and 3 contradicted the initial hypotheses

Correct. Paragraph 3 and 4 in section 3.2 show that findings 2 and 3 contradicted the authors’ hypotheses.

The individuals who want to pursue skills required for data roles like Applied Engineer, Data Analyst, Data Engineer, Data Scientist, Data Solutions Architect, Machine Learning Engineer, Research Scientist, etc., are confused because the fields are relatively new, and there is a lot of overlap between these roles. Moreover, the definitions of the roles and skills required are different for different organizations because organizations have a different understanding of each role based on their requirements, organizational culture, and allocated budget.

This chapter introduces three different environments for AI professionals and focuses on different tasks and skills required for each environment. In general, a good AI professional needs to be aware of the basics of all three environments and be an expert in some tasks in at least one environment. Based on her interest and expertise in tasks of environments, she can further pursue skills in depth and expand their skill set. Awareness of these environments can help individuals avoid confusion while making career decisions in the dynamic data world.

Research Environment:

AI professionals first collaborate with stakeholders and domain experts to understand and define the business problem. They might also present and validate assumptions related to the problem. Once the problem is defined and assumptions are validated, AI professionals can start working in the research environment.

A research environment is where AI professionals define experiments and might use tools like Jupyter Notebook and Jupyter Lab to collect data, clean it, perform Exploratory Data Analysis (EDA), and present findings to the team. Each experiment's findings might help select and generate new features from data and build models that can potentially solve the problem. Later, metrics are defined to evaluate and select models across different experiments. Sometimes, an ensemble of models from different experiments might result in higher performance. The code might be very messy in this environment or phase. It might also be hard for others to run your code and/or reproduce your results successfully on their machines.

The different steps in the research environment include:

Define Experiments: Different experiments can be defined based on the definition of the problem. For example, suppose we have a classification problem. In that case, experiments might be defined based on different approaches like conventional supervised learning, weak supervision active learning, semi-supervised learning, pre-training, etc. Experiments are prioritized based on the type, project timeline, and quantity and quality of data.

Data Collection: For a given experiment, the AI professional might collect structured or unstructured data from existing proprietary databases, use open-source datasets, or extract data using python scripts like crawling text or images from relevant websites.

Data Cleaning: The steps in cleaning depend on the data, problem, and experiment. For example, AI professionals can impute missing values, normalize extreme values, remove duplicate samples, etc., to classify structured data.

Exploratory Data Analysis (EDA): The goal of EDA is to find patterns in cleaned data which helps in selecting relevant features for modeling and understanding relationships among them. EDA can also help identify how to further clean the data for modeling.

Feature Engineering: The knowledge from domain experts and EDA patterns help AI professionals create new features that might increase the performance of models in the experiment. Remember that generating relevant new features from existing features is called feature engineering.

Data Modeling: The goal of a model is to try to replicate domain experts’ decision-making process. AI professionals come up with mathematical algorithms and build models using relevant features to automate the decision-making process.

Tuning and Evaluation: Optimal hyperparameters can be found to maximize the model performance by comparing the metrics of each version of the model in an experiment on evaluation data.

Experiments Tracking and Evaluation: Steps 2 to 7 are repeated for each experiment and evaluated at the end. Experiment tracking tools like Neptune AI and Weights and Biases can efficiently track experiment information with a good user interface.

The model in an experiment with the highest performance is selected for working further in the development environment.

Development Environment:

A development environment is where AI professionals create components by cleaning and modularising code from, e.g., Jupyter notebooks, adding dependencies (PyTorch, Numpy, and Pandas, etc.), and packaging them. A component is an organized, modular, maintainable, and reusable code that performs one step, like data extraction in the AI/ML pipeline.

In applied machine learning, the AI/ML Pipeline automates performing a sequence of steps in components and interaction between the components defined by the AI/ML system design. The components include data collection, data preprocessing, model development and fine-tuning, post-processing on predictions, model evaluation, model deployment, maintenance, and monitoring.

Some best practices while packaging the code:

Use a version control tool. Version control plays a crucial role in the development environment. Version control tools like perforce and assembla make the processes like creating a GIT repository, defining the code repository structure, and branching strategy easy.

Install IDE like PyCharm to automatically create virtual environments for projects and allow easy integration with GIT.

Convert Jupyter Notebook code into object-oriented code and save in .py files. Have appropriate variable names, add comments, and organize different files into components with proper hierarchy.

Create config files containing standard information across multiple components like input file location, model location, output file location, cloud or external API credentials, model parameter values, hyperparameters values, etc. Config files make adding new variables easy for all components across the pipeline and modifying and removing existing variables.

Write and automate tests for multiple components. Write modules to test each component individually (unit testing) and test the interaction between components (integrating testing).

Use a logger to log the message and time. Logging makes debugging easy, especially when the code base becomes huge and complex. A logging message can have a logging level like critical, error, warning, info, debug, or notset. Critical is an essential message to log, and notset is an unimportant message to log. Levels ensure the minimum level to log. For example, if you set “level = logging.warning”, any message logged as critical, error, or warning is only logged, and other levels are ignored.

Unlike traditional software engineering where only changes in code are tracked (code versioning), data used for training, testing, and evaluation can also be tracked (data versioning) especially if data is large and dynamic. DVC , Delta Lake , and LakeFS are some open-source data versioning tools.

Often, based on the requirement, a server is built using web frameworks like FastAPI, Flask, or Django to deliver predictions to other software components.

The packaged code is further used in the production environment.

Production Environment:

Based on the size and timeline of the project, development and production environments are the same or different. Generally, the production environment is a phase where the models in the pipeline are scalable, monitored, and served in real-time by containers.

Some of the tasks performed in the production environment are:

Design Optimization: In general, there is a lot of gap between the number of models and the quality of models in the research environment, development environment, and production environment. Hence, if required, the AI/ML system design created before in the development environment needs to be optimized and redesigned for production.

Containerization using Docker: Developers might use multiple components like Data Extractor, Elastic Search, Rest API, Messaging Queues, etc. Each component has its respective dependency libraries. Having components with different versions of a library in the same environment might lead to conflict. With the help of Docker, AI Professionals can standardize environments and run different containers for different components in isolation, where each container has dependent libraries for the respective component. An environment can be created by Docker using a DockerFile. DockerFile contains instructions like navigating to a respective folder, installing dependencies, setting environment variables, loading configuration parameters for the model, etc. Scaling is easy with containers because AI professionals can spin up new containers for the same component in seconds to satisfy the scaling requirements.

Continuous Integration and Continuous Delivery (CI/CD): CI/CD enables AI professionals to work together in a shared code repository where updates to a part of code by an individual are automatically pushed, built, tested, delivered, and deployed to the shared code repository, and code issues can be tracked and resolved respectively.

Workflow Orchestration and Infrastructure Abstraction: Workflow Orchestration tools like Google’s Kubernetes and Red Hat's Openshift can quickly spin up multiple containers on different machines on demand, manage resources like memory and compute for containers, have high container availability for the product. Depending on the organization, the infrastructure of the workflow orchestration tool is owned by separate teams like DevOps or the AI professionals themselves. Some AI professionals might find it tedious to work with infrastructure abstraction tools. They can use infrastructure abstraction tools like Google’s Kubeflow and Netflix’s Metaflow , which are built on top of workflow orchestration tools that allow them to focus more on models and stop worrying about low-level infrastructure.

Monitoring and Maintaining the Deployed Models: Unlike traditional software, AI/ML models are dynamic and degrade over time. Hence, it is essential to measure, monitor, and govern the different metrics and tune models before they negatively impact user experience and business value. In general, the model’s health can be measured by three different metrics.

Resource Metrics: These measure incoming traffic, CPU/GPU memory usage or utilization (Does server efficiently utilize resources?), prediction latency (Does server handle requests quickly?), throughput (Does server maintain good throughput and scales based on requests?), and cost (Are hosting and inference costs of the entire ML pipeline are as expected or more?).

Data Metrics: It is essential to check if the input data format is correct first instead of debugging the entire pipeline.

Anomaly Checks: Simple checks like having max and minimum values for each feature (age cannot be negative or 100000) can identify and validate extreme or anomalous data points in input data. Later, the team can brainstorm and find root causes for receiving these anomalies from users.

Data Quality Issues: Users might give synonyms (“Girl” for “Female”) or incorrect values (“Mail” instead of “Male”) as input to the pipeline. In these cases, the model might fail to recognize the value in the feature “Gender” (data might be absent while training the model) and assign NaN (not a number) for the feature. Even though the model doesn’t break, the predictions produced by the model might be wrong. Hence, testing new data that the model hasn’t seen before is essential.

Data Drift: When we train a model with some static data, it assumes specific patterns based on the distribution of provided data. However, real-world data is dynamic. Because of these changes, the assumptions made by the model might no longer be valid, and the model might get biased, which leads to bad performance in real-time model evaluation. For example, water consumption in hospitals during COVID-19 was very high compared to historical data. Hence, we cannot use a model built on historical water consumption data during COVID-19. This phenomenon is called “Data Drift.” Periodically detecting changes in the distribution of data using statistical tests can help to detect data drift.

Model Metrics: It is crucial to estimate the expected performance of models before deploying them into production and periodically check if expected KPIs are met. If model predictions or expected KPI values are bad compared to benchmarks, AI professionals might consider the Model Drift issue. Model Drift is a phenomenon where the relationship between features changes, and the model no longer gives accurate predictions. For example, the relationship between the births and deaths ratio changed during COVID-19, causing model drift. Model drift can be detected by periodically analyzing feedback from feedback loops and correlating it with what is affecting the business.

Monitoring: Based on Model Metrics and Data Metrics, if re-training is required, the AI professional might start repeating the research, development, and production environment to deploy and monitor the new model. Often, retraining is also a way to improve the model's performance to reflect the change in data over time. Some of the Data Monitoring tools include SuperwiseAI , ArtherAI , and VertaAL .

Conclusion:

It is good for any individual who wants to make a career in AI to be aware of the basics of all three environments. This awareness can help individuals to identify the skills required to work in an environment. Based on their interest, they can choose to specialize in one or more of these three environments and eventually make their career decisions in the current rapidly changing data world.

Which of the following statements incorrectly describes the use of git-lfs (Git Large File Storage) to track data in a machine learning project with large datasets?

Git-lfs may not be as efficient as DVC because it requires the installation and maintenance of a dedicated server, which can add complexity and cost to the project.

Git-lfs lacks the ability to easily integrate with popular cloud storage services, such as AWS S3, GCP, and Azure, which is a notable feature of DVC.

Git-lfs lacks built-in mechanisms for monitoring and validating data changes compared to DVC.

Teams using git-lfs may need to rely on additional tools or custom solutions to manage their data processing pipelines, which can result in a less efficient and more complex project setup.

Similar to DVC, Git-lfs offers additional features like data quality management, automated testing, and deployment, or integration with MLflow for tracking experiments.

This statement is incorrect. Git-lfs does not offer additional features like data quality management, automated testing, and deployment, or integration with MLflow for tracking experiments, which are available with DVC.


AI projects are data intensive. Data can be

public (from government websites and open data)

paid (from the marketplace, brokers, and other services)

gathered (from customers using the product platform)

owned (from employees like annotators who manually create data)

We get the data mentioned above periodically to build models. Therefore, we need to analyze the quality of data and model performance every time new data is available.

It is critical to track data efficiently. For example, let's say we are working with a customer who provides us with newly collected data weekly. Let us also assume we found a drift in data distribution, and we want to roll back to the model version and keep tuning the model from that checkpoint. The previous model can be reproduced by model parameters (weights) and hyper-parameters. However, we might want to reproduce the model using another library that optimizes the performance, which requires retraining or using old data to build another model. Therefore, we need a tool to version model weights and training data efficiently.

Sometimes, after our initial engagement, we return to old projects based on customer requirements. We might want to reproduce the previous models built by other developers to deliver predictions to customers. However, the performance of models depends on the data used to train the model. Hence, data needs to be tracked periodically.

Why are traditional code versioning tools inefficient in tracking data?

The Git system (e.g., in Assembla / Atlassian) can only track comparatively small files (e.g., the source code) used for the project. Because git contains a complete history of file changes, disk and memory requirements will grow significantly if we commit data files. Hence, DVC (Data Versioning Control) aims to bring git to projects that use a lot of data and helps to track and version data efficiently. DVC is also easy to learn as it runs on top of git and uses the same git vocabulary.

git-lfs can be a solution for data versioning using pointers and remote storage. However, one of the main advantages of DVC over git-lfs is it doesn't require installing a dedicated server, and it can be used with cloud storage like AWS S3, GCP, Azure, etc. We can also assign tags for each data file version, which allows us to track necessary metadata, such as who gave us the data and when we got it.

Other advantages of Data Tracking efficiently help in:

Managing the Data Quality

Automating testing and deployment

Easy integration with MLflow, which helps in tracking experiments

Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)

1. Install Anaconda

2. Define the project folder structure

Example


Clone the above sample repo here .

3. Create a virtual environment for DVC in the respective project folder and activate it

conda create -n mypython3 python=3.8
conda activate mypython3
4. Install dvc

pip install dvc
5. Install boto3 for pushing data to AWS S3

pip install boto3
6. Install dvc[s3]

pip install dvc[s3]
7. Initiate DVC which creates .dvc/.gitignore, .dvc/config and .dvcignore files

dvc init
8. Commit the above dvc files

git commit -m "Initializing DVC - First Commit"
9. Avoid tracking the data folder for git

rm data/**/.gitkeep
git rm --cached -r data
git commit -m "stop tracking data"
10. DVC tracks the data files by adding a data folder to the DVC cache. It prevents adding to GIT by implicitly adding the data folder to .gitignore.

dvc add data
11. Git add and commit data.dvc

git add data.dvc
git commit -m "add data"
12. Create a bucket on S3

13. DVC Remote Add - creates a remote section in DVC's config file

dvc remote add -d remote s3://yourBucketName/folderName
14. Set AWS credentials

dvc remote modify remote access_key_id AWS_ACCESS_KEY_ID
dvc remote modify remote secret_access_key AWS_SECRET_ACCESS_KEY
15. Push the data folder to S3

dvc push
16. Remove AWS credential information in .dvc/config before pushing to git

17. Push the updates to git

git push origin master
Kudos, you completed the hands-on Data Versioning tutorial with DVC and AWS S3. Now you have the version history of the data and can revisit the respective files in the future.

Module 24
123.
Introduction and Evolution of Language Representations
Due:
not yet scheduled
Machines don’t understand characters, words, or sentences. They can only process numbers. Most natural language processing tasks begin with converting textual to numerical data that machines can understand. A good representation is critical for the success of downstream tasks. The NLP module provided an introduction to the most straightforward text representation techniques like bag of words, term frequency (tf), and term frequency-inverse document-frequency (tf-idf). However, these techniques had the following two significant limitations:

The individual items in a vocabulary (terms) were represented as dimensions, and thus the representations suffered from the curse of dimensionality: the representations grew with the size of language vocabulary.

No useful information like context and word order that can be useful for the downstream tasks could be encoded within the numerical representations themselves.

These shortcomings lead to the emergence of word embeddings. Word embeddings is a term used for the representation of words, typically in the form of fixed-size real-valued vectors that encode the semantics of words essentially by capturing the contexts in which they appear. Words that are closer in the vector space of the word embedding vectors are expected to be similar in meaning, i.e., vector representations of semantically similar words have a smaller distance than dissimilar words. For example, ‘learn,’ and ‘study’ will be closer than the pair ‘learn’ and ‘eat.’ Operations on these on these embeddings could also be used to derive meaning. For example, subtracting the embedding of Germany from the embedding of Berlin and then adding the embedding of France would get you an embedding close to the embedding for Paris.

These embeddings are often learned automatically from large text corpora and are based on the idea that contextual information alone can help in generating a viable representation of linguistic items. Since the semantics are captured solely using raw text data, it’s a great idea to use embeddings that are pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset. It turns out that using such pre-trained embeddings vastly improves performance in multiple tasks.

Word2Vec (2013) and GloVe (”Global Vectors for Word Representation”) (2014) are two early models to generate word embeddings that are still used widely. Word2vec embeddings are based on training a shallow feedforward neural network and leveraging occurrence within local context (neighboring words). Glove embeddings, on the other hand, are learned based on matrix factorization techniques and leverage global word-to-word occurrence counts, leveraging the entire corpus. In practice, both these embeddings give similar results for many tasks.

Neither of them, however, handles polysemy very well. These models output just one embedding for each word, combining all the semantic representations of the different senses of a word into that one vector. For example, embeddings for the different occurrences of the word “cell” in the sentence, “He went to the prison cell with his cell phone to extract blood cell samples from inmates,” would be the same.

This problem was solved by the ELMo ("Embeddings from Language Model") model developed in 2018. Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning an embedding to each word. It uses a bi-directional LSTM architecture. ELMo is trained through the task of predicting the next word in a sequence of words - a task called Language Modeling.

Another issue is that the same general word embeddings (or contextualized word embeddings) are often not enough to get a good performance in all kinds of NLP tasks. The representations often need task-specific fine-tuning to obtain better results.

ULM-FiT (“Universal Language Model Fine-tuning”), also introduced in 2018, proposed an effective inductive transfer learning method that can be applied to any NLP task and further demonstrated techniques that are key to fine-tuning a language model. It proposed a three-stage process:

General Domain LM Pre-Training, where the language model is trained on a general-domain corpus to capture general features of language in different network layers.

Target task Discriminative Fine-Tuning where the trained language model is fine-tuned on a target task dataset using discriminative fine-tuning and a changing learning rate schedule to learn task-specific features.

Target task Classifier Fine-Tuning where the classifier is fine-tuned on the target task using gradual unfreezing (unfreezing weights from the last to the first layer in different learning epochs) and repeating stage 2. This helps the network to preserve low-level representations and adapt to high-level ones.

Finally, the Transformer architecture released in 2017 revolutionized the NLP field. The release of the Transformer paper and code and the results it achieved on tasks such as machine translation made it replace LSTM models, which were most prevalent in NLP at that time. We’ll discuss the Transformer model and the motivation behind its architecture in detail next.

Traditional sequence2sequence models transform an input sequence (source) to a new one (target), and both sequences can be of arbitrary lengths. They generally have an encoder-decoder architecture where both the encoder and decoder are recurrent neural networks with LSTM or GRU units.

The Encoder processes the input sequence and compresses the information into a context vector (also known as sentence embedding or “thought” vector) of a fixed length. At a particular timestep, the encoder takes a word embedding and produces an output called the “hidden state,” which is then fed as an input with the next word embedding at the next time step. The final output is the context vector which is then used by the decoder.

The Decoder is initialized with the context vector to emit the transformed output. At a particular timestep, the decoder takes the output from the last timestep (and the context vector for the first timestep) to generate the result one-word embedding at a time.


Figure 1: Traditional Sequence2Sequence model architecture.

A critical and apparent disadvantage of this fixed-length context vector design is the incapability to remember long sentences. Often it may forgotten the first part of a long sequence once it completes processing the whole input.


Figure 2: Limitation of the traditional Sequence2Sequence model architecture.

The attention mechanism introduced in Bahdanau et al., 2015 tried to resolve this “bottleneck problem”.

Attention
Attention allows a model to focus on specific, most important parts of the sequence in the case of natural language processing or a vision model to concentrate visually on different regions of an image.

In the following example, when we see “eating,” we expect to encounter a food word very soon. The color term (”green”) describes the food but is probably not related much to “eating” directly.


Figure 3: Attention between words in a sequence.

In NLP, the attention mechanism in models try to imitate this behavior and provides a different amount of “attention” to different parts of the text with respect to some reference element. We can explain the relationship between words in one sentence or in a close context.

Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or “attends to”) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.

Attention in Sequence2Sequence Models
In the sequence2sequence models with attention, at each decoder step, the model can decide which parts of the source are more important. In this setting, the encoder does not have to compress the whole source into a single context vector - it gives representations for all source tokens by passing the intermediate hidden states to the decoder (remember that each input RNN cell produces one hidden state vector for each input word). Through the training process, the model itself learns which input words to “attend to” at each step without the need to manually provide this information. The decoder weighs the encoder's hidden states to give higher importance to words — from the input sentence — that are most relevant to decoding the next word (of the output sentence). Adding attention to Seq2Seq RNN architectures perform better across multiple translation tasks than their counterparts without attention.


Figure 4: Attention in Seq2Seq RNN architectures.

Towards Transformer Models
Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or “attends to”) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.

The Transformer model was introduced in the famous paper Attention is All You Need in 2017.

A good way to understand Transformers is to think about the fact that in the Sequence2Sequence models with attention, we are replacing the one final context vector with a hidden state generated for every output step. So do we need the hidden states at all? After all, attention alignment is supposed to define which part of the input the given output step should focus on, and the hidden states are only an indirect representation of input embeddings. A given hidden state vector represents the context of all input steps until that point and not just a single input embedding alone. Wouldn’t using the input embeddings directly make more sense?

Transformers do exactly this by replacing the sequential processing performed by RNNs in Sequence2Sequence models with a simpler attention mechanism.


Figure 5: Interactions within components in different architectures.

Instead of using attention to connect the encoder and decoder, Transformers use attention within the encoder and decoder blocks. Instead of deriving hidden states using RNNs, they use self-attention.

Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.

On the encoder side, Transformers use self-attention to generate a richer representation of a given input step 
 , with respect to all other items in the input 
 . This can be done for all input steps in parallel, unlike hidden state generation in an RNN-based encoder.

On the decoder side, an attention-based decoder is used. There are no hidden states anymore and no computation of a separate context vector for every decoder step. Instead, at a particular time-step, self-attention on all outputs generated till that point 
 along with the entire encoder output is used to generate 
 . In other words, we are applying attention to whatever we know so far.

Which one of the following statements about Transformer is Correct?

Transformer utilizes self-attention where each element in the sentence attends to other elements.++

In the typical training of Transformer for a sequence-to-sequence task, we need to mask previous tokens for the current token at the encoder side.

Transformer utilizes only one attention head at each layer.

The positional encodings in Transformer have to be fixed to the sinusoidal scheme.



Both the encoder and the decoder units in a Transformer are made up of multiple individual encoders and decoders. The units are all identical in structure but they do not share weights.


Figure 6: Transformer architecture.

Encoding
Since the input is not processed sequentially, we need a method to account for the order of words in the input sequence. For this, the transformer begins by adding a vector to each input embedding. These vectors follow a specific pattern that is either generated using a fixed function or learned, which helps the transformer determine the position of each word or the distance between different words in the sequence.


Figure 7: Transformer Encoder Inputs.

Each of the position-encoded inputs is then passed into the encoding stack. Each encoder in the stack is broken down into two sub-layers, as shown in Figure 6. The inputs first flow through a self-attention layer – that helps the encoder look at other words in the input sentence as it encodes a specific word.

The self-attention layer begins by creating three vectors from each of the encoder’s input vectors. So for each word, it creates a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that are trained during the training process.

The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best-matched videos (values). The attention operation can be thought of as a retrieval process as well. The query vectors of a particular input 
 when multiplied with the keys of the other inputs (
 ) give weights that represent how much 
 attends to those other inputs. These weights are then normalized using a softmax layer and multiplied with the value vectors for 
 . The products are then added to get a weighted sum of attention values. In essence, each token (query) is free to take as much information using the dot-product mechanism from the other words (values), and it can pay as much or as little attention to the other words as it likes by weighting the other words with keys.

The outputs of the self-attention layer are then fed into a feed-forward neural network. The exact same feed-forward network is independently applied to each position of the input sequence. The self-attention and feed-forward sublayers in each encoder have a residual connection around them and are followed by a layer-normalization step. The final results of the first encoder are then fed into the next one, and the process continues till the last encoder.


Figure 8: Transformer Encoder Architecture.

Decoding

Figure 9: Transformer Encoder-Decoder.

Decoding is shown in Figure 9. Decoding looks very similar to encoding in terms of the components. However, unlike encoding, it works sequentially. Decoders can only pay attention to the words in the sentence that they have already generated (so, only the words before the word currently being generated). For example, when we have predicted the first three target words, we give them to the decoders, which then, along with all the outputs of the last encoder, try to predict the fourth word.

In the figure, the input of the first decoder (from the bottom) are the embeddings of the output sequence (added with the positional embedding) already generated. Future positions are masked (by setting them to -inf). This way, the self-attention layer is only allowed to attend to earlier positions in the output sequence. For the first timestamp, everything is masked, and the masks are removed one by one in successive iterations as the output sequence words are generated.

After applying self-attention and then layer normalization, the results are fed into an encoder-decoder attention layer (which is something different from the encoder stack). This block tries to find the cross-attention between the encoded input sequence and the generated output sequence till a particular timestep. The entire output of the top encoder of the encoder stack is transformed into a set of attention vectors K and V for use in this layer. This layer helps the decoder focus on appropriate places in the input sequence at a given timestep to generate the next output. The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its queries from the layer below it and takes the keys and values from the output of the encoder stack. Output from the encoder-decoder attention layer then goes through feed-forward and layer normalization layers. All these components are contained in a single decoder block. The decoding results are then bubbled up through all the other decoders in the stack. The final decoded result from the last decoder is then fed into a linear layer which maps the output to a vector of the size of the vocabulary, which is then normalized using the softmax layer turning the values into probabilities. The word corresponding to the highest probability becomes the next predicted word in the output sequence.

At a high level, the following steps repeat the process until a special symbol is reached, indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did to produce successive output tokens.

Here are some additional sources for more details on Transformers.

The Annotated Transformer

The Illustrated Transformer


Both the encoder and the decoder stacks form a Transformer model as described in the previous module. However, each of the two parts can be used independently too.

Encoder-Only Models
Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having “bi-directional” attention and are often called auto-encoding models.

The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.

Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally, word classification), and extractive question answering.

Representatives of this family of models include BERT, ALBERT, RoBERTa.

Decoder-Only Models
Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.

The pretraining of decoder models usually revolves around predicting the next word in the sentence.

These models are best suited for tasks involving text generation.

Representatives of this family of models include CTRL, GPT, GPT-2.

We are now finally ready to study arguably the most famous encoder model, BERT and its variants in detail.

BERT
One of the latest milestones in NLP is the release of BERT (Bidirectional Encoder Representations from Transformers), an event described as marking the beginning of a new era in NLP. It achieved state-of-the-art performance on several language tasks.

BERT makes use of the Transformer architecture. In its vanilla form, a transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT’s goal is to generate a language model, only the encoder mechanism is necessary. In other words, BERT is basically a trained transformer encoder stack.

As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it’s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word). The fundamental units which enable the ability to comprehend the entire context of input without treating it like a sequence are made possible by the mechanism of attention. Attention is a mechanism that can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Transformer models use multi-head attention to efficiently capture the context and relative importance of input sequence and also enable parallelization of model training.

The original BERT paper presented two variants of BERT based on the number of encoder units (which the paper calls Transformer Blocks) used in the architecture.

BERT BASE is composed of 12 Encoder layers, 768 hidden units in the feedforward network and 12 attention heads for a total 110 million parameters.

BERT LARGE is composed of 24 Encoder layers, 1024 hidden units in the feedforward network and 16 attention heads for a total of 345 million parameters.

BERT Training
For any NLP task, BERT is generally trained in two steps:

First, the model is trained in a semi-supervised manner on textual data. This enables the model to develop a general sense of the language and to grasp its patterns. To build good language understanding, this step requires very large amounts of text and hence, training is resource-intensive.

Then, this pre-trained model is further fine-tuned for a specific task in a supervised manner with a labeled dataset. Additional layers can be added on top of the core model if needed. Since the pre-trained model already has some general language understanding, this step requires comparatively lesser data.

The first step is common across all tasks of a particular language and so models pre-trained on large amounts of text are often distributed publicly for fine-tuning directly for the task at hand. Thus, most of the time we only fine-tune. We will now try to understand the pre-training process which is done using Masked Language Modeling and Next Sentence Prediction.

Masked Language Modeling (MLM)
Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a special [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. Output embedding from BERT corresponding to the [MASK] input token is passed through a final classification layer (feed-forward+softmax) which generates a probability vector of the size of the language vocabulary. Token with the maximum probability represents BERT’s prediction of the masked token.


Figure 1. Illustration of masking and input flow across the model in BERT.

Next Sentence Prediction (NSP)
While training, the model receives pairs of sentences as input and through this objective, it learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.

Special tokens [CLS] and [SEP] are used to represent the start of the first and the second sentences in the input respectively. Output representation corresponding to the position of the [CLS] token is passed to a final classification layer (feed-forward+softmax) which predicts the likelihood of sentence B belonging with sentence A.


Figure 2: Illustration of the mechanism of next sentence prediction into BERT during training.

When training the BERT model, Masked LM and Next Sentence Prediction are used together, with the goal of minimizing the combined loss function of the two strategies.

Using BERT

Figure 3: Overview of fine-tuning BERT and usage of BERT in various downstream tasks.

Pre-trained BERT models can be used for a wide variety of language tasks by fine-tuning. Some examples are:

Classification tasks such as sentiment analysis are done like Next Sentence Prediction classification, by adding a classification layer on top of the Transformer output for the [CLS] token.

Question Answering tasks, where the system receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.

Named Entity Recognition (NER) where the system receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label. This is similar to what we saw in MLM.

Most plausible sentence continuation tasks where the system should choose between the best continuation for a given sentence given multiple choices. For this, all the possible concatenations are passed through BERT. A task-specific parameter vector is introduced whose dot product with the [CLS] token output representation denotes a score for each choice. These scores are normalized with a softmax layer to choose the best option.

BERT Variants
The original BERT architecture has since been modified to improve performance in terms of speed or accuracy for different use cases. A few of the famous variants are discussed below.

ALBERT (A Lite BERT)

ALBERT model has 12 million parameters (with 768 hidden layers and 128 embedding layers) as compared to 110 million parameters of BERT-Base. The lighter model reduced the training time and inference time.

To achieve a lesser number of parameters, cross-layer parameter sharing is used in which the parameter of only the first encoder is learned and the same is used across all encoders. Instead of keeping the embedding layer at 768, the embedding layer is also reduced by factorization to 128 layers.

In addition to ALBERT being light, unlike BERT which works on NSP, ALBERT works on a concept called SOP (Sentence Order Prediction). SOP is a “classification model” where the goal is to “classify” whether the 2 given sentences are swapped or not i.e., whether they are in the right order.

DistilBERT (Distilled BERT)

DistilBERT has 40% fewer parameters than BERT-Base, and runs 60% faster while preserving over 95% of BERT’s performances. It reduced the number of layers in BERT by a factor of two.

DistilBERT uses a technique called distillation, which approximates BERT, the large neural network, by a smaller one. The idea is that once a large neural network (teacher) has been trained, its full output distributions (its knowledge) can be approximated using a smaller network (student). This transfer learning technique is called teacher-student training.

RoBERTa (Robustly Optimized BERT pre-training Approach)

Introduced at Facebook, RoBERTa is a retraining of BERT with improved training methodology, 1000% more data and compute power.

To improve the training procedure, RoBERTa removed the Next Sentence Prediction (NSP) task from BERT’s pre-training and introduced a dynamic masking so that the masked token changes during the training epochs. Larger batch-training sizes were also found to be more useful in the training procedure.

ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)

Instead of MLM for pre-training, ELECTRA uses a task called “Replaced Token Detection” (RTD). In RTD, instead of masking the token, the token is replaced by a wrong token and the model is expected to classify, whether the tokens are replaced with wrong or not.

This method of pre-training the model as a discriminator rather than a generator is more sample-efficient. As a result, the learned contextual representations outperform the ones learned by BERT given the same model size, data, and compute.

Consider the scenario where an NLP system is designed to provide movie recommendations based on a user's description of their favorite movie. Identifying the context in which words appear in the description is crucial for understanding the user's preferences. Which of the following techniques is able to encode bidirectional context, helping the system better understand the user's description?

TF-IDF

GloVe

BERT ++

Word2Vec

Correct. BERT supports context modeling where previous and next sentence context is taken into consideration. This would enable the movie recommendation system to better understand the user's preferences by capturing the bidirectional context of their description.

Which of the following statements about Next Sentence Prediction (NSP) in BERT is correct?

The Masked LM and Next Sentence Prediction techniques are never used together in training BERT.

In NSP, 25% of the time, sentence pairs A and B are sequential sentences, while the other 75% of the time, they are chosen at random.

NSP is beneficial for many important downstream tasks, such as Question Answering (QA) and Natural Language Inference (NLI).+

NSP is not used to train a model that understands sentence relationships.

NSP does not contribute to BERT's understanding of language structure.

Correct. NSP is indeed beneficial for many important downstream tasks, such as Question Answering (QA) and Natural Language Inference (NLI), as it helps the model understand sentence relationships and context.

Question Answering (or QA for short) is a subdomain of Natural Language Processing and Information Retrieval with the goal of building systems that would be able to answer natural language questions based on some form of knowledge (database, set of documents, etc.). Question Answering problems consist of two parts - question and context, where context is a set of documents/sentences from which an answer to the question must be extracted. An example of a QA problem can be answering questions like "In which year Carnegie Mellon University was created?" given a set of Wikipedia articles.

There are traditionally two paradigms of question-answering systems:

Information retrieval-based question answering: These systems rely on the large amounts of information present on the web or knowledge bases and knowledge graphs like PubMed and ConceptNet. Typical systems of this type first find the documents relevant to the user question, using information retrieval techniques and use reading comprehension techniques to understand the question, and the information extracted from the knowledge sources, and to answer the questions.

Knowledge-based question answering: Such systems build a semantic representation of the user query, which is then used to query large databases efficiently.

For example, a question such as Which cities are near Pittsburgh? can be represented as ∀𝑥.city(𝑥)∧near(x, Pittsburgh); this can be mapped to a query in a database.

QA problems are typically categorized into single-hop and multi-hop problems, where "hop" represents the hop of reasoning - the number of logical steps that need to be performed in order to answer the question.

An example of a single hop of reasoning would be the question and context pair "When did Elvis Presley die? with the text "Elvis Presley died suddenly in 1977 at his Graceland estate at the age of 42."

An example of two hops of reasoning would be to pair "When did King of Rock and Roll died?" with the context "Elvis Presley is regarded as one of the most significant cultural icons of the 20th century and is often referred to as the "King of Rock and Roll" or simply "the King." He died suddenly in 1977 at his Graceland estate at the age of 42."

Another example of multiple hops of reasoning would be the question "Has company X produced disel fuel?" given the sentence "Company X has capabilities to produce disel fuel.". While there is no direct answer whether company X actually produced disel fuel or not, the assumption is that it has produced it (since the company has the capability to), which is the reasoning that the model needs to make.

Single-hop QA problems are usually much easier to solve compared to multi-hop problems.

QA problems can have different cardinality of the answer set. For instance, questions like "What is one of the top five computer science universities in USA?" naturally have an answer set cardinality of at least five while additionally adding subjectivity to the answer. This introduces additional complexity while labeling and training QA models.

A typical IR-based QA pipeline has the following components.


The picture above is the courtesy of Speech and Language Processing Daniel Jurafsky and James H. Martin

Question Processing: The main goal of this module is to extract the query and expand it. Query expansion involves evaluating an input query and expanding it to match additional documents. Simple examples of this could be finding the 'Wh' words like where, when, which etc. from the query, to a assign a type to the query. Query extraction involves extracting keywords from the query that get passed to the information retrieval(IR) system to match potential documents. The query formulation does the former task of extracting the query and passes it to the IR system.

Document Retrieval: The IR system then gets the query, extracts relevant documents from the knowledge source and ranks them according to their relevance to the question. The QA system then divides the top n documents into smaller passages, which are then passed to the subsequent steps of passage retrieval.

Passage retrieval: The most naive way to deal with passages is to send every passage to the answer extraction phase. However, more sophisticated techniques might also include understanding the passage, the named entities, the keywords, n-gram overlap, etc.

Answer Extraction: This marks the final stage of a question answering pipeline and requires either extraction of the exact span of the answer (span labeling) or recognition of named entities (for factoid type questions).

Typically, QA tasks that employ reading comprehension to solve the questions can be broadly categorized into two classes on the dimension of the presence of context knowledge along with the questions.

Closed-domain QA: These tasks have a body of context knowledge present with the question to find the answers from, which ranges from a few words to a few paragraphs. Thus the job essentially boils down to reading the paragraph(s) and the questions to bring the text into a machine-interpretable format and then employing extraction techniques to find the answer from the context.

Open-domain QA: These tasks usually involve multiple steps, it uses the same initial stages as above for reading the question and converting the question into a machine-interpretable format for which the most important concepts and/or topics of the questions are extracted. A search engine is then used to get relevant documents based on those topics. Finally, the model tries to answer the question based on the information obtained from those topics.

Unsupervised Learning Methods
Answer extraction in unsupervised fashion is extremely challenging area that is not solved yet. Therefore, we will briefly cover retrieval tasks here. Both document and passage retrieval problems can be formulated in a next way - encode sentence (or document) with some feature extraction technique (e.g. TF-IDF, or generate sentence embedding with Sent2Vec model) and find sentence/document that is the closest to the question that is encoded in a same way.

For instance, given context (array of sentences) to find sentence that contains answer to the question (where question is single sentence as well) one can use pre-trained sentence-to-vector model to encode question and each sentence from the context. Then, similarity function (e.g. distance) can be used to find sentence that is closest to the question. While this approach is naive and unsophisticated, it is easily parallelizable and can perform quite well for simple problems.

More sophisticated approaches would require supervised formulations - e.g. siamese networks have proven to be efficient for similarity calculations but require labeled pairs.

Supervised Learning Methods
In year 2006 SQuAD dataset was released. This arguably started new era in supervised Question Answering, since it introduced a new, open dataset consisting of more than 100,000 factoid questions on which many models were (finally) able to converge and provided reasonable performance.

Formulating a problem in a supervised learning domain is inherently challenging because:

The input dimensions are not fixed (questions and contexts may have variable number of sentences in them and each sentence has variable number of words)

Finding the span of the answer involves keeping a track of the answer start and answer end positions.

There are various ways the task of question answering can be formulated, which gives different granularities over the output extracted from the model. This is going to be demonstrated by the following example:

Question: How many parameters does BERT-large have?

Context: BERT-large is a really big model. It has 24 layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance. BERT is one of the state of the art models in natural language processing.

Answer: "340M"

Context pruning: This method involves pruning the number of sentences in the context to a fixed length and treating the problem as that of multiclass classification.

One caveat with this approach is that of information loss, to the extent that while pruning one might end up removing the sentence that contains the answer and the model might not be able to answer the question at all.

In the above example, the context consists of 4 sentences, a naive way to formulate this problem would be to truncate all the contexts in the dataset to a length of 4 sentences, and to train a multi-class logistic regression model over 4 classes that predict a target sentence given the question and context.

Binary classification: We can formulate the problem into that of binary classification, where, given a question and a part of the context, the model predicts whether the context is/contains the answers to the question.

The benefit of this method is that it can work with an arbitrary and variable number of sentences in the context. Whereas one of the caveats is that it usually suffers from an imbalance in the target dataset as there are many more non-target functions than the target ones.

Applying this method to the example above we can train logistic regression classifiers with four examples one with the question and correct target and 3 questions with incorrect targets. And use that model to predict the target sentence index.

Binary Classification with various-length spans: In the above method, we can also train our model with various spans over the context, we consider all the 𝑂(𝐿2) spans in the context, where 𝐿 is the length of the context.

Few benefits of this method are that we can effectively get answers at a much finer granularity and the size of the training dataset increases.

An example of this method of classification would be to take all the sentences and calculate features for all the different spans on the context. One of the problems with this method is that it leads to the blowup of the input features. To the extent that the baseline model trained using this methodology in the SQuAD dataset paper was trained on 180 million features.

Textual entailment: Another perspective of looking at the problem is that of textual entailment, where the question is treated as the premise, and the context is treated as the hypothesis. This is usually performed using sequence-to-sequence networks.

A more fine-grained span prediction can be made through the use of the attention mechanism.

The above example could be modeled for this task by considering each context sentence individually and checking for an entailment relationship between the question and the context sentence.

Language Models: Language models trained on large datasets are powerful natural language understanding machines and have found an application in question answering.

Span prediction over BERT based question answering can be achieved through transfer learning by adding two parallel layers which detect answer start and answer end probabilities for each token in the input.

To feed the above example to a BERT model we just need to concatenate the question and the context text and tokenize it using the BERT tokenizer and then feed it to the model.


[REQUIRED READING] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. [pdf]

The content of this paper is essentially similar to the Language Representation, and Transformers module introduced earlier in the course. However, because this is a foundational work in modern NLP, we have opted to cover it again in this research paper module. Here we will focus more on understanding how the paper was presented to the research community and which areas it contributes to.

Who are the papers' authors? Why are they qualified to write about this topic?

The authors are a group of researchers and engineers from Google Brain, Google Research, and the University of Toronto, with expertise in deep learning and natural language understanding. Prior to this paper, they worked on various language research projects related to Google’s services, such as language translation, speech tagging, and language inference. Interestingly, the majority of the authors have left their affiliations at the time of this paper to found their own NLP startups.

Who is the audience of the paper?

The paper targets ML researchers and engineers who build large-scale language models. It is pivoting a switch from the RNN/LSTM paradigm to a new transformer architecture that was shown to achieve state-of-the-art performance in language translation.

Why is the paper’s topic relevant at the time of its writing?

Since the resurgence of deep learning in 2012, many advances have been made in neural network architectures and methodologies, although they mostly apply to computer vision (e.g., AlexNet, VGGNet, ResNet). There wasn’t a similarly impactful innovation for natural language processing - RNNs are designed to handle sequential data but suffer from exploding/vanishing gradients; LSTMs address this issue but require three times more matrix computations. In addition, these recurrent models can only process data sequentially and do not benefit from the powerful parallel processing of modern GPUs. This paper is part of an effort to build new neural architectures that address these issues.

What is the paper’s contribution? Which research gap is it trying to address?

There are two primary innovations from the paper.

Positional Encoding is a novel way of representing word order in a sentence. Given the sentence “I like data science,” an RNN knows that “like” comes after “I” because it processes the tokens sequentially and therefore receives “I” as input before “like.” Transformers, on the other hand, construct inputs that consist of both the original tokens and their index locations in the sentence, i.e., [(“I”, 1), (“like”, 2), (“data”, 3), (“science”, 4)]. In addition to learning the embedding of the tokens, they will also learn the encoding of these index locations and, therefore, the importance of word ordering (the paper actually uses fixed formulas for the positional encoding, 
 and 
 , because they were shown to produce similar results to the learned positional embeddings). Note also that this approach enables the parallel processing of all tokens because their ordering within the input sentence has already been represented by the index locations.

Self-attention is a mechanism that relates different positions of a single sequence to compute a representation of this sequence. At a high level, self-attention allows a neural network to understand a word in the context of the other words around it – for example, it may know that “back” has different meanings in “I came back from work” and in “my back hurts” because it attends to the token “came” in the first sentence and “hurts” in the second. While self-attention has been used in prior works in conjunction with recurrent or convolutional neural networks, the innovation of this paper lies in using self-attention alone, without the associated recurrent or convolutional structure, to achieve state-of-the-art results. This is also where the paper title “Attention is all you need” comes from. As an unrelated note, the template “X is all you need” subsequently became popular in the machine learning literature, with a recent paper from CMU that both make use of it and poke fun at it.

Summarize the paper’s experiments and findings

The paper proposes the Transformer model architecture (Figure 2) for language translation, whose training procedure can be summarized as follows. Given an input sequence of tokens (e.g., an English sentence) and an output sequence of tokens (e.g., a French sentence):

Step 1: Convert each input sequence token to its vector embedding. Add this vector to the positional encoding vector, i.e., 
 or 
 to yield the word vector with positional information for each token.


Step 2: Pass the input sequence word vectors into the encoder block, which consists of a multi-headed attention unit and a fully-connected feedforward neural network unit. The attention unit generates an attention vector for every token in the input sequence to represent how much the token is related to other tokens in the same sentence. This process is performed 
 times with different, learned linear projections to different dimensions. Thus, every input token yields 
 attention vectors, which are then concatenated to form a single vector (the name multi-headed refers to the fact that multiple vectors are concatenated in this step). These attention vectors are then passed to identical but independent feedforward neural networks in parallel, outputting an encoded vector for every input token.


Step 3 is similar to Step 1 but carried out on the output sequence tokens.


Step 4: Pass the output sequence word vectors into the decoder block, which contains a masked multi-headed attention unit, followed by a multi-headed attention unit and a feedforward unit. The first attention unit generates an attention vector for every token in the output sequence to represent how much the token is related to other tokens before and including it in the same sentence (this is where the term “masked” comes from – we mask away the tokens after the current token because those are our prediction goals). These attention vectors for the output tokens, combined with the output of the encoder block, are passed into the second attention unit. This unit generates an attention vector for every token in both the input and output sequence to represent how much the token is related to every other token in both sequences (in other words, this unit relates every input English token to all the other input English tokens and to all the output French tokens).


Step 5: The output from step 4 is fed to a standard linear classifier, represented as a fully connected layer with a softmax activation function. The layer outputs the probability that each word in French is the next output (in other words, this is a multi-class classification problem where the classes are all the French words, and the word with the highest probability value is predicted to be the next output token).


Additionally, batch normalization is applied after every unit to smoothen the data and make it easier to learn with larger learning rates.

During inference, the same process as above applies, but the output sequence is replaced by an empty sequence with only a start-of-sequence token (because this is the prediction goal). The transformer will predict the next token one by one and add each predicted token to the output sequence, so that it can be used as the basis for the next token prediction. This is the same inference technique used by Sequence2Sequence models, except that at each timestep, we use the entire output sequence generated so far, rather than only the most recent prediction.

The above architecture was evaluated in two tasks: English-German translation and English-French translation. Details about the training process and hyperparameters used can be found in Section 5 of the paper. Results from the experiment (Table 2) showed a better BLEU scores than previous state-of-the-art models at a fraction of the training cost.

What are the implications of the paper’s findings? What can an outsider like us expect to learn from this paper?

The paper presents a novel Transformer architecture that significantly improves upon the standard RNN/LSTM variations in both performance and efficiency. Transformers have been extensively used in both natural language processing and computer vision research following the publication of this paper (which has been cited more than 47,000 times, based on Google Scholar). It also led to the release of large-scale pre-trained Transformer models, such as BERT , GPT-3 and T5 , which anyone can utilize for their own projects.




