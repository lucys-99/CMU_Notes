Value iteration with neural networks: $Q(s, a) = r(s, a) + \mathbb{E}_{s'}\left( \max_{a'} Q(s', a') \right)$
### Loss function: 
As we've shown $V(s‚Ä≤)=maxa‚Ä≤‚ÄãQ(s‚Ä≤,a‚Ä≤)$, we want we can rewrite $Q(s,a)=r(s,a)+Es‚Ä≤‚Äã(maxa‚Ä≤‚ÄãQ(s‚Ä≤,a‚Ä≤))$. People have aimed to approximate QŒ∏‚Äã with a neural network, but the challenge is defining a suitable loss function to run gradient descent.

Let us define $y(s,a)=r(s,a)+Œ≥maxa‚Ä≤‚ÄãQ(s‚Ä≤,a‚Ä≤)$. Gradient descent is difficult since y changes as QŒ∏‚Äã changes, so keeping our algorithm on-policy is not optimal.

We can draw inspiration from **TD-learning** (temporal difference) to construct an off-policy algorithm called **fitted Q-iteration**. We want to train $Q_Œ∏‚Äã(s,a)$ to mimic $y(s,a)$, such that
$\theta' = \min_{\theta} \mathbb{E}_{s,a \sim \text{somewhere}} [(Q_\theta(s,a) - y(s,a))^2]$ 
$s‚Ä≤‚àºP(‚ãÖ‚à£s,a)$
This algorithm can use data from **any policy Œ∏** to construct $y(s,a)$. A general pipeline would be: starting with policy $Œ∏_0$‚Äã, calculating $y_{Œ∏0}‚Äã‚Äã(s,a)$, obtaining $Œ∏_1$‚Äã from $y_{Œ∏_0}‚Äã‚Äã(s,a)$, and so on until we converge.

If we don't already have (s,a) pairs, we need data to train on. These (s,a) samples can come from any distribution, but the distribution should provide **good coverage** over the state-action space. Ideally, it is close to uniform, though we can't assume uniform coverage at the start since many states may not have been visited yet. In practice, we collect tuples (s,a,r,s‚Ä≤) from experience. Training then amounts to minimizing the squared error over these tuples, and using squared error ensures the learning remains unbiased with respect to the sampled data.

**Important clarification.** Although we use gradients to minimize the Bellman regression loss, this is _not_ the same as performing gradient descent directly on the policy objective (i.e., it's not a policy gradient step). Instead:
- We are performing supervised-style regression to make $Q_Œ∏$‚Äã consistent with the Bellman equation,
- and only after (or during) fitting $Q_Œ∏$‚Äã we extract a policy by greedy improvement: m$œÄ(s)=argmax_a‚Äã Q_Œ∏‚Äã(s,a)$.
Fitted Q-Iteration (FQI) is an offline, batch-based reinforcement learning algorithm that uses a function approximator to learn the optimal Q-function in environments with large or continuous state spaces. It reframes the problem of Q-learning as a sequence of supervised learning (regression) problems.
### How Fitted Q-Iteration Works üîÑ
The core idea is to iteratively improve an estimate of the Q-function by training a supervised learning model on targets generated by the Bellman equation.
1. **Collect a Static Dataset**: First, collect a dataset $\mathcal{D}$ of transitions `$(s, a, r, s')$` by having an agent interact with the environment. This is done only once, and the policy used for collection doesn't need to be optimal (it can even be random).
2. **Initialize the Q-Function**: Start with an initial function approximator, $\hat{Q}_0$, which can be as simple as a function that returns zero for all inputs.
3. **Iterate and Fit**: Repeat the following steps for a fixed number of iterations ($k=0, 1, 2, \dots$):
    - **Create Targets**: For every transition $(s_i, a_i, r_i, s'_i)$ in your dataset, create a target value $y_i$ using the Q-function from the _previous_ iteration, $Q^‚Äãk$‚Äã: $y_i‚Äã=r_i‚Äã+Œ≥max_{a‚Ä≤}‚ÄãQ^‚Äãk‚Äã(s_i‚Ä≤‚Äã,a‚Ä≤)$
    - **Train a New Model**: Create a new training set where the inputs are the state-action pairs $(s_i, a_i)$ and the labels are the targets $y_i$. Train a _new_ supervised learning model ($\hat{Q}_{k+1}$) on this dataset to minimize the prediction error.
4. **Extract Policy**: After the final iteration, the optimal policy is the one that acts greedily with respect to the final Q-function estimate, $\hat{Q}_K$.
##### Why It Works
FQI works because it is a practical, sample-based implementation of the **Value Iteration** algorithm. Each iteration of FQI approximates one full backup of the Bellman optimality operator.
- The target calculation step, $y_i = r_i + \gamma \max_{a'} \hat{Q}_k(s'_i, a')$, is a direct application of the Bellman update rule using the collected data samples.
- The supervised learning step finds a function $\hat{Q}_{k+1}$ that best approximates the result of this Bellman update across all the states and actions in the dataset.
By repeatedly applying this process, the learned function $\hat{Q}_k$ converges towards the optimal Q-function, $Q^*$, just as tabular Value Iteration would. The function approximator allows this value information to be generalized to unseen states.
##### Advantages and Disadvantages of FQI
###### Advantages üëç
- **High Sample Efficiency**: As a **batch** and **off-policy** algorithm, FQI reuses the same dataset in every iteration. It can extract a significant amount of information from a limited number of samples, making it far more sample-efficient than on-policy methods.
- **Handles Continuous Spaces**: By using a function approximator (like a random forest or neural network), it can naturally handle continuous state and action spaces where tabular methods are impossible.
- **Stable and Reproducible**: Since it operates on a fixed dataset, the training process is deterministic and can be more stable than online methods like DQN that learn from a continuous stream of potentially correlated data.
###### Disadvantages üëé
- **Error Accumulation**: This is the primary drawback. Small errors made by the function approximator in one iteration ($\hat{Q}_k$) are "baked into" the training targets for the next iteration. These errors can compound over many iterations, potentially causing the algorithm to become unstable or converge to a poor solution.
- **Computationally Expensive**: Training a new supervised learning model from scratch at every iteration can be very slow and resource-intensive, especially with large datasets.
- **Offline Only**: FQI is not designed for online learning. It cannot easily incorporate new experiences without re-running the entire iterative process on an updated dataset.

|Feature|Fitted Q-Iteration (FQI)|Q-Learning / DQN|
|---|---|---|
|**Data Usage**|**Batch (Offline)**: Learns from a fixed dataset.|**Incremental (Online)**: Learns from a stream of data.|
|**Update Mechanism**|**Retrains a new model** from scratch at each iteration.|**Updates an existing model** with each new experience.|
|**Sample Efficiency**|**Very High**: Reuses the entire dataset at every iteration.|**Lower**: Online methods are typically less sample-efficient.|
|**Computational Cost**|**High per iteration**: Must train a full model.|**Low per step**: A single, quick update.|
|**Typical Use Case**|Offline RL, where you have a pre-existing log of data.|Online RL, where an agent is actively exploring.|
