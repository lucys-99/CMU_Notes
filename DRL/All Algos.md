| Algorithm          | Type         | Policy     | State Space | Action Space | Replay Buffer?           | Solves/Addresses                                                        | Advantages                                                              | Disadvantages                                                             |
| ------------------ | ------------ | ---------- | ----------- | ------------ | ------------------------ | ----------------------------------------------------------------------- | ----------------------------------------------------------------------- | ------------------------------------------------------------------------- |
| Fitted Q Iteration | Value-Based  | Off-Policy | Continuous  | Discrete     | No (uses a static batch) | Offline learning from a fixed dataset.                                  | Highly sample efficient.                                                | Prone to error accumulation; computationally heavy per iteration.         |
| DQN                | Value-Based  | Off-Policy | Continuous  | Discrete     | Yes                      | Applying Q-learning to high-dimensional state spaces (e.g., images).    | Stable for a value-based method; learns directly from pixels.           | Suffers from overestimation bias; can't handle continuous actions.        |
| DDQN               | Value-Based  | Off-Policy | Continuous  | Discrete     | Yes                      | The overestimation bias of DQN.                                         | More accurate value estimates; often leads to better policies than DQN. | Still can be sample inefficient; limited to discrete actions.             |
| REINFORCE          | Policy-Based | On-Policy  | Continuous  | Both         | No                       | Direct optimization of a policy using Monte Carlo gradient estimates.   | Simple to understand and implement; a foundational concept.             | Very high variance; extremely poor sample efficiency.                     |
| A2C / A3C          | Actor-Critic | On-Policy  | Continuous  | Both         | No                       | The high variance of REINFORCE by using a critic as a baseline.         | Lower variance than REINFORCE; A3C is highly parallelizable.            | Still on-policy (less sample efficient than off-policy methods).          |
| TRPO               | Policy-Based | On-Policy  | Continuous  | Both         | No                       | Instability of large policy updates by using a trust region constraint. | Very stable with monotonic improvement guarantees.                      | Computationally complex and hard to implement; largely replaced by PPO.   |
| PPO                | Actor-Critic | On-Policy  | Continuous  | Both         | No                       | Instability of policy updates by using a simpler clipped objective.     | Stable and reliable; excellent performance for an on-policy method.     | Less sample efficient than the best off-policy methods.                   |
| AWR                | Policy-Based | Off-Policy | Continuous  | Both         | Yes (typically)          | Simple offline RL and imitation learning without complex corrections.   | Simple to implement (weighted regression); stable for offline learning. | Can be less performant than dedicated off-policy actor-critic algorithms. |
| DDPG               | Actor-Critic | Off-Policy | Continuous  | Continuous   | Yes                      | Applying actor-critic methods to continuous action spaces.              | Sample efficient (uses replay buffer); can solve complex control tasks. | Extremely sensitive to hyperparameters; can be very unstable.             |
| TD3                | Actor-Critic | Off-Policy | Continuous  | Continuous   | Yes                      | The overestimation bias and instability of DDPG.                        | Much more stable and higher performing than DDPG.                       | More complex than DDPG (e.g., uses two critics).                          |
| SAC                | Actor-Critic | Off-Policy | Continuous  | Continuous   | Yes                      | The instability of DDPG and encourages robust exploration via entropy.  | Highly sample efficient; very stable and robust to hyperparameters.     | Can be more computationally intensive than other methods.                 |
| CEM                | Evolutionary | N/A        | Continuous  | Both         | No                       | Gradient-free optimization for policies with simple distributions.      | Very simple to implement; highly parallelizable.                        | Only optimizes a simple distribution; can be sample inefficient.          |
| CMA-ES             | Evolutionary | N/A        | Continuous  | Both         | No                       | Optimizing policies in complex, correlated parameter spaces.            | Very efficient for difficult, non-convex problems; adapts search space. | More complex than CEM; can be sample inefficient.                         |
| NES                | Evolutionary | N/A        | Continuous  | Both         | No                       | Bridging evolutionary methods with gradient-based ideas.                | Principled gradient estimate; more efficient than simpler ES.           | Can be complex to implement; sample inefficient.                          |